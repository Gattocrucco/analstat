% Giacomo Petrillo
% lezione di Punzi

\section{Stima puntuale}

\begin{definition}[Stima]
	Dato un modello con parametro $\theta$,
	una \emph{stima di $\theta$} è una statistica a valori nello spazio di $\theta$.
\end{definition}

\begin{definition}[Consistenza]
	Dato un modello con parametro $\theta$ per $N$ estrazioni di $x$
	e una stima $\hat\theta_N$ di $\theta$,
	la stima è \emph{consistente}
	se per $N\to\infty$ converge debolmente al \emph{valore vero},
	cioè il valore del parametro in cui calcoliamo la probabilità:
	\begin{equation*}
		\forall\epsilon\, \lim_{N\to\infty} P(|\hat\theta_N(\mathbf x)-\theta| > \epsilon;\theta) = 0.
	\end{equation*}
\end{definition}

\begin{definition}[Bias]
	Dato un modello con parametro $\theta$ e una stima $\hat\theta$ di $\theta$,
	il \emph{bias} è la differenza tra la media della stima e il valore vero:
	\begin{equation*}
		b(\theta) \is E[\hat\theta;\theta] - \theta.
	\end{equation*}
	Una stima si dice \emph{corretta} o \emph{unbiased} se il bias è nullo;
	\emph{asintoticamente corretta} se il modello è per $N$ estrazioni
	e nel limite $N\to\infty$ il bias tende a zero.
\end{definition}

%%% parte aggiunta 2022-02-24 %%%

\begin{exercise}
    %
    Dimostrare che per la poissoniana
    %
    \begin{equation*}
        %
        p(k;\mu) = \frac{\mu^k}{k!}e^{-\mu}
        %
    \end{equation*}
    %
    l'unico stimatore unbiased di $e^{-\mu}$ è $\delta_{k0}$.
    %
\end{exercise}

\begin{solution}
    %
    Chiamiamo $f(k)$ lo stimatore di $e^{-\mu}$ che vogliamo ricavare. In base
    alla definizione di bias, la condizione di unbiasedness è
    %
    \begin{equation*}
        E[f(k);\mu]
        = \sum_{k=0}^\infty \frac{\mu^k}{k!}e^{-\mu} f(k)
        = e^{-\mu}.
    \end{equation*}
    %
    Il termine $e^{-\mu}$ si cancella da entrambi i lati, quindi abbiamo
    %
    \begin{equation*}
        %
        \sum_{k=0}^\infty \frac{\mu^k}{k!} f(k)  =  1.
        %
    \end{equation*}
    %
    Prendendo il limite per $\mu \to 0$ a entrambi i membri, risulta
    $f(0) = 1$, quindi ci rimane
    %
    \begin{equation*}
        %
        \sum_{k=1}^\infty \frac{\mu^k}{k!} f(k) = 0.
        %
    \end{equation*}
    %
    Poiché le potenze di $\mu$ sono linearmente indipendenti (considerate nello
    spazio delle funzioni di $\mu$), necessariamente $f(k) = 0$ per $k \ge 1$.
    %
\end{solution}

\begin{exercise}
    %
    Sempre con la poissoniana, dimostrare che l'unico stimatore unbiased di
    $\mu^2$ è $k(k-1)$. In particolare viene 0 per $k = 1$.
    %
\end{exercise}

\begin{solution}
    %
    La condizione di unbiasedness è
    %
    \begin{align*}
        %
        E[f(k);\mu]
        = \sum_{k=0}^\infty \frac{\mu^k}{k!}e^{-\mu} f(k)
        &= \mu^2 \implies \\
        %
        \implies
        \sum_{k=0}^\infty \frac{\mu^k}{k!} f(k)
        &= \mu^2 e^\mu
        = \sum_{q=0}^\infty \frac{\mu^{q+2}}{q!},
        %
    \end{align*}
    %
    dove nell'ultimo passaggio abbiamo espanso in serie $\mu^2e^\mu$.
    Per unicità della serie, abbiamo che non ci possono essere le potenze
    $\mu^0$ e $\mu^1$, quindi $f(0) = f(1) = 0$. Rimane
    %
    \begin{align*}
        %
        \sum_{k=2}^\infty \frac{\mu^k}{k!} f(k)
        &= \sum_{q=0}^\infty \frac{\mu^{q+2}}{q!} \implies \\
        %
        \implies
        \sum_{k'=0}^\infty \frac{\mu^{k'+2}}{k'!} \frac{f(k'+2)}{(k'+1)(k'+2)}
        &= \sum_{q=0}^\infty \frac{\mu^{q+2}}{q!},
        %
    \end{align*}
    %
    da cui sempre per unicità ricaviamo $f(k) = k(k-1)$.
    %
\end{solution}

Gli ultimi due esercizi mostrano che avere bias nullo anche se suona bene non è
detto che sia una proprietà sensata. Nel primo caso ci viene che la stima di
$e^{-\mu}$ è zero per qualsiasi $k \ge 1$, anche se sicuramente $e^{-\mu} > 0$
perché se fosse $\mu \to \infty$ allora avremmo anche in probabilità $k \to
\infty$. Nel secondo caso la stima di $\mu^2$ è zero se $k=1$, e anche lì
sapremmo che $\mu > 0$ perché altrimenti $P(k=0) = 1$.

Il senso del bias però come vedremo è che è abbastanza semplice da poterci fare
delle dimostrazioni e ricavarne alcune proprietà utili in pratica seppur non
in tutti i casi, la prima delle quali lasciamo come esercizio:

\begin{exercise}
    %
    Dimostrare che se $\hat\theta(x)$ è uno stimatore unbiased di $\theta$,
    allora al primo ordine $f(\hat\theta(x))$ è uno stimatore unbiased di
    $f(\theta)$.
    %
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Efficienza}

\begin{theorem}[Disuguaglianza di Cramér-Rao]
	\label{th:cramerrao}
	Consideriamo un modello con parametro $\theta\in\R$.
	Sia $\hat\theta$ una stima con bias~$b$
	la cui pdf ha supporto che non dipende da~$\theta$.
	Allora c'è un limite inferiore alla varianza della stima:
	\begin{equation*}
		\var[\hat\theta;\theta] \ge \frac{\left( 1 + \frac{\partial b}{\partial\theta} \right)^2}{I_{\hat\theta}(\theta)},
	\end{equation*}
	dove $I$ è l'informazione di Fisher.
\end{theorem}

\begin{exercise}
    %
    Dimostrare la disuguaglianza di Cramér-Rao.
    %
\end{exercise}

\begin{solution}
	Sia la varianza che l'informazione di Fisher sono dei valori attesi, quindi,
	se riusciamo a scrivere il termine $1+b'(\theta)$ come un opportuno valore atteso,
	possiamo usare la disguaglianza di Schwartz,
	che scritta con i valori attesi è (vedi \autoref{ex:corrbound})
	\begin{equation*}
		E[fg]^2 \le E[f^2]E[g^2].
	\end{equation*}
	Scriviamo la varianza e l'informazione di Fisher,
	definendo per comodità $L \is p(\hat\theta;\theta)$:
	\begin{align*}
		\var[\hat\theta;\theta]
		&= \int\de\hat\theta\, L (\hat\theta-E[\hat\theta;\theta])^2, \\
		I_{\hat\theta}(\theta)
		&= \int\de\hat\theta\, L \left( \pdv{\log L}\theta \right)^2.
	\end{align*}
	Quindi per usare Schwartz dobbiamo mostrare che $1+b'(\theta)$ è uguale a
	\begin{equation*}
		\int\de\hat\theta\, L (\hat\theta-E[\hat\theta;\theta]) \pdv{\log L}\theta.
	\end{equation*}
	Calcoliamo il primo termine:
	\begin{align*}
		\int \de\hat\theta\,
		L \hat\theta \pdv{\log L}\theta 
		&= \int \de\hat\theta\,
		\pdv L\theta \hat\theta
		= \int \de\hat\theta\,
		\pdv{}\theta (\hat\theta L)
		= \pdv{}\theta \int \de\hat\theta\, \hat\theta L = \\
		&= \pdv{}\theta E[\hat\theta;\theta]
		= \pdv{}\theta (\theta + b(\theta)) = \\
		&= 1 + b'(\theta),
	\end{align*}
	vediamo che il secondo termine è nullo:
	\begin{align*}
		\int \de\hat\theta\,
		L\cdot E[\hat\theta;\theta] \pdv{\log L}\theta
		&= E[\hat\theta;\theta] \int \de\hat\theta\, \pdv L\theta = \\
		&= E[\hat\theta;\theta] \pdv{}\theta \int \de\hat\theta\, L = \\
		&= 0. \qedhere
	\end{align*}
\end{solution}

Per il \autoref{th:fishstat} l'informazione di Fisher di $x$ è maggiore o uguale a quella di $\hat\theta$,
quindi si ha il corollario che
\begin{equation*}
	\var[\hat\theta;\theta] \ge \frac{\left( 1 + \frac{\partial b}{\partial\theta} \right)^2}{I_x(\theta)};
\end{equation*}
nel caso di bias che dipende poco dal parametro ($|\partial b/\partial\theta| \ll 1$)
questo limite inferiore, detto \emph{minimum variance bound}, non dipende dallo stimatore.

Nel caso di informazione di Fisher proporzionale al numero di estrazioni otteniamo
\begin{equation*}
	\var[\hat\theta] \ge O \left(\frac1N\right),
\end{equation*}
questa è la forma generale dell'osservazione che comunemente
<<l'errore cala con la radice del numero di misure>>.

\begin{definition}[Efficienza]
	Con la notazione del \autoref{th:cramerrao} e con $x$ variabile del modello,
	l'\emph{efficienza di $\hat\theta$} è
	\begin{equation*}
		\frac{\left( 1 + \frac{\partial b}{\partial\theta} \right)^2}{I_x(\theta)\var[\hat\theta;\theta]},
	\end{equation*}
	che nelle ipotesi del teorema è minore o uguale a~1.
\end{definition}

\begin{theorem}
	\label{th:suffeff}
	Con la notazione e le ipotesi del \autoref{th:cramerrao},
	se $\hat\theta$ è sufficiente per $\theta$ allora ha efficienza unitaria.
	\marginpar{Il Del Prete 2010-01-21 a pagina 177 (in fondo)
	dice che efficiente $\rightarrow$ sufficiente e che non vale il viceversa.
    Inoltre per applicare Darmois bisogna che lo stimatore sia sufficiente per
    qualsiasi numero di estrazioni, non bastano le ipotesi del \autoref{th:cramerrao}.
    Per la gaussiana con media e varianza libere questo teorema non vale,
    perché la varianza campione e la media sono sufficienti e unbiased ma
    la varianza campione non è efficiente (vedi \autoref{ex:varvar}), quindi
    ipotizzo che il teorema valga solo per parametro 1D e non con la versione
    generale.}
\end{theorem}

\begin{proof}
	Nella dimostrazione della disuguaglianza di Cramér-Rao (\autoref{th:cramerrao})
	abbiamo usato la disuguaglianza di Schwartz,
	che è saturata se le due funzioni sono proporzionali,
	cioè se per una qualche funzione $g$
	\begin{equation}
		\label{eq:dethetalogp}
		\pdv{}{\theta}\log p(\hat\theta;\theta) = g(\theta) (\hat\theta-E[\hat\theta;\theta]).
	\end{equation}
	Poiché $\hat\theta$ è sufficiente, $I_{\hat\theta}(\theta)=I_x(\theta)$,
	quindi dalla saturazione di Cramér-Rao si ottiene l'efficienza unitaria.
	Cerchiamo di calcolare $\partial_\theta\log p(\hat\theta;\theta)$.
	Per il teorema di Darmois (\autoref{th:darmois}),
	la pdf di $x$ si scrive come
	\begin{equation*}
		p(x;\theta)
		= \exp \left( \hat\theta(x)a(\theta) + \beta(x) + \gamma(\theta) \right).
	\end{equation*}
	Per l'ipotesi di sufficienza, la pdf di $x$ si fattorizza con quella di $\hat\theta$:
	\begin{equation*}
		p(x;\theta)
		= f(x) p(\hat\theta(x);\theta), \quad f > 0,
	\end{equation*}
	quindi, per qualsiasi $x\in\hat\theta^{-1}(\hat\theta_0)$, si ha
	\begin{align*}
		p(\hat\theta_0;\theta)
		&= \frac{p(x;\theta)}{f(x)} = \\
		&= \exp \left( \hat\theta_0 a(\theta) + \beta(x) + \gamma(\theta) - \log f(x) \right).
	\end{align*}
	Poiché l'espressione dipende solo da $\theta_0$ (e $\theta$),
	possiamo definire $B(\hat\theta_0)\is\beta(x)-\log f(x)$,
	quindi infine
	\begin{equation*}
		p(\hat\theta;\theta)
		= \exp \left( \hat\theta a(\theta) + B(\hat\theta) + \gamma(\theta) \right).
	\end{equation*}
	La derivata del logaritmo è
	\begin{align*}
		\pdv{}\theta \log p(\hat\theta;\theta)
		&= \hat\theta a'(\theta) + \gamma'(\theta),
	\end{align*}
	quindi per far tornare la \eqref{eq:dethetalogp} bisogna che
	\begin{equation*}
		\gamma'(\theta) = -a'(\theta) E[\hat\theta;\theta]
	\end{equation*}
	con $g(\theta)=a'(\theta)$.
	Calcoliamo:
	\begin{align*}
		a'(\theta) E[\hat\theta;\theta]
		&= a'(\theta) \int\de\hat\theta\, p(\hat\theta;\theta) \hat\theta = \\
		&= \int\de\hat\theta\, \hat\theta a'(\theta) e^{\hat\theta a(\theta)+B(\hat\theta)+\gamma(\theta)} = \\
		&= \int\de\hat\theta\, \pdv{}\theta \big(e^{\hat\theta a(\theta)+B(\hat\theta)}\big) e^{\gamma(\theta)} = \\
		&= \pdv{}\theta \left( \int\de\hat\theta\, p(\hat\theta;\theta) e^{-\gamma(\theta)} \right) e^{\gamma(\theta)} = \\
		&= \pdv{}\theta \big(e^{-\gamma(\theta)})e^{\gamma(\theta)} = \\
		&= -\gamma'(\theta). \qedhere
	\end{align*}
\end{proof}

Questo teorema non dice nulla sul bias di $\hat\theta$,
però ha bias nullo se lo consideriamo come stimatore della sua media,
che è una funzione del parametro.

\begin{example}
	Consideriamo la gaussiana a varianza fissata
	\begin{equation*}
		p(x;\mu)
		= \frac1{\sqrt{2\pi}} \exp\left(-\frac12 (x-\mu)^2\right).
	\end{equation*}
	L'informazione di Fisher per $N$ estrazioni è $N$ (vedi \autoref{th:fishgauss}).
	La media aritmetica è uno stimatore sufficiente e con bias nullo,
	e infatti la sua varianza è $1/N$.
\end{example}

\begin{example}
	Consideriamo la gaussiana a media fissata
	\begin{equation*}
		p(x;\sigma)
		= \frac1{\sqrt{2\pi}\sigma} \exp\left(-\frac12 \frac{x^2}{\sigma^2}\right).
	\end{equation*}
	Lo scarto quadratico medio
	\begin{equation*}
		\hat\sigma^2(\mathbf x)
		\is \frac1N \sum_{i=1}^N x_i^2
	\end{equation*}
	è sufficiente e l'informazione di Fisher per $\sigma^2$ è $N/(2\sigma^4)$.
	Calcoliamo il valore atteso:
	\begin{align*}
		E[\hat\sigma^2]
		&= E \left[ \frac 1N \sum x^2 \right] = \\
		&= \frac 1N \sum E[x^2] = \\
		&= \sigma^2,
	\end{align*}
	quindi ha bias nullo.
	La varianza deve allora essere $1/I(\sigma^2)=2\sigma^4/N$, verifichiamolo:
	\begin{align*}
		E[\hat\sigma^4]
		&= E \left[ \frac1{N^2}\left(\sum_ix_i^2\right)^2 \right] = \\
		&= \frac1{N^2} \sum_{ij} E[x_i^2x_j^2] = \\
		&= \frac1{N^2} \left( \sum_{i=j} E[x_i^4] + \sum_{i\neq j} E[x_i^2]E[x_j^2] \right) = \\
		\Bigg[ E[x^4]
		&= \frac{\sigma^4}{\sqrt{2\pi}} \int\de x\, e^{-\frac12 x^2} x^4 = \\
		&= - \frac{\sigma^4}{\sqrt{2\pi}} \int\de x\, \dv{}{x} \left(e^{-\frac12x^2}\right) x^3
		= 3 \sigma^4 \Bigg] \\
		&= \frac1{N^2} \left( 3N + \frac{N(N-1)}2 \right) \sigma^4 = \\
		&= \frac{N+2}N \sigma^4,
	\end{align*}
	\begin{equation*}
		\var[\hat\sigma^2]
		= E[\hat\sigma^4] - E[\hat\sigma^2]^2
		= \frac{2\sigma^4}N.
	\end{equation*}
	Dunque $\hat\sigma^2$ è uno stimatore corretto ed efficiente di $\sigma^2$.
	Si può mostrare invece che $\hat\sigma$ come stimatore di $\sigma$ ha bias non nullo
	ma che non dipende da $\sigma$,
	quindi calcolato il bias lo si può sottrarre e per il \autoref{th:suffeff} $\hat\sigma$ rimane efficiente,
	quindi dall'informazione di Fisher e dal bias si ricava la varianza.
	\marginpar{Sul quaderno alla data 2017-11-23 ho scritto il conto del bias, magari metterlo in appendice.
	Forse il conto è più veloce dando per buona la distribuzione del $\chi^2$. \\
	\emph{Sul Del Prete a pagina 177 dice che non c'è uno stimatore efficiente di $\sigma$.} \\
    Forse Del Prete intende nel caso con media non fissata? Però allora
    bisogna prendere la versione multidimensionale di Cramér-Rao.}
\end{example}

%%%% esercizio aggiunto 2022-01-07
%%%% per sottolineare che sufficienza==>efficienza non funziona in generale
\begin{exercise} \label{ex:cramerraond}
    %
    La disuguaglianza di Cramér-Rao (\autoref{th:cramerrao}) ha anche una
    versione multidimensionale cioè con parametro in $\mathbb R^n$, $n > 1$:
    la matrice
    %
    \begin{equation*}
        %
        D_{ij} \is
        \cov[\hat\theta_i, \hat\theta_j;\boldsymbol\theta] -
        \frac{\partial E[\hat\theta_i;\boldsymbol\theta]}{\partial\theta_k}
        \frac{\partial E[\hat\theta_j;\boldsymbol\theta]}{\partial\theta_l}
        (I^{-1}(\boldsymbol\theta))_{kl}
    \end{equation*}
    %
    è semidefinita positiva. L'efficienza si ha quando $D = 0$. Mostrare che
    nel caso $n = 1$ è equivalente a quella già enunciata, e trovare un
    controesempio a sufficienza $\implies$ efficienza (\autoref{th:suffeff})
    quando $n > 1$ (aiuto: il controesempio è già comparso tra i vari esercizi
    ed esempi del capitolo).
    %
\end{exercise}

\begin{solution}
    %
    Una matrice $1\times1$ è semidefinita positiva se è $\ge 0$. Quindi viene
    %
    \begin{align*}
        %
        D &= \cov[\hat\theta,\hat\theta;\theta] -
        \left(\frac{\partial E[\hat\theta;\theta]}{\partial\theta}\right)^2
        I^{-1}(\theta) = \\
        %
        &= \var[\hat\theta;\theta] -
        \left(\frac{\partial}{\partial\theta}(\theta + b(\theta))\right)^2
        I^{-1}(\theta) = \\
        %
        &= \var[\hat\theta;\theta] - (1 + b'(\theta))/I(\theta) \ge 0.
        %
    \end{align*}
    %
    Consideriamo la gaussiana con media e varianza libere. La matrice di
    informazione di Fisher è (vedi \autoref{th:fishgauss})
    %
    \begin{equation*}
        %
        I(\mu,\sigma^2) = \begin{pmatrix}
            %
            N/\sigma^2 & 0 \\
            %
            0 & N/(2\sigma^4)
            %
        \end{pmatrix}.
        %
    \end{equation*}
    %
    La media e la varianza campione
    %
    \begin{equation*}
        %
        \bar x(\mathbf x) = \frac 1N \sum_{i=1}^N x_i, \qquad
        \hat V(\mathbf x) = \frac 1{N-1} \sum_{i=1}^N (x_i - \bar x)^2
    \end{equation*}
    %
    sono uno stimatore sufficiente di $(\mu,\sigma^2)$ (vedi
    \autoref{ex:suffgauss}). Il bias di $\bar x$ è zero, calcoliamo quello di
    $\hat V$:
    %
    \begin{align*}
        %
        E[\hat V(\mathbf x)]
        &= E\left[ \frac 1{N-1} \sum_{i=1}^N (x_i - \bar x)^2 \right] = \\
        %
        &= \frac 1{N-1} \sum_{i=1}^N
        E\left[\left(x_i - \frac1N \sum_{j=1}^N x_j\right)^2\right] = \\
        %
        &= \frac 1{N-1} \sum_{i=1}^N \left(
        E[x_i^2]
        - 2 E\left[ x_i \frac1N \sum_{j=1}^N x_j \right] + {}\right. \\
        %
        &\qquad\left.
        + E\left[ \frac1N \sum_{j=1}^N x_j \frac1N \sum_{j=k}^N x_k \right]
        \right) = \\
        %
        &= \frac1{N-1} \sum_{i=1}^N (\var[x_i] + E[x_i]^2) + {} \\
        %
        &\qquad - \frac2{N(N-1)} \sum_{i,j=1}^N (\cov[x_i, x_j] + E[x_i]E[x_j]) + {} \\
        %
        &\qquad + \frac1{N^2(N-1)} \sum_{i,j,k=1}^N (\cov[x_j, x_k] + E[x_j]E[x_k]) = \\
        %
        &= \frac N{N-1} (\sigma^2 + \mu^2)
        - \frac2{N-1} \sigma^2 - \frac{2N}{N-1} \mu^2 + {} \\
        %
        &\qquad + \frac1{N-1} \sigma^2 + \frac N{N-1} \mu^2 = \\
        %
        &= \sigma^2,
        %
    \end{align*}
    %
    quindi è unbiased. Poiché $I$ è diagonale e il bias è nullo, per applicare
    Cramér-Rao basta confrontare le varianze. La varianza di $\bar x$ è
    $\sigma^2/N = I_{\mu\mu}^{-1}$, mentre quella di $\hat V$ è
    $2\sigma^4/(N-1)$ (vedi \autoref{ex:varvar}) diversa da
    $I_{\sigma^2\sigma^2}^{-1}$.
    %
    Notiamo che nel conto di $E[\hat V]$ non abbiamo usato il fatto che la
    distribuzione è gaussiana, quindi $\hat V$ è uno stimatore unbiased della
    varianza per qualunque distribuzione.
    %
\end{solution}

% parte spostata da 25 ottobre
\subsection{Riduzione del bias}

Sia $\hat\theta(x)$ uno stimatore con bias $b(\theta)$.
Lo stimatore
\begin{equation*}
	\hat\theta^{(u)}(x)
	\is \hat\theta(x) - b(\theta)
\end{equation*}
ha bias nullo,
però questa formula non si può usare in pratica perché dipende dal valore vero $\theta$.
Si può tuttavia cercare uno stimatore del bias $\hat b(x)$.
Consideriamo un modello per $N$ estrazioni di $x$
e uno stimatore $\hat\theta_N(\mathbf x)$
con bias che ha sviluppo in serie
\begin{equation*}
	b_N(\theta)
	= \frac{\alpha(\theta)}N + O(N^{-2}).
\end{equation*}
Stimiamo il bias di $\hat\theta$ con la media artimetica su due sottoinsiemi dei dati meno lui stesso:
\begin{equation*}
	\hat\theta^{(u)}_N(\mathbf x)
	\is 2\hat\theta_N(\mathbf x)
	- \frac12 \left( \hat\theta_{N/2}(x_1,\dots,x_{N/2}) + \hat\theta_{N/2}(x_{1+N/2},\dots,x_N) \right).
\end{equation*}
Calcoliamo il nuovo bias:
\begin{align*}
	E[\hat\theta^{(u)}_N]
	&= 2 \left( \theta + \frac{\alpha(\theta)}N + O(N^{-2}) \right)
	- \frac12 \left( 2\theta + 2 \frac{\alpha(\theta)}{N/2} + O(N^{-2}) \right) = \\
	&= \theta + O(N^{-2}).
\end{align*}
Quindi il bias di $\hat\theta^{(u)}$ è all'ordine successivo in $N$.

