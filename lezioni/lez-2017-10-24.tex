% Giacomo Petrillo
% lezione di Punzi

\section{Stima puntuale}

\begin{definition}[Stima]
	Dato un modello con parametro $\theta$,
	una \emph{stima di $\theta$} è una statistica a valori nello spazio di $\theta$.
\end{definition}

\begin{definition}[Consistenza]
	Dato un modello con parametro $\theta$ per $N$ estrazioni di $x$
	e una stima $\hat\theta_N$ di $\theta$,
	la stima è \emph{consistente}
	se per $N\to\infty$ converge debolmente al \emph{valore vero},
	cioè il valore del parametro in cui calcoliamo la probabilità:
	\begin{equation*}
		\forall\epsilon\, \lim_{N\to\infty} P(|\hat\theta_N(\mathbf x)-\theta| > \epsilon;\theta) = 0.
	\end{equation*}
\end{definition}

\begin{definition}[Bias]
	Dato un modello con parametro $\theta$ e una stima $\hat\theta$ di $\theta$,
	il \emph{bias} è la differenza tra la media della stima e il valore vero:
	\begin{equation*}
		b(\theta) \is E[\hat\theta;\theta] - \theta.
	\end{equation*}
	Una stima si dice \emph{corretta} se il bias è nullo;
	\emph{asintoticamente corretta} se il modello è per $N$ estrazioni
	e nel limite $N\to\infty$ il bias tende a zero.
\end{definition}

\subsection{Efficienza}

\begin{theorem}[Disuguaglianza di Cramer-Rao]
	\label{th:cramerrao}
	Consideriamo un modello con parametro $\theta\in\R$.
	Sia $\hat\theta$ una stima con bias~$b$
	la cui pdf ha supporto che non dipende da~$\theta$.
	Allora c'è un limite inferiore alla varianza della stima:
	\begin{equation*}
		\var[\hat\theta;\theta] \ge \frac{\left( 1 + \frac{\partial b}{\partial\theta} \right)^2}{I_{\hat\theta}(\theta)},
	\end{equation*}
	dove $I$ è l'informazione di Fisher.
\end{theorem}

\begin{proof}
	Sia la varianza che l'informazione di Fisher sono degli integrali, quindi,
	se riusciamo a scrivere il termine $1+b'(\theta)$ come un integrale opportuno,
	possiamo usare la disguaglianza di Schwartz,
	che scritta con i valori attesi è
	\begin{equation*}
		E[fg]^2 \le E[f^2]E[g^2].
	\end{equation*}
	Scriviamo la varianza e l'informazione di Fisher,
	definendo per comodità $L \is p(\hat\theta;\theta)$:
	\begin{align*}
		\var[\hat\theta;\theta]
		&= \int\de\hat\theta\, L (\hat\theta-E[\hat\theta;\theta])^2, \\
		I_{\hat\theta}(\theta)
		&= \int\de\hat\theta\, L \left( \pdv{\log L}\theta \right)^2.
	\end{align*}
	Quindi per usare Schwartz dobbiamo mostrare che $1+b'(\theta)$ è uguale a
	\begin{equation*}
		\int\de\hat\theta\, L (\hat\theta-E[\hat\theta;\theta]) \pdv{\log L}\theta.
	\end{equation*}
	Calcoliamo il primo termine:
	\begin{align*}
		\int \de\hat\theta\,
		L \hat\theta \pdv{\log L}\theta 
		&= \int \de\hat\theta\,
		\pdv L\theta \hat\theta
		= \int \de\hat\theta\,
		\pdv{}\theta (\hat\theta L)
		= \pdv{}\theta \int \de\hat\theta\, \hat\theta L = \\
		&= \pdv{}\theta E[\hat\theta;\theta]
		= \pdv{}\theta (\theta + b(\theta)) = \\
		&= 1 + b'(\theta),
	\end{align*}
	vediamo che il secondo termine è nullo:
	\begin{align*}
		\int \de\hat\theta\,
		L\cdot E[\hat\theta;\theta] \pdv{\log L}\theta
		&= E[\hat\theta;\theta] \int \de\hat\theta\, \pdv L\theta = \\
		&= E[\hat\theta;\theta] \pdv{}\theta \int \de\hat\theta\, L = \\
		&= 0. \qedhere
	\end{align*}
\end{proof}

Per il \autoref{th:fishstat} l'informazione di Fisher di $x$ è maggiore o uguale a quella di $\hat\theta$,
quindi si ha il corollario che
\begin{equation*}
	\var[\hat\theta;\theta] \ge \frac{\left( 1 + \frac{\partial b}{\partial\theta} \right)^2}{I_x(\theta)};
\end{equation*}
nel caso di bias che dipende poco dal parametro ($|\partial b/\partial\theta| \ll 1$)
questo limite inferiore, detto \emph{minimum variance bound}, non dipende dallo stimatore.

Nel caso di informazione di Fisher proporzionale al numero di estrazioni otteniamo
\begin{equation*}
	\var[\hat\theta] \ge O \left(\frac1N\right),
\end{equation*}
questa è la forma generale dell'osservazione che comunemente
<<l'errore cala con la radice del numero di misure>>.

\begin{definition}[Efficienza]
	Con la notazione del \autoref{th:cramerrao} e con $x$ variabile del modello,
	l'\emph{efficienza di $\hat\theta$} è
	\begin{equation*}
		\frac{\left( 1 + \frac{\partial b}{\partial\theta} \right)^2}{I_x(\theta)\var[\hat\theta;\theta]},
	\end{equation*}
	che nelle ipotesi del teorema è minore o uguale a~1.
\end{definition}

\begin{theorem}
	\label{th:suffeff}
	Con la notazione e le ipotesi del \autoref{th:cramerrao},
	se $\hat\theta$ è sufficiente per $\theta$ allora ha efficienza unitaria.
	\marginpar{Il Del Prete 2010-01-21 a pagina 177 (in fondo)
	dice che efficiente $\rightarrow$ sufficiente e che non vale il viceversa. \\
	\emph{Sì, e due pagine dopo <<stressa>> che la likelihood è il posteriore!}}
\end{theorem}

\begin{proof}
	Nella dimostrazione della disuguaglianza di Cramer-Rao (\autoref{th:cramerrao})
	abbiamo usato la disuguaglianza di Schwartz,
	che è saturata se le due funzioni sono proporzionali,
	cioè se per una qualche funzione $g$
	\begin{equation}
		\label{eq:dethetalogp}
		\pdv{}{\theta}\log p(\hat\theta;\theta) = g(\theta) (\hat\theta-E[\hat\theta;\theta]).
	\end{equation}
	Poiché $\hat\theta$ è sufficiente, $I_{\hat\theta}(\theta)=I_x(\theta)$,
	quindi dalla saturazione di Cramer-Rao si ottiene l'efficienza unitaria.
	Cerchiamo di calcolare $\partial_\theta\log p(\hat\theta;\theta)$.
	Per il teorema di Darmois (\autoref{th:darmois}),
	la pdf di $x$ si scrive come
	\begin{equation*}
		p(x;\theta)
		= \exp \left( \hat\theta(x)a(\theta) + \beta(x) + \gamma(\theta) \right).
	\end{equation*}
	Per l'ipotesi di sufficienza, la pdf di $x$ si fattorizza con quella di $\hat\theta$:
	\begin{equation*}
		p(x;\theta)
		= f(x) p(\hat\theta(x);\theta), \quad f > 0,
	\end{equation*}
	quindi, per qualsiasi $x\in\hat\theta^{-1}(\hat\theta_0)$, si ha
	\begin{align*}
		p(\hat\theta_0;\theta)
		&= \frac{p(x;\theta)}{f(x)} = \\
		&= \exp \left( \hat\theta_0 a(\theta) + \beta(x) + \gamma(\theta) - \log f(x) \right).
	\end{align*}
	Poiché l'espressione dipende solo da $\theta_0$ (e $\theta$),
	possiamo definire $B(\hat\theta_0)\is\beta(x)-\log f(x)$,
	quindi infine
	\begin{equation*}
		p(\hat\theta;\theta)
		= \exp \left( \hat\theta a(\theta) + B(\hat\theta) + \gamma(\theta) \right).
	\end{equation*}
	La derivata del logaritmo è
	\begin{align*}
		\pdv{}\theta \log p(\hat\theta;\theta)
		&= \hat\theta a'(\theta) + \gamma'(\theta),
	\end{align*}
	quindi per far tornare la \eqref{eq:dethetalogp} bisogna che
	\begin{equation*}
		\gamma'(\theta) = -a'(\theta) E[\hat\theta;\theta]
	\end{equation*}
	con $g(\theta)=a'(\theta)$.
	Calcoliamo:
	\begin{align*}
		a'(\theta) E[\hat\theta;\theta]
		&= a'(\theta) \int\de\hat\theta\, p(\hat\theta;\theta) \hat\theta = \\
		&= \int\de\hat\theta\, \hat\theta a'(\theta) e^{\hat\theta a(\theta)+B(\hat\theta)+\gamma(\theta)} = \\
		&= \int\de\hat\theta\, \pdv{}\theta \big(e^{\hat\theta a(\theta)+B(\hat\theta)}\big) e^{\gamma(\theta)} = \\
		&= \pdv{}\theta \left( \int\de\hat\theta\, p(\hat\theta;\theta) e^{-\gamma(\theta)} \right) e^{\gamma(\theta)} = \\
		&= \pdv{}\theta \big(e^{-\gamma(\theta)})e^{\gamma(\theta)} = \\
		&= -\gamma'(\theta). \qedhere
	\end{align*}
\end{proof}

Questo teorema non dice nulla sul bias di $\hat\theta$,
però ha bias nullo se lo consideriamo come stimatore della sua media,
che è una funzione del parametro.

\begin{example}
	Consideriamo la gaussiana a varianza fissata
	\begin{equation*}
		p(x;\mu)
		= \frac1{\sqrt{2\pi}} e^{-\frac12 (x-\mu)^2}.
	\end{equation*}
	L'informazione di Fisher\footnote{Vedi \autoref{th:fishgauss}.} per $N$ estrazioni è $N$.
	La media aritmetica è uno stimatore sufficiente e con bias nullo,
	e infatti la sua deviazione standard è $1/N$.
\end{example}

\begin{example}
	Consideriamo la gaussiana a media fissata
	\begin{equation*}
		p(x;\sigma)
		= \frac1{\sqrt{2\pi}\sigma} e^{-\frac12 \frac{x^2}{\sigma^2}}.
	\end{equation*}
	Lo scarto quadratico medio
	\begin{equation*}
		\hat\sigma^2(\mathbf x)
		\is \frac1N \sum_{i=1}^N x_i^2
	\end{equation*}
	è sufficiente e l'informazione di Fisher per $\sigma^2$ è $N/(2\sigma^4)$.
	Calcoliamo il valore atteso:
	\begin{align*}
		E[\hat\sigma^2]
		&= E \left[ \frac 1N \sum x^2 \right] = \\
		&= \frac 1N \sum E[x^2] = \\
		&= \sigma^2,
	\end{align*}
	quindi ha bias nullo.
	La varianza deve allora essere $1/I(\sigma^2)=2\sigma^4/N$, verifichiamolo:
	\begin{align*}
		E[\hat\sigma^4]
		&= E \left[ \frac1{N^2}\left(\sum_ix_i^2\right)^2 \right] = \\
		&= \frac1{N^2} \sum_{ij} E[x_i^2x_j^2] = \\
		&= \frac1{N^2} \left( \sum_{i=j} E[x_i^4] + \sum_{i\neq j} E[x_i^2]E[x_j^2] \right) = \\
		\Bigg[ E[x^4]
		&= \frac{\sigma^4}{\sqrt{2\pi}} \int\de x\, e^{-\frac12 x^2} x^4 = \\
		&= - \frac{\sigma^4}{\sqrt{2\pi}} \int\de x\, \dv{}{x} \left(e^{-\frac12x^2}\right) x^3
		= 3 \sigma^4 \Bigg] \\
		&= \frac1{N^2} \left( 3N + \frac{N(N-1)}2 \right) \sigma^4 = \\
		&= \frac{N+2}N \sigma^4,
	\end{align*}
	\begin{equation*}
		\var[\hat\sigma^2]
		= E[\hat\sigma^4] - E[\hat\sigma^2]^2
		= \frac{2\sigma^4}N.
	\end{equation*}
	Dunque $\hat\sigma^2$ è uno stimatore corretto ed efficiente di $\sigma^2$.
	Si può mostrare invece che $\hat\sigma$ come stimatore di $\sigma$ ha bias non nullo
	ma che non dipende da $\sigma$,
	quindi calcolato il bias lo si può sottrarre e per il \autoref{th:suffeff} $\hat\sigma$ rimane efficiente,
	quindi dall'informazione di Fisher e dal bias si ricava la varianza.
	\marginpar{Sul quaderno alla data 2017-11-23 ho scritto il conto del bias, magari metterlo in appendice.
	Forse il conto è più veloce dando per buona la distribuzione del $\chi^2$. \\
	\emph{Sul Del Prete a pagina 177 dice che non c'è uno stimatore efficiente di $\sigma$.}}
\end{example}

% parte spostata da 25 ottobre
\subsection{Riduzione del bias}

Sia $\hat\theta(x)$ uno stimatore con bias $b(\theta)$.
Lo stimatore
\begin{equation*}
	\hat\theta^{(u)}(x)
	\is \hat\theta(x) - b(\theta)
\end{equation*}
ha bias nullo,
però questa formula non si può usare in pratica perché dipende dal valore vero $\theta$.
Si può tuttavia cercare uno stimatore del bias $\hat b(x)$.
Consideriamo un modello per $N$ estrazioni di $x$
e uno stimatore $\hat\theta_N(\mathbf x)$
con bias che ha sviluppo in serie
\begin{equation*}
	b_N(\theta)
	= \frac{\alpha(\theta)}N + O(N^{-2}).
\end{equation*}
Stimiamo il bias di $\hat\theta$ con la media artimetica su due sottoinsiemi dei dati meno lui stesso:
\begin{equation*}
	\hat\theta^{(u)}_N(\mathbf x)
	\is 2\hat\theta_N(\mathbf x)
	- \frac12 \left( \hat\theta_{N/2}(x_1,\dots,x_{N/2}) + \hat\theta_{N/2}(x_{1+N/2},\dots,x_N) \right).
\end{equation*}
Calcoliamo il nuovo bias:
\begin{align*}
	E[\hat\theta^{(u)}_N]
	&= 2 \left( \theta + \frac{\alpha(\theta)}N + O(N^{-2}) \right)
	- \frac12 \left( 2\theta + 2 \frac{\alpha(\theta)}{N/2} + O(N^{-2}) \right) = \\
	&= \theta + O(N^{-2}).
\end{align*}
Quindi il bias di $\hat\theta^{(u)}$ è all'ordine successivo in $N$.

