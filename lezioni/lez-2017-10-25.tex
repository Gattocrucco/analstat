% Giacomo Petrillo
% non c'ero a lezione quindi uso gli appunti di Francesco Serra
% lezione di Punzi

\subsection{Metodo dei momenti}

Consideriamo la statistica $s_k(x)\is x^k$.
Il valore atteso è per definizione il momento $k$-esimo
$E[s_k]=\mu_k$,
quindi $s_k$ è uno stimatore con bias nullo di $\mu_k$.
Verifichiamo che sia consistente;
per $N$ estrazioni lo estendiamo alla media aritmetica (che lascia invariato il bias):
\begin{align*}
	s_k(\mathbf x)
	\is \frac{\sum_i x_i^k}N, \quad
	E[s_k(\mathbf x)]
	= \frac1N \sum_i E[x_i^k]
	= \mu_k,
\end{align*}
e calcoliamo la varianza:
\begin{align*}
	E[s_k^2]
	&= \frac1{N^2} E \left[ \sum_{ij} x_i^k x_j^k \right] = \\
	&= \frac1{N^2} \sum_{ij} E[x_i^k x_j^k] = \\
	&= \frac1{N^2} \left( \sum_{i=j} E[x_i^{2k}] + \sum_{i\neq j} E[x_i^k] E[x_j^k] \right) = \\
	&= \frac1{N^2} (N\mu_{2k} + N(N-1)\mu_k^2), \\
	\var[s_k]
	&= E[s_k^2] - E[s_k]^2 = \\
	&= \frac{\mu_{2k} - \mu_k^2}N
	\propto \frac1N.
\end{align*}
In base al seguente teorema, se la varianza tende a zero lo stimatore è consistente:
\begin{theorem}[Disuguaglianza di Čebyšëv]
	Per una variabile $x$ con media $\mu$ e varianza $\sigma^2$ definite, per $k$ qualsiasi vale
	\begin{equation*}
		P(|x-\mu| \ge k\sigma) \le \frac1{k^2}.
	\end{equation*}
\end{theorem}
\begin{proof}
	\begin{align*}
		P(|x-\mu| \ge k\sigma)
		&= \int_{|x-\mu| \ge k\sigma} \de x\, p(x) = \\
		&= \int_{\left( \frac {x-\mu} {k\sigma} \right)^2 \ge 1} \de x\, p(x) \le \\
		&\le \int \de x\, p(x) \left( \frac {x-\mu} {k\sigma} \right)^2 = \\
		&= \frac 1{k^2} \frac{E[(x-\mu)^2]}{\sigma^2} = \frac 1 {k^2}. \qedhere
	\end{align*}
\end{proof}
I momenti sono quindi un modo generale di parametrizzare una distribuzione in modo che esista uno stimatore corretto, consistente e semplice da calcolare.
Tuttavia tipicamente il problema specifico fisserà quali sono i parametri che vogliamo stimare.

\marginpar{spiegare che poi in pratica non si usa perché la varianza esplode al
crescere di $k$ (fare grafico per la gaussiana di $N(k)$ necessario per avere
un errore del 10 \% su $\mu_k$), e menzionare che se voglio ridisegnare una
distribuzione a partire dai momenti devo usare massima entropia, citare
Abramov.}

\subsection{Massima likelihood}
\label{sec:ml}

\begin{definition}
	Dato un modello $p(x;\theta)$,
	se per ogni $x$ esiste un unico $\hat\theta(x)$ che massimizza $p(x;\theta)$,
	allora è definito lo \emph{stimatore di massima likelihood} (\emph{MLE})~$\hat\theta(x)$.
\end{definition}

\begin{theorem}
	\label{teo:mle}
	Dato un modello $p(x;\theta)$ il cui supporto non dipende dal parametro,
	se è definito lo stimatore di massima likelihood
	e il punto di massimo è anche un punto stazionario,
	allora lo stimatore di massima likelihood è consistente
	ed è asintoticamente (nel numero $N$ di estrazioni) gaussiano ed efficiente,
	con bias all'ordine $N^{-1}$.
\end{theorem}

\begin{proof}
	\marginpar{Questa dimostrazione va sistemata.
	Possibile referenza: James e Frederick, \emph{Data analysis for experimental physics}.}
	Fissiamo un valore vero $\theta_0$.
	Sia $f(x,\theta)$ tale che la funzione
	\begin{equation*}
		h(\theta)\is E[f(x,\theta);\theta_0]
	\end{equation*}
	si annulla solo in $\theta_0$.
	Consideriamo $N$ estrazioni.
	Per la legge dei grandi numeri (\autoref{th:grossnum}),
	la media aritmetica $\sum_i f(x_i,\theta)/N$ converge in probabilità a $h(\theta)$.
	Sia $s_N(\mathbf x)$ una statistica tale che
	\begin{equation*}
		\forall N \,\forall\mathbf x: \frac{\sum_i f(x_i,s_N(\mathbf x))}N = 0,
	\end{equation*}
	allora $s_N$ converge in probabilità a $\theta_0$.
	\marginpar{Ipotesi tecniche su $f$?}%
	Ora esibiamo una $f$ e una $s_N$ con queste proprietà:
	\begin{align*}
		0
		&= \pdv{}{\theta} \int\de x\, p(x;\theta) = \\
		&= \int\de x\, \pdv{}{\theta} p(x;\theta) = \\
		&= \int\de x\, p(x;\theta) \pdv{}{\theta} \log p(x;\theta) = \\
		&= E \left[ \pdv{}{\theta} \log p(x;\theta);\theta \right],
	\end{align*}
	quindi $\partial_\theta \log p(x;\theta)$ ha le proprietà richieste per $f$,
	assumendo che lo zero del valore atteso sia unico.
	\marginpar{Eh, è unico?}
	Definiamo
	\begin{equation*}
		K_{\mathbf x}(\theta)
		\is \frac{\sum_i \pdv{}{\theta} \log p(x_i;\theta)} N.
	\end{equation*}
	Per ipotesi lo stimatore ML $\hat\theta$ è un punto stazionario di $p(\mathbf x,\theta)$.
	Allora
	\begin{align*}
		0
		&= \pdv{}{\theta} \log p(\mathbf x;\theta) \big|_{\hat\theta} = \\
		&= \pdv{}{\theta} \sum_i \log p(x_i;\theta) \big|_{\hat\theta} \implies \\
		\implies 0
		&= K_{\mathbf x}(\hat\theta),
	\end{align*}
	quindi $\hat\theta$ soddisfa le proprietà richieste per $s_N$.
	Notiamo che
	\begin{align*}
		E \left[ K_{\mathbf x}'(\theta_0) \right]
		&= E \left[ \Pdv{}{\theta} \log p(x;\theta) \big|_{\theta_0} \right] = \\
		&= -I_x(\theta_0)
		\le 0,
	\end{align*}
	che è coerente con il fatto che $\hat\theta$ è un massimo.
	Abbiamo dunque che $\hat\theta$ è consistente.
	\marginpar{Del Prete a pagina 183
	dice che se esiste uno stimatore sufficiente/efficiente,
	ML è sufficiente/efficiente (anche non asintoticamente),
	ma le dimostrazioni mi sembrano bacate. \\
	\emph{La dimostrazione della sufficienza non l'ho capita,
	però si dimostra usando Darmois.
	La dimostrazione dell'efficienza invece mi sembra richieda la sufficienza.}}%
	Consideriamo l'espansione in serie di $K$:
	\begin{align*}
		0 = K(\hat\theta)
		&= K(\theta_0) + K'(\theta_0)(\hat\theta-\theta_0) + O\big((\hat\theta-\theta_0)^2\big) \implies \\
		\implies \hat\theta
		&= \theta_0 + \frac{K(\hat\theta)}{-K'(\theta_0)} + O\big((\hat\theta-\theta_0)^2\big).
	\end{align*}
	Per il teorema del limite centrale (\autoref{th:limitecentrale}),
	per $N\to\infty$
	la distribuzione di $K(\hat\theta)$ converge a una gaussiana
	con media 0 e varianza $I/N$ dove $I\is I_x(\theta_0)$.
	Inoltre abbiamo visto che $K'(\theta_0)$ converge in probabilità a $-I$,
	quindi la distribuzione di $\hat\theta$ converge a una gaussiana
	con media $\theta_0$ e varianza $1/(IN)$.
	Il termine all'ordine quadratico converge a un ordine della varianza,
	quindi il bias è $O(1/N)$.
	Notiamo che il rapporto tra il bias e la deviazione standard va a zero come $1/\sqrt{N}$.
\end{proof}

\begin{example}
	Consideriamo la gaussiana
	\begin{equation*}
		p(\mathbf x;\mu,\sigma)
		= \frac1{(\sqrt{2\pi}\,\sigma)^N} \exp \left( -\frac12\sum_i\left(\frac{x_i-\mu}\sigma\right)^2 \right).
	\end{equation*}
	Calcoliamo lo stimatore di massima likelihood:
	\begin{align*}
		L \is \log p(\mathbf x;\mu,\sigma)
		&= (\dots) - N\log\sigma - \frac12 \sum_i \left( \frac{x_i-\mu}\sigma \right)^2 \\
		0 = \pdv L\mu
		&= -\frac1{\sigma^2}\sum_i (x_i-\mu) \implies \\
		\implies \mu
		&= \bar x \\
		0 = \pdv L\sigma
		&= -\frac N\sigma + \frac1{\sigma^3}\sum_i (x_i-\mu)^2 \implies \\
		\implies \sigma^2
		&= \frac1N \sum_i (x_i-\bar x)^2.
	\end{align*}
	Le proprietà di $\hat\mu$ le sappiamo.
	Vediamo il valore atteso di $\hat\sigma^2$: abbiamo già calcolato
    quello della varianza campione $\hat V = N/(N-1)\hat\sigma$ (vedi
    \autoref{ex:cramerraond}), quindi
    %
    \begin{equation*}
        %
        E[\hat\sigma(\mathbf x)]
        = \frac{N-1}N E[\hat V(\mathbf x)]
        = \frac{N-1}N \sigma^2,
    \end{equation*}
    %
    che tende a $\sigma^2$ per $N\to\infty$.
    %
\end{example}

\marginpar{Mettere come esercizio il conto della varianza della varianza fatto
nel modo furbo che c'è su mathexchange.}

% parte sulla riduzione del bias spostata al 24 ottobre
