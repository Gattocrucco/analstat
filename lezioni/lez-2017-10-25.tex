% Giacomo Petrillo
% non c'ero a lezione quindi uso gli appunti di Francesco Serra
% lezione di Punzi

\subsection{Metodo dei momenti}

Consideriamo la statistica $s_k(x)\is x^k$.
Il valore atteso è per definizione il momento $k$-esimo
$E[s_k]=\mu_k$,
quindi $s_k$ è uno stimatore con bias nullo di $\mu_k$.
Verifichiamo che sia consistente;
\marginpar{Spiegare perché se la varianza va a zero allora è consistente
(disuguaglianza di Čebichev).}
per $N$ estrazioni lo estendiamo alla media aritmetica (che lascia invariato il bias):
\begin{align*}
	s_k(\mathbf x)
	\is \frac{\sum_i x_i^k}N, \quad
	E[s_k(\mathbf x)]
	= \frac1N \sum_i E[x_i^k]
	= \mu_k,
\end{align*}
e calcoliamo la varianza:
\begin{align*}
	E[s_k^2]
	&= \frac1{N^2} E \left[ \sum_{ij} x_i^k x_j^k \right] = \\
	&= \frac1{N^2} \sum_{ij} E[x_i^k x_j^k] = \\
	&= \frac1{N^2} \left( \sum_{i=j} E[x_i^{2k}] + \sum_{i\neq j} E[x_i^k] E[x_j^k] \right) = \\
	&= \frac1{N^2} (N\mu_{2k} + N(N-1)\mu_k^2), \\
	\var[s_k]
	&= E[s_k^2] - E[s_k]^2 = \\
	&= \frac{\mu_{2k} - \mu_k^2}N
	\propto \frac1N.
\end{align*}
I momenti sono quindi un modo generale di parametrizzare una distribuzione in modo che esista uno stimatore corretto, consistente e semplice da calcolare.
Tuttavia tipicamente il problema specifico fisserà quali sono i parametri che vogliamo stimare.

\subsection{Massima likelihood}
\label{sec:ml}

\begin{definition}
	Dato un modello $p(x;\theta)$,
	se per ogni $x$ esiste un unico $\hat\theta(x)$ che massimizza $p(x;\theta)$,
	allora è definito lo \emph{stimatore di massima likelihood} (\emph{MLE})~$\hat\theta(x)$.
\end{definition}

\begin{theorem}
	Dato un modello $p(x;\theta)$ il cui supporto non dipende dal parametro,
	se è definito lo stimatore di massima likelihood
	e il punto di massimo è anche un punto stazionario,
	allora lo stimatore di massima likelihood è consistente
	ed è asintoticamente (nel numero $N$ di estrazioni) gaussiano ed efficiente,
	con bias all'ordine $N^{-1}$.
\end{theorem}

\begin{proof}
	Fissiamo un valore vero $\theta_0$.
	Sia $f(x,\theta)$ tale che la funzione
	\begin{equation*}
		h(\theta)\is E[f(x,\theta);\theta_0]
	\end{equation*}
	si annulla solo in $\theta_0$.
	Consideriamo $N$ estrazioni.
	Per la legge dei grandi numeri (\autoref{th:grossnum}),
	la media aritmetica $\sum_i f(x_i,\theta)/N$ converge in probabilità a $h(\theta)$.
	Sia $s_N(\mathbf x)$ una statistica tale che
	\begin{equation*}
		\forall N \,\forall\mathbf x: \frac{\sum_i f(x_i,s_N(\mathbf x))}N = 0,
	\end{equation*}
	allora $s_N$ converge in probabilità a $\theta_0$.
	\marginpar{Ipotesi tecniche su $f$?}%
	Ora esibiamo una $f$ e una $s_N$ con queste proprietà:
	\begin{align*}
		0
		&= \pdv{}{\theta} \int\de x\, p(x;\theta) = \\
		&= \int\de x\, \pdv{}{\theta} p(x;\theta) = \\
		&= \int\de x\, p(x;\theta) \pdv{}{\theta} \log p(x;\theta) = \\
		&= E \left[ \pdv{}{\theta} \log p(x;\theta);\theta \right],
	\end{align*}
	quindi $\partial_\theta \log p(x;\theta)$ ha le proprietà richieste per $f$,
	assumendo che lo zero del valore atteso sia unico.
	\marginpar{Eh, è unico?}
	Definiamo
	\begin{equation*}
		K_{\mathbf x}(\theta)
		\is \frac{\sum_i \pdv{}{\theta} \log p(x_i;\theta)} N.
	\end{equation*}
	Per ipotesi lo stimatore ML $\hat\theta$ è un punto stazionario di $p(\mathbf x,\theta)$.
	Allora
	\begin{align*}
		0
		&= \pdv{}{\theta} \log p(\mathbf x;\theta) \big|_{\hat\theta} = \\
		&= \pdv{}{\theta} \sum_i \log p(x_i;\theta) \big|_{\hat\theta} \implies \\
		\implies 0
		&= K_{\mathbf x}(\hat\theta),
	\end{align*}
	quindi $\hat\theta$ soddisfa le proprietà richieste per $s_N$.
	Notiamo che
	\begin{align*}
		E \left[ K_{\mathbf x}'(\theta_0) \right]
		&= E \left[ \Pdv{}{\theta} \log p(x;\theta) \big|_{\theta_0} \right] = \\
		&= -I_x(\theta_0)
		\le 0,
	\end{align*}
	che è coerente con il fatto che $\hat\theta$ è un massimo.
	Abbiamo dunque che $\hat\theta$ è consistente.
	\marginpar{Del Prete a pagina 183
	dice che se esiste uno stimatore sufficiente/efficiente,
	ML è sufficiente/efficiente (anche non asintoticamente),
	ma le dimostrazioni mi sembrano bacate. \\
	\emph{La dimostrazione della sufficienza non l'ho capita,
	però si dimostra usando Darmois.
	La dimostrazione dell'efficienza invece mi sembra richieda la sufficienza,
	ma abbiamo già che sufficiente $\rightarrow$ efficiente.}}%
	Consideriamo l'espansione in serie di $K$:
	\begin{align*}
		0 = K(\hat\theta)
		&= K(\theta_0) + K'(\theta_0)(\hat\theta-\theta_0) + O\big((\hat\theta-\theta_0)^2\big) \implies \\
		\implies \hat\theta
		&= \theta_0 + \frac{K(\hat\theta)}{-K'(\theta_0)} + O\big((\hat\theta-\theta_0)^2\big).
	\end{align*}
	Per il teorema del limite centrale (\autoref{th:limitecentrale}),
	per $N\to\infty$
	la distribuzione di $K(\hat\theta)$ converge a una gaussiana
	con media 0 e varianza $I/N$ dove $I\is I_x(\theta_0)$.
	Inoltre abbiamo visto che $K'(\theta_0)$ converge in probabilità a $-I$,
	quindi la distribuzione di $\hat\theta$ converge a una gaussiana
	con media $\theta_0$ e varianza $1/(IN)$.
	Il termine all'ordine quadratico converge a un ordine della varianza,
	quindi il bias è $O(1/N)$.
	Notiamo che il rapporto tra il bias e la deviazione standard va a zero come $1/\sqrt{N}$.
\end{proof}

\begin{example}
	Consideriamo la gaussiana
	\begin{equation*}
		p(\mathbf x;\mu,\sigma)
		= \frac1{(\sqrt{2\pi}\sigma)^N} \exp \left( -\frac12\sum_i\left(\frac{x_i-\mu}\sigma\right)^2 \right).
	\end{equation*}
	Calcoliamo lo stimatore di massima likelihood:
	\begin{align*}
		L \is \log p(\mathbf x;\mu,\sigma)
		&= (\dots) - N\log\sigma - \frac12 \sum_i \left( \frac{x_i-\mu}\sigma \right)^2 \\
		0 = \pdv L\mu
		&= -\frac1{\sigma^2}\sum_i (x_i-\mu) \implies \\
		\implies \mu
		&= \bar x \\
		0 = \pdv L\sigma
		&= -\frac N\sigma + \frac1{\sigma^3}\sum_i (x_i-\mu)^2 \implies \\
		\implies \sigma^2
		&= \frac1N \sum_i (x_i-\bar x)^2.
	\end{align*}
	Le proprietà di $\hat\mu$ sono ovvie.
	Calcoliamo il bias di $\hat\sigma^2$:
	\begin{align*}
		E[\hat\sigma^2]
		&= \frac{\sum_i}N E[x_i^2 - 2x_i\bar x + \bar x^2] = \\
		\Bigg[ E[x_i]^2
		&= \var[x_i] + E[x_i]^2
		= \mu^2 + \sigma^2 \\
		E[x_i\bar x]
		&= \frac 1N E \left[ x_i^2 + x_i \sum_{j\neq i} x_j \right] = \\
		&= \frac{\mu^2 + \sigma^2 + (N-1)\mu^2}N
		= \mu^2 + \frac{\sigma^2}N \\
		E[\bar x^2]
		&= \mu^2 + \frac{\sigma^2}N \Bigg] \\
		&= \mu^2 + \sigma^2 - \mu^2 - \frac{\sigma^2}N = \\
		&= \frac{N-1}N \sigma^2,
	\end{align*}
	quindi per avere uno stimatore unbiased devo usare il noto
	\begin{equation*}
		\hat V(\mathbf x)
		= \frac{\sum_i (x_i-\bar x)^2}{N-1}.
	\end{equation*}
	Notiamo che nel conto non abbiamo usato il fatto che la distribuzione è gaussiana,
	quindi $\hat V$ è uno stimatore con bias nullo della varianza per qualunque distribuzione.
\end{example}

\paragraph{Riduzione del bias}

Sia $\hat\theta(x)$ uno stimatore con bias $b(\theta)$.
Lo stimatore
\begin{equation*}
	\hat\theta^{(u)}(x)
	\is \hat\theta(x) - b(\theta)
\end{equation*}
ha bias nullo,
però questa formula non si può usare in pratica perché dipende dal valore vero $\theta$.
Si può tuttavia cercare uno stimatore del bias $\hat b(x)$.
Consideriamo un modello per $N$ estrazioni di $x$
e uno stimatore $\hat\theta_N(\mathbf x)$
con bias che ha sviluppo in serie
\begin{equation*}
	b_N(\theta)
	= \frac{\alpha(\theta)}N + O(N^{-2}).
\end{equation*}
Stimiamo il bias di $\hat\theta$ con la media artimetica su due sottoinsiemi dei dati meno lui stesso:
\begin{equation*}
	\hat\theta^{(u)}_N(\mathbf x)
	\is 2\hat\theta_N(\mathbf x)
	- \frac12 \left( \hat\theta_{N/2}(x_1,\dots,x_{N/2}) + \hat\theta_{N/2}(x_{1+N/2},\dots,x_N) \right).
\end{equation*}
Calcoliamo il nuovo bias:
\begin{align*}
	E[\hat\theta^{(u)}_N]
	&= 2 \left( \theta + \frac{\alpha(\theta)}N + O(N^{-2}) \right)
	- \frac12 \left( 2\theta + 2 \frac{\alpha(\theta)}{N/2} + O(N^{-2}) \right) = \\
	&= \theta + O(N^{-2}).
\end{align*}
Quindi il bias di $\hat\theta^{(u)}$ è all'ordine successivo in $N$.
