% Giacomo Petrillo
% lezione di Punzi

\subsection{Likelihood ratio}

Supponiamo di aver costruito una banda di confidenza per la poissoniana che ci dà limiti superiori a $\mu$.
Se poi nell'esperimento otteniamo, ad esempio, $k=10$,
tipicamente preferiremo usare una stima puntuale oppure un intervallo il più corto possibile.
Il problema è che in questo modo, quando usiamo il limite superiore,
il confidence level non è quello che ci aspettiamo.
Infatti, se in base a $k$ scegliamo una stima intervallare diversa,
non stiamo veramente usando quelle stime intervallari,
bensì una nuova statistica definita a tratti,
che avrà proprietà diverse (tipicamente peggiori).
L'errore di scegliere la statistica da usare \emph{dopo}
aver visto i dati è chiamato \emph{flip-flopping}.
In generale, in un'inferenza frequentista,
è necessario scegliere quale statistica calcolare \emph{prima}
di guardare i dati (o una funzione dei dati).
Se usiamo i dati per decidere quale metodo usare,
allora dobbiamo raccogliere nuovi dati su cui applicarlo.

\begin{example}
	Consideriamo una poissoniana \emph{con fondo}
	\begin{equation*}
		p(k;\mu)
		= \frac{(\mu+b)^k}{k!}e^{-(\mu+b)}
	\end{equation*}
	con $b>0$ e $\mu>0$.
	Se, per mettere un limite superiore a $\mu$,
	facciamo una stima intervallare di $\mu+b$ come nell'\autoref{th:poisssup} e poi sottraiamo $b$,
	possiamo ottenere delle stime intervallari vuote.
	Ad esempio, prendiamo $b=5$, $\mathrm{CL}=\SI{90}\%$ e $k=0$,
	si ottiene l'intervallo $(0,2.3)$ per $\mu+b$ che,
	ricordando che $\mu>0$, sottraendo $b$ è vuoto.
\end{example}

L'esempio evidenzia il problema che si possono costruire bande di confidenza
che soddisfano tutte le richieste fatte e che tuttavia non sono soddisfacenti.
Facciamo un altro esempio.

\begin{example}
	Prendiamo una gaussiana con media positiva
	\begin{equation*}
		p(x;\mu)
		= \frac1{\sqrt{2\pi}} e^{-\frac12(x-\mu)^2}, \quad \mu>0.
	\end{equation*}
	Costruiamo la banda di confidenza con il probability ordering a $\mathrm{CL}=\SI{68}\%$,
	che corrisponde a intervalli lungo $x$ di $\pm1$ intorno a $\mu$,
	allora per $x<-1$ otteniamo una stima intervallare vuota.
	Notiamo che questo problema non dipende dal fatto di avere un \emph{bound} a sinistra,
	basta cambiare parametro a $\log\mu$ per usare tutto $\R$ e il problema persiste perché
	le stime intervallari trasformano in modo banale per cambio di parametro.
\end{example}

\begin{example}
	Una persona pensa un numero a caso,
	vogliamo stimarlo con un livello di confidenza del \SI{95}\%.
	Tiriamo un dado a 20 facce,
	se esce 1 diciamo che l'intervallo è $\emptyset$, altrimenti~$\R$.
\end{example}

Fortunatamente esiste un ordinamento che non genera stime vuote e che è invariante per cambio di variabile,
\marginpar{Però non credo sia l'unico invariante per cambio di variabile, nel
caso discreto tutti i cambi (biunivoci) lasciano invariate le stime
intervallari, nel caso continuo posso ad esempio fare una cosa simile al
likelihood ratio ma usando un altro funzionale lineare della likelihood anziché
$\sup_{\theta'} p(x;\theta')$.}
è il \emph{likelihood ratio}\footnote{Vedi Gary J. Feldman, Robert D. Cousins, \emph{A Unified Approach to the Classical Statistical Analysis of Small Signals},  Phys.Rev.D57:3873-3889, 1998 (\url{https://arxiv.org/abs/physics/9711021}).}:
\begin{equation*}
	O(\theta,x) = -\lambda_\theta(x)
	\is -2\log\frac {\sup\limits_{\theta'} p(x;\theta')} {p(x;\theta)}.
\end{equation*}
Inoltre con questo ordinamento la stima intervallare contiene sempre,
se esiste,
la stima di massima likelihood.

Notiamo che per i dati in cui probability ordering avrebbe dato una stima vuota,
il likelihood ratio tenderà a dare regioni piccole.
Può essere desiderabile gonfiare a mano la banda di confidenza in queste regioni
per evitare di triggerare molestatori che vogliono discutere della validità del modello.

\begin{definition}[Pivot]
	Dato un modello $p(x;\theta)$,
	una statistica $g_\theta(x)$ è un \emph{pivot}
	se $p(g_\theta;\theta)$ non dipende da $\theta$.
\end{definition}

\begin{example}
	Consideriamo un \emph{parametro di localizzazione},
	cioè tale che $p(x;\theta)=f(x-\theta)$.
	Allora $g_\theta=x-\theta$ è un pivot:
	$p(g_\theta;\theta) = p(x=g_\theta;0)$.
\end{example}

\begin{fact}[Teorema di Wilks]
	\label{th:wilks}
	Consideriamo un modello per $N$ estrazioni di $x$ con parametro $\boldsymbol\theta\in\R^d$,
	e supporto che non dipende da $\boldsymbol\theta$.
	Allora l'espressione
	\begin{equation*}
		\Lambda \is
		2\log\frac {\sup\limits_{\boldsymbol\theta'} p(\mathbf x;\boldsymbol\theta')}
		{\sup\limits_{\theta_1',\dotsc,\theta_n'} p(\mathbf x;\theta_1',\dotsc,\theta_n',\theta_{n+1},\dots,\theta_d)}
	\end{equation*}
	è un pivot nel limite $N\to\infty$,
	con distribuzione $\chi^2$ a $k\is d-n$ gradi di libertà:
	\begin{equation*}
		p(\Lambda)
		= \frac1{2^{k/2}\Gamma(k/2)} \Lambda^{k/2-1} e^{-\Lambda/2}.
	\end{equation*}
\end{fact}

I pivot servono a semplificare il calcolo della banda di confidenza a partire dalla funzione di ordinamento.
In generale per una statistica $g_\theta(x)$ vale
\begin{align*}
	\int\limits_{O(\theta,x)>q_\theta(\mathrm{CL})} \de x\, p(x;\theta)
	&= \int\limits_{g_\theta\big(\setdef[x]{O(\theta,x)>q_\theta(\mathrm{CL})}\big)}
	\de g_\theta\, p(g_\theta;\theta) \\
	\intertext{se $g_\theta$ è un pivot, allora}
	&= \int\limits_{g_\theta\big(\setdef[x]{O(\theta,x)>q_\theta(\mathrm{CL})}\big)}
	 \de g\, p(g) \\
	\intertext{e se la funzione di ordinamento è proprio $O(\theta,x)=g_\theta(x)$}
	&= \int\limits_{g>q(\mathrm{CL})} \de g\, p(g),
\end{align*}
con cui troviamo $q(\mathrm{CL})$ indipendentemente da $\theta$.
Per il \autoref{th:wilks} il likelihood ratio è asintoticamente un pivot,
quindi non solo ha delle buone proprietà,
ma è anche spesso facile da calcolare.
