% Giacomo Petrillo
% lezione di Punzi

\subsection{Likelihood ratio}

Supponiamo di aver costruito una banda di confidenza per la poissoniana che ci dà limiti superiori a $\mu$.
Se poi nell'esperimento otteniamo, ad esempio, $k=10$,
tipicamente preferiremo usare una stima puntuale oppure un intervallo il più corto possibile.
Il problema è che in questo modo, quando usiamo il limite superiore,
il confidence level non è quello che ci aspettiamo.
Infatti, se in base a $k$ scegliamo una stima intervallare diversa,
non stiamo veramente usando quelle stime intervallari,
bensì una nuova statistica definita a tratti,
che avrà proprietà diverse (tipicamente peggiori).
L'errore di scegliere la statistica da usare \emph{dopo}
aver visto i dati è chiamato \emph{flip-flopping}.
In generale, in un'inferenza frequentista,
è necessario scegliere quale statistica calcolare \emph{prima}
di guardare i dati (o una funzione dei dati).
Se usiamo i dati per decidere quale metodo usare,
allora dobbiamo raccogliere nuovi dati su cui applicarlo.

\begin{example}
	Consideriamo una poissoniana \emph{con fondo}
	\begin{equation*}
		p(k;\mu)
		= \frac{(\mu+b)^k}{k!}e^{-(\mu+b)}
	\end{equation*}
	con $b>0$ e $\mu>0$.
	Se, per mettere un limite superiore a $\mu$,
	facciamo una stima intervallare di $\mu+b$ come nell'\autoref{th:poisssup} e poi sottraiamo $b$,
	possiamo ottenere delle stime intervallari vuote.
	Ad esempio, prendiamo $b=5$, $\mathrm{CL}=\SI{90}\%$ e $k=0$,
	si ottiene l'intervallo $(0,2.3)$ per $\mu+b$ che,
	ricordando che $\mu>0$, sottraendo $b$ è vuoto.
\end{example}

L'esempio evidenzia il problema che si possono costruire bande di confidenza
che soddisfano tutte le richieste fatte e che tuttavia non sono soddisfacenti.
Facciamo un altro esempio.

\begin{example}
	Prendiamo una gaussiana con media positiva
	\begin{equation*}
		p(x;\mu)
		= \frac1{\sqrt{2\pi}} e^{-\frac12(x-\mu)^2}, \quad \mu>0.
	\end{equation*}
	Costruiamo la banda di confidenza con il probability ordering a $\mathrm{CL}=\SI{68}\%$,
	che corrisponde a intervalli lungo $x$ di $\pm1$ intorno a $\mu$,
	allora per $x<-1$ otteniamo una stima intervallare vuota.
	Notiamo che questo problema non dipende dal fatto di avere un \emph{bound} a sinistra,
	basta cambiare parametro a $\log\mu$ per usare tutto $\R$ e il problema persiste perché
	le stime intervallari trasformano in modo banale per cambio di parametro.
\end{example}

\begin{example}
	Una persona pensa un numero a caso,
	vogliamo stimarlo con un livello di confidenza del \SI{95}\%.
	Tiriamo un dado a 20 facce,
	se esce 1 diciamo che l'intervallo è $\emptyset$, altrimenti~$\R$.
\end{example}

Quest'ultimo esempio serve per far capire quanto è debole la proprietà di
avere un livello di confidenza definito, si possono avere intervalli con una
precisa probabilità di contenere il valore ignoto da misurare e che comunque
sono palesemente inutili.

Fortunatamente esiste un ordinamento che funziona sempre decentemente, è il
seguente \emph{likelihood ratio}\footnote{Vedi Gary J. Feldman, Robert D. Cousins,
\emph{A Unified Approach to the Classical Statistical Analysis of Small
Signals}, Phys.Rev.D57:3873-3889, 1998
(\url{https://arxiv.org/abs/physics/9711021}).}:
%
\begin{equation} \label{eq:fclr}
	O(\theta,x) = -\lambda_\theta(x)
	\is -2\log\frac {\sup\limits_{\theta'} p(x;\theta')} {p(x;\theta)}.
\end{equation}

Una funzione di ordinamento è definita a meno di una trasformazione crescente,
vedremo a breve a cosa serve prendere il logaritmo e metterci un fattore 2.

\begin{exercise}
    %
    Mostrare che gli intervalli costruiti con il likelihood ratio contengono
    sempre la stima di massima likelihood.
    %
\end{exercise}

\begin{solution}
    %
    Fissato $x_0$, lo stimatore di massima likelihood è
    %
    \begin{equation*}
        %
        \hat\theta = \operatorname*{arg\,sup}_\theta p(x_0;\theta).
        %
    \end{equation*}
    %
    Per ogni $\theta$, il likelihood ratio \eqref{eq:fclr} raggiunge il
    valore massimo (zero) quando $p(x;\theta) = \sup_{\theta'}p(x;\theta')$,
    ovvero tutte le volte che $\theta$ è lo stimatore di massima likelihood
    per $x$. In particolare, per $\theta = \hat\theta$, anche quando $x = x_0$.
    Poiché la banda è costruita aggiungendo le $x$ partendo da quelle con
    funzione di ordinamento più alta, $x_0$ verrà inclusa nella sezione della
    banda corrispondente a $\hat\theta$, che implica che la sezione
    corrispondente a $x_0$ conterrà $\hat\theta$.
    %
\end{solution}

Corollario: il likelihood ratio non produce mai l'insieme vuoto, come invece
accadeva negli esempi sopra.

\begin{exercise} \label{ex:lrinv}
    %
    Dimostrare che gli intervalli ottenuti con il likelihood ratio sono
    invarianti per cambiamento di variabile (sulle $x$).
    %
\end{exercise}

\begin{solution}
    %
    Scriviamo formalmente lo stimatore:
    %
    \begin{equation*}
        %
        f(x) = \setdef[\theta]{O(\theta,x) > q(\theta) \text{ tale che }
        P_{x'}(O(\theta,x') > q(\theta);\theta) = \mathrm{CL}}.
        %
    \end{equation*}
    %
    Per dimostrare l'invarianza dobbiamo considerare una variabile $y = g(x)$
    con $g$ biunivoca, definire uno stimatore $f'$ come quello sopra ma usando
    $y$ al posto di $x$, e verificare che $f(x) = f'(g(x))$. Abbiamo che
    $p_y(g(x);\theta) = p(x;\theta)/J(x)$, dove $J$ è il modulo del
    determinante del jacobiano di $g$ se siamo in $\mathbb R^n$ o altrimenti in
    ogni caso una qualche funzione di $x$. Quindi la funzione di ordinamento
    \eqref{eq:fclr}, definita usando $y$, diventa
    %
    \begin{align*}
        %
        O'(\theta, g(x)) &= -2\log\frac
        {\sup_{\theta'} p(x;\theta)/J(x)}{p(x;\theta)/J(x)} = \\
        %
        &= -2\log\frac
        {\sup_{\theta'} p(x;\theta)}{p(x;\theta)}
        = O(\theta, x).
        %
    \end{align*}
    %
    Adesso calcoliamo $f'(g(x))$:
    %
    \begin{align*}
        %
        f'(g(x))
        &= \setdef[\theta]{O'(\theta,g(x)) > q(\theta) \text{ t.c. }
        P_{y'}(O'(\theta, y') > q(\theta);\theta) = \mathrm{CL}} = \\
        %
        &= \setdef[\theta]{O(\theta,x) > q(\theta) \text{ t.c. }
        P_{x'}(O'(\theta, g(x')) > q(\theta);\theta) = \mathrm{CL}} = \\
        %
        &= \setdef[\theta]{O(\theta,x) > q(\theta) \text{ t.c. }
        P_{x'}(O(\theta, x') > q(\theta);\theta) = \mathrm{CL}}
        = f(x).
        %
    \end{align*}
    %
\end{solution}

Notiamo che per i dati in cui probability ordering avrebbe dato una stima vuota,
il likelihood ratio tenderà a dare regioni piccole.
Può essere desiderabile gonfiare a mano la banda di confidenza in queste regioni
per evitare di triggerare molestatori che vogliono discutere della validità del modello.

\begin{definition}[Pivot]
	Dato un modello $p(x;\theta)$,
	una statistica $g_\theta(x)$ è un \emph{pivot}
	se $p(g_\theta;\theta)$ non dipende da $\theta$.
\end{definition}

\begin{example}
	Consideriamo un \emph{parametro di localizzazione},
	cioè tale che $p(x;\theta)=f(x-\theta)$.
	Allora $g_\theta=x-\theta$ è un pivot:
	$p(g_\theta=g;\theta) = p(x=g;\theta=0)$.
\end{example}

\begin{fact}[Teorema di Wilks]
	\label{th:wilks}
	Consideriamo un modello per $N$ estrazioni di $x$ con parametro $\boldsymbol\theta\in\R^d$,
	e supporto che non dipende da $\boldsymbol\theta$.
	Allora l'espressione
	\begin{equation*}
		\Lambda \is
		2\log\frac {\sup\limits_{\boldsymbol\theta'} p(\mathbf x;\boldsymbol\theta')}
		{\sup\limits_{\theta_1',\dotsc,\theta_n'} p(\mathbf x;\theta_1',\dotsc,\theta_n',\theta_{n+1},\dots,\theta_d)}
	\end{equation*}
	è un pivot nel limite $N\to\infty$,
	con distribuzione $\chi^2$ a $k\is d-n$ gradi di libertà:
	\begin{equation*}
		p(\Lambda)
		= \frac1{2^{k/2}\Gamma(k/2)} \Lambda^{k/2-1} e^{-\Lambda/2}.
	\end{equation*}
\end{fact}

I pivot servono a semplificare il calcolo della banda di confidenza a partire dalla funzione di ordinamento.
In generale per una statistica $g_\theta(x)$ vale
\begin{align*}
	\int\limits_{O(\theta,x)>q_\theta(\mathrm{CL})} \de x\, p(x;\theta)
	&= \int\limits_{g_\theta\big(\setdef[x]{O(\theta,x)>q_\theta(\mathrm{CL})}\big)}
	\de g_\theta\, p(g_\theta;\theta) \\
	\intertext{se $g_\theta$ è un pivot, allora}
	&= \int\limits_{g_\theta\big(\setdef[x]{O(\theta,x)>q_\theta(\mathrm{CL})}\big)}
	 \de g\, p(g) \\
	\intertext{e se la funzione di ordinamento è proprio $O(\theta,x)=g_\theta(x)$}
	&= \int\limits_{g>q(\mathrm{CL})} \de g\, p(g),
\end{align*}
con cui troviamo $q(\mathrm{CL})$ indipendentemente da $\theta$.
Per il \autoref{th:wilks} il likelihood ratio è asintoticamente un pivot,
quindi non solo ha delle buone proprietà,
ma è anche spesso facile da calcolare.
