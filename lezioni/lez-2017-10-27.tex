% Giacomo Petrillo
% lezione di Morello

\begin{example}
	Calcoliamo lo stimatore di massima likelihood per la poissoniana:
	\begin{align*}
		L \is \log p(\mathbf k;\mu)
		&= -\sum_i \log k_i! + \log\mu \sum_i k_i - N\mu \\
		0 = \pdv L\mu
		&= \sum_i k_i - N\mu \implies \\
		\implies \mu
		&= \bar k.
	\end{align*}
	Risulta la media aritmetica che già sappiamo essere uno stimatore unbiased e efficiente.
\end{example}

\begin{exercise}
	Calcolare lo stimatore di massima likelihood per $p$ della binomiale.
\end{exercise}

\begin{solution}
	\begin{align*}
		L \is \log p(\mathbf k;p)
		&= \sum_i\log\binom n{k_i} + \log p\sum_i k_i + \log(1-p) \sum_i(n-k_i) \\
		0 = \pdv Lp
		&= \frac{N\bar k}{p} - \frac{N(n-\bar k)}{1-p} \implies \\
		\implies p
		&= \frac{\bar k}n.
	\end{align*}
\end{solution}

\begin{example}[Media pesata]
	\label{th:wavg}
	Consideriamo due variabili gaussiane indipendenti con la stessa media e calcoliamo lo stimatore di massima likelihood per la media:
	\begin{align*}
		L \is \log p(x_1,x_2;\mu)
		&= -\log(2\pi\sigma_1\sigma_2)
		-\frac12 \left(\frac{x_1-\mu}{\sigma_1}\right)^2
		- \frac12 \left(\frac{x_2-\mu}{\sigma_2}\right)^2 \\
		0 = \pdv L\mu
		&= \frac{x_1-\mu}{\sigma_1^2} + \frac{x_2-\mu}{\sigma_2^2} \implies \\
		\implies \mu
		&= \frac {\frac{x_1}{\sigma_1^2} + \frac{x_2}{\sigma_2^2}} {\frac1{\sigma_1^2} + \frac1{\sigma_2^2}}.
	\end{align*}
	Per $N$ variabili si generalizza immediatamente alla media pesata con $1/\sigma_i^2$:
	\begin{equation*}
		\hat\mu
		= \frac {\sum_i \frac{x_i}{\sigma_i^2}} {\sum_i \frac1{\sigma_i^2}}.
	\end{equation*}
	Calcoliamo la varianza:
	\begin{align*}
		\var[\hat\mu]
		&= \frac {\sum_i \frac{\sigma_i^2}{\sigma_i^4}} {\left(\sum_i \frac1{\sigma_i^2}\right)^2} = \\
		&= \frac1{\sum_i \frac1{\sigma_i^2}}.
	\end{align*}
\end{example}

\marginpar{Mettere come esercizio la media pesata con la matrice di covarianza.}

\begin{example}
	Consideriamo $N$ estrazioni della distribuzione esponenziale
	\begin{equation*}
		p(t;\tau)
		= \frac1\tau e^{-t/\tau}
	\end{equation*}
	e calcoliamone lo stimatore di massima likelihood per $\tau$:
	\begin{align*}
		L \is \log p(\mathbf t;\tau)
		&= -N\log\tau - \frac{N\bar t}\tau \\
		0 = \pdv L\tau
		&= -\frac N\tau + \frac{N\bar t}{\tau^2} \implies \\
		\implies \tau
		&= \bar t.
	\end{align*}
	Le proprietà di $\hat\tau$ ci sono già note.
	Se vogliamo ricavare lo stimatore di massima likelihood per $\lambda=1/\tau$,
	essendo che il massimo non dipende dalla parametrizzazione,
	la trasformazione è banale:
	\begin{equation*}
		\hat\lambda = \frac1{\hat\tau} = \frac1{\bar t}.
	\end{equation*}
	Calcoliamo il bias di $\hat\lambda$.
	La distribuzione della somma di esponenziali\footnote{Vedi \autoref{th:salvexp}.} è
	\begin{equation*}
		p(s;\lambda)
		= \frac{\lambda^N}{(N-1)!}s^{N-1}e^{-\lambda s},
	\end{equation*}
	calcoliamo il valore atteso di $\hat\lambda$:
	\begin{align*}
		E \left[\frac1{\bar t}\right]
		&= N E \left[\frac1{\sum_i t_i}\right] = \\
		&= N \int_0^\infty \de s\, \frac{\lambda^N}{(N-1)!}s^{N-1}e^{-\lambda s} \frac 1s = \\
		&= \frac{N\lambda}{(N-1)!} \int_0^\infty \lambda\de s\, (\lambda s)^{N-2} e^{-\lambda s} = \\
		&= \frac{N\lambda}{(N-1)!} (N-2)! = \\
		&= \frac N{N-1} \lambda.
	\end{align*}
\end{example}

\begin{example}
	Consideriamo $N$ estrazioni della distribuzione uniforme
	\begin{equation*}
		p(x;m) = \frac 1m \chi_{(0,m)}(x)
	\end{equation*}
	e calcoliamo lo stimatore di massima likelihood:
	\begin{align*}
		p(\mathbf x;m)
		&= \frac1{m^N} \chi_{(0,m)}(\max\mathbf x) = \\
		&= \frac1{m^N} \chi_{(\max\mathbf x,\infty)}(m) \implies \\
		\implies \hat m&=\max\mathbf x.
	\end{align*}
	Abbiamo già calcolato\footnote{Vedi \autoref{th:unifmaxpdf}.} la distribuzione di $\hat m$
	\begin{equation*}
		p(\hat m;m)
		= \frac Nm \left(\frac{\hat m}m\right)^{N-1} \chi_{(0,m)}(\hat m),
	\end{equation*}
	da cui si vede subito che è consistente
	e che ha bias negativo,
	ma anche che asintoticamente non ha distribuzione gaussiana.
	Calcoliamo media e varianza, fissando $m=1$ per fare il conto:
	\begin{align*}
		E[\hat m]
		&= \int_0^1 \de\hat m\, N \hat m^{N-1} \hat m = \\
		&= \frac N{N+1} \\
		E[\hat m^2]
		&= \int_0^1 \de\hat m\, N \hat m^{N-1} \hat m^2 = \\
		&= \frac N{N+2} \\
		\var[\hat m]
		&= E[\hat m^2] - E[\hat m]^2 = \\
		&= \frac{N(N+1)^2 - N^2(N+2)}{(N+2)(N+1)^2} = \\
		&= \frac N{(N+2)(N+1)^2}.
	\end{align*}
\end{example}

% parte spostata dal 31 ottobre
\subsection{Varianza di massima likelihood}

Esponiamo una stima della varianza dello stimatore di massima likelihood.
Sappiamo che asintoticamente la varianza tende a $1/(IN)$:
\begin{align*}
	\frac1{IN}
	&= -\frac1N E \left[ \Pdv[2]{}{\theta} \log p(x;\theta); \theta \right]^{-1} \approx \\
	\intertext{sostituiamo il valore atteso con la media aritmetica}
	&\approx -\frac1N \left( \frac1N \sum_i \Pdv[2]{}{\theta} \log p(x_i;\theta) \right)^{-1} = \\
	&= - \left( \Pdv[2]{}{\theta} \log p(\mathbf x;\theta) \right)^{-1} \approx \\
	\intertext{sostituiamo il parametro con il suo stimatore}
	&\approx - \left( \Pdv[2]{}{\theta} \log p(\mathbf x;\theta) \right)^{-1}_{\theta=\hat\theta}.
\end{align*}
Le sostituzioni fatte valgono asintoticamente quindi questo stimatore è consistente.
I programmi numerici comunemente usati per calcolare gli stimatori di massima likelihood calcolano questa stima della varianza.
\marginpar{Nel caso di minimi quadrati non lineari credo che di solito utilizzino la stima Gauss-Newton dell'hessiana cioè la varianza del caso lineare linearizzando il modello non lineare nel minimo.}
