% Giacomo Petrillo
% lezione di Punzi

\section{Interpretazione della probabilità}

Nella \autoref{sec:assiomi} abbiamo definito matematicamente la probabilità,
però non abbiamo detto esplicitamente come si applica in pratica,
anche se nell'\autoref{teo:sommadadi} abbiamo già dato per scontato
come calcolare le probabilità del lancio di un dado.
Come applicare la probabilità è in definitiva soggettivo ed esistono varie interpretazioni:
\begin{description}
	\item[``Classica'']
		Vale il principio di equiprobabilità e i calcoli probabilistici sono in sostanza calcoli combinatori.
		Questo è storicamente il primo approccio alla probabilità ed è il modo in cui si ragiona
		nei tipici problemini con le carte e i dadi:
		si prende un insieme di eventi ``fondamentali'' che hanno la stessa probabilità
		e per un evento ``composto'' si contano gli eventi fondamentali che lo costituiscono,
		ad esempio per due dadi gli eventi fondamentali sono le coppie ordinate di uscite dei dadi.
		Il ``principio di equiprobabilità'' sposta il problema nel dire quali siano gli eventi fondamentali.
	\item[Frequentista]
		Le quantità di cui ha senso calcolare la probabilità sono quelle che almeno concettualmente potrei campionare nella realtà per verificare la probabilità assegnata.
		Cioè, se dico che le uscite del dado sono equiprobabili,
		e anche che le coppie ordinate di due uscite di un dado sono equiprobabili, etc.,
		posso lanciare un dado un numero arbitrariamente grande di volte
		e fare istogrammi delle singole uscite, delle coppie di uscite consecutive, etc.
		per verificare l'ipotesi con accuratezza arbitraria.
	\item[Bayesiana]
		La probabilità formalizza la conoscenza soggettiva;
		posso assegnare probabilità anche a quantità che non ha senso pensare di poter ``estrarre'' tante volte.
		Nel caso del dado, potrei pormi la seguente domanda:
		ottenute certe uscite lanciando un dado, qual è la \emph{probabilità che le facce siano equiprobabili}?
		Notare che non stiamo intendendo magari la probabilità che fabbricando il dado
		le facce vengano equiprobabili, ma della probabilità assegnata alla proposizione
		<<questo fissato dado ha le facce equiprobabili>>.
		Per come ho formulato la domanda,
		il dado è fissato e quindi non posso in qualche senso riestrarlo,
		quindi nell'approccio frequentista la domanda non ha senso.
		Nell'approccio frequentista posso farmi solo domande del tipo
		<<qual è la probabilità di aver ottenuto queste uscite se le probabilità delle facce sono\dots?>>.
\end{description}
Non approfondiamo oltre il discorso
perché sarà più chiaro dopo il \autoref{sec:cap2}.

\section{Variabili e pdf}

\begin{definition}[Distribuzione di probabilità]
	Consideriamo una partizione dell'insieme universo,
	ovvero una famiglia $V\subseteq\pset(S)$ di sottoinsiemi di $S$ disgiunti a due a due.
	Diciamo che gli elementi di $V$ sono i valori di una \emph{variabile aleatoria}
    \marginpar{Gli statistici e i matematici definiscono le variabili come funzioni applicate
    a uno spazio misurabile comune. Questa definizione credo sia comoda per
    definire le cose e fare le dimostrazioni ma secondo me ti fa capire meno il
    senso. Ho notato che anche Pearl definisce le variabili nel modo che ho
    usato io.}
	e che la \emph{distribuzione} della variabile è la probabilità
	ristretta agli insiemi ottenibili unendo elementi di $V$:
	\begin{align*}
		v &\in V, \\
		P_v &\is P\big|_{\Setdef[\bigcup V']{V'\subseteq V}};
	\end{align*}
	cioè $P_v$ ha gli stessi valori di $P$
	però stabiliamo di calcolarla solo su una sottofamiglia, chiusa per unione, delle parti dell'universo.
\end{definition}

\begin{definition}[Osservabile]
	Chiamiamo \emph{osservabile} una variabile aleatoria a cui assegnamo un preciso significato fisico e che possiamo misurare.
\end{definition}

\begin{definition}[Variabili indipendenti]
	Due variabili aleatorie $x$ e $y$ sono indipendenti se ogni insieme di $x$ è indipendente da ogni insieme di $y$.
\end{definition}

\begin{example}[Variabili reali]
	Consideriamo i sottoinsiemi di $S=\R^2$
	\begin{align*}
		X(x) &= \setdef[(x,y)]{y\in\R}, \\
		Y(y) &= \setdef[(x,y)]{x\in\R}.
	\end{align*}
	Le famiglie
	\begin{align*}
		\mathcal X &= \setdef[X(x)]{x\in\R} \\
		\text{e }\mathcal Y &= \setdef[Y(y)]{y\in\R}
	\end{align*}
	formano due variabili aleatorie.
	Sono indipendenti se vale
	\begin{align*}
		&\phantom{{}={}} P\left( \bigcup_{x\in A} X(x) \cap \bigcup_{y\in B} Y(y) \right) = \\
		&= P\left( \bigcup_{x\in A} X(x) \right) \cdot P\left( \bigcup_{y\in B} Y(y) \right)
	\end{align*}
	per ogni coppia di sottoinsiemi $A,B\subseteq\R$.
	Tipicamente si usa implicitamente un solo simbolo scrivendo
	<<la variabile $x$>>, $P(x)$, $P(x\in A)$.
	Quando si scrive <<$P(x)$>> si intende con $x$ sia la variabile che il valore.
\end{example}

\begin{definition}[Distribuzione congiunta]
	La \emph{distribuzione congiunta} di due variabili non è altro che
	la probabilità dell'intersezione:
	\begin{equation*}
		P(x\in A,y\in B)
		\is P(A\cap B).
	\end{equation*}
\end{definition}

\begin{definition}[Marginalizzazione]
	Usando le definizioni si verifica subito che, data la distribuzione congiunta di due variabili $P(x,y)$,
	ottengo la distribuzione di una variabile con
	\begin{equation*}
		P(x) = \sum_y P(x,y);
	\end{equation*}
	questa operazione si chiama \emph{marginalizzazione}.
\end{definition}

\begin{definition}[Densità di probabilità]
	Data una probabilità su $S=\R^n$, se esiste una funzione $\fundef[p]{\R^n}{\R}$ tale che
	\begin{equation*}
		\forall A\subseteq\R^n : P(A) = \int_A p,
	\end{equation*}
	dove $\int$ è l'integrale di Lebesgue,
	si dice che $p$ è la \emph{densità di probabilità},
	spesso indicata con <<pdf>> (probability density function).
\end{definition}

La definizione di densità implica due requisiti:
\begin{description}
	\item[Positività] $p \ge 0$,
	\item[Normalizzazione] $\int_S p = 1$.
\end{description}
Le proprietà dell'integrale di Lebesgue fanno sì che in ogni caso una densità con queste proprietà genera una probabilità ben definita.
Viceversa, ci sono probabilità che non hanno una densità, a meno di non usare le distribuzioni\footnote{Qui con ``distribuzioni'' si intende il concetto matematico che formalizza, ad esempio, la $\delta$ di Dirac.}.
L'esempio più semplice è
\begin{align*}
	P(X) &= \begin{cases}
		1 & 0 \in X \\
		0 & \text{altrimenti}
	\end{cases} \\
	\implies p(x) &= \delta(x).
\end{align*}
Usando la $\delta$ si può estendere una probabilità discreta a una continua:
\begin{equation*}
	p(x) = \sum_n P(x_n)\delta(x-x_n).
\end{equation*}
Scritta con la densità la marginalizzazione diventa
\begin{equation*}
	p(x) = \int_{-\infty}^\infty \de y\,p(x,y).
\end{equation*}
Notiamo che la pdf è omogenea di grado $-1$:
\begin{equation*}
	p(kx) = \frac1k\, p(x),
\end{equation*}
cioè l'unità di misura della pdf è l'inverso di quella di $x$.

\begin{definition}[Cumulante]
	Per una densità su $\R$, si definisce \emph{cumulante} o in breve \emph{cdf} (cumulative distribution function)
	\begin{equation*}
		F(x) = P(\setdef[x']{x'<x}) = \int_{-\infty}^x \de x'\,p(x').
	\end{equation*}
\end{definition}

\section{Statistiche}

\begin{definition}[Statistica]
	Una funzione $\fundef[s]{S_1}{S_2}$ tra due insiemi dotati di probabilità è una \emph{statistica} se soddisfa
	\marginpar{Nota: la definizione con $\forall A\subseteq S_1:P(s(A))=P(A)$ è sbagliata.}
    \marginpar{La definizione standard di statistica è una funzione che
    viene applicata a variabili osservabili e non a parametri incogniti, qui
    invece la sto usando per indicare il pushforward.}
	\begin{equation*}
		\forall A\subseteq S_2 : P(A) = P(s^{-1}(A)).
	\end{equation*}
\end{definition}

Tipicamente ho la funzione $s$ e definisco la probabilità su $S_2$ in modo che sia una statistica.
Passando alle densità, se $s$ è biunivoca e derivabile, basta fare un cambio di variabili nell'integrale:
\begin{align*}
	P(s(A)) &= \int_{s(A)} p_2 = \int_A p_2 J[s] = \int_A p_1 = P(A) \\
	\implies p_2 &= \frac{p_1}{J[s]}
\end{align*}
dove $J[s]$ è il modulo del determinante jacobiano di $s$.
Se $s$ non è biunivoca bisogna dividere $S_1$ in sottodomini in cui lo è e risommare.

\begin{example}
	\label{th:stat1d}
	Consideriamo una densità monodimensionale $p(x)$ e una funzione monotona $y(x)$.
	Intuitivamente per cambiare variabile conservando la probabilità scrivo l'equazione
	\begin{align*}
		\de P_y &= \de P_x \\
		\implies p(y)\de y &= p(x) \de x \\
		\implies p(y) &= \frac{p(x)}{\left|\frac{\de y}{\de x}\right|}.
	\end{align*}
	Supponiamo che $y$ non sia monotona ma invece ad esempio $y(x)=x^2$.
	Allora dividiamo il dominio in $x<0$, $x>0$ e otteniamo
	\begin{equation*}
		p(y) = \frac{p(x)}{\left|\frac{\de y}{\de x}(x)\right|} + \frac{p(-x)}{\left|\frac{\de y}{\de x}(-x)\right|} =
		\left.\frac{p(x)+p(-x)}{2x}\right|_{x=\sqrt y}.
	\end{equation*}
\end{example}

\begin{exercise}
	Partendo da una densità uniforme su $[0,1]$,
	trovare $y(x)$ tale che $p(y) = ke^{-ky}$.
\end{exercise}

\begin{solution}
	Cerchiamo $y$ monotona e, senza perdita di generalità, crescente; possiamo anche fissare $y(0) = 0$.
	\begin{align*}
		p(y) &= \frac{p(x)}{\frac{\de y}{\de x}} \\
		\implies \frac{\de y}{\de x} &= \frac{p(x)}{p(y)} = \frac{1}{ke^{-ky}}\\
		\implies ke^{-ky}\de y &= \de x \\
		\implies x &= \int_{0}^{y} \de y\, ke^{-ky} = \left[ -e^{-ky} \right]_{0}^{y} = 1 - e^{-ky} \\
		\implies y &= -\frac1k \log(1-x).
	\end{align*}
\end{solution}

\begin{definition}[Valore atteso]
	Detto anche \emph{valore di aspettazione} o \emph{speranza matematica},
	è un funzionale sulle statistiche definito come
	\begin{equation*}
		\langle s \rangle \is E[s] \is \int_S sp.
	\end{equation*}
\end{definition}

\noindent Il valore atteso è lineare:
\begin{equation*}
	E[s_1+\alpha s_2] = E[s_1] + \alpha E[s_2],
\end{equation*}
e per variabili indipendenti vale
\begin{equation*}
	\langle xy \rangle = \langle x \rangle \langle y \rangle.
\end{equation*}

\begin{definition}[Convergenza in probabilità]
	Data una variabile $x$, si dice che $f(x)$ \emph{converge debolmente in probabilità} a $f_0$ per $x$ che tende a $x_0$ se
	\begin{equation*}
		\forall\varepsilon > 0 \lim_{x\to x_0} P\left( \left| f(x) - f_0 \right| > \varepsilon \right) = 0.
	\end{equation*}
	Si dice invece che la convergenza è \emph{forte} se
	\begin{equation*}
		P\left( \lim_{x\to x_0} f(x) = f_0 \right) = 1
	\end{equation*}
	cioè se, estesa opportunamente la probabilità alle successioni, l'insieme di quelle che non convergono a $f_0$ ha probabilità nulla.
\end{definition}

In generale se $P(A)=1$ si dice che $A$ è \emph{quasi certo}. Il <<quasi>> è perché nel continuo si possono costruire insiemi di probabilità nulla nei quali la densità è non nulla.

\begin{fact}[Leggi dei grandi numeri\protect\footnotemark]
	\label{th:grossnum}
	\footnotetext{Ci sono una dozzina di proposizioni che vengono chiamate <<leggi dei grandi numeri>>.}
	Considero sequenze di $N$ variabili indipendenti con la stessa densità (ovvero, detto intuitivamente, $N$ \emph{estrazioni} di una variabile $x$) per la quale esiste il valore atteso $E[x]$.
    \marginpar{Mi sa che serve anche che $E[|x|] < \infty$.}
	Allora la media aritmetica $\frac1N\sum_ix_i$ converge sia debolmente che fortemente a $E[x]$ per $N\to\infty$.
	Esplicitamente:
	\begin{align*}
		\forall\varepsilon > 0 \lim_{N\to \infty} P\left( \left| \frac{\sum_ix_i}N - E[x] \right| > \varepsilon \right) &= 0, \\
		P\left( \lim_{N\to \infty} \frac{\sum_ix_i}N = E[x] \right) &= 1.
	\end{align*}
\end{fact}

\noindent Per certe distribuzioni che non ammettono $E[x]$ la legge debole continua a valere sostituendo $E[x]$ con un certo valore fissato.

\begin{definition}[Momenti]
	Si definisce \emph{momento $n$-esimo centrato su $x_0$} il valore di aspettazione
	\begin{equation*}
		\mu_n(x_0) \is E[(x - x_0)^n].
	\end{equation*}
	Il momento di ordine 1 centrato su 0 si chiama \emph{media} e quello di ordine 2 centrato sulla media \emph{varianza}:
	\begin{align*}
		\mu = \text{media} &\is \mu_1(0) = E[x], \\
		\sigma^2 = \text{varianza} &\is \mu_2(\mu_1(0)) = E[(x-\mu)^2].
	\end{align*}
	La quantità $\sigma$ è la \emph{deviazione standard}.
\end{definition}

\begin{fact}
	\label{th:pdfmom}
	\marginpar{L'analiticità basta richiederla per la differenza delle pdf.
	È equivalente a richiederla per entrambe o è più generale?}
	Se per due pdf analitiche in un certo punto esistono tutti i momenti e sono uguali, allora le pdf sono uguali.
\end{fact}
