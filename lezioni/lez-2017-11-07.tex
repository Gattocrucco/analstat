% Giacomo Petrillo
% lezione di Punzi

\section{Monte Carlo}

Supponiamo di avere un libro che contiene un elenco di numeri.
Si possono considerare questi numeri come estrazioni di una variabile casuale?
Riformulando la domanda come
<<si può applicare la teoria della probabilità a quella sequenza di numeri?>>
appare più evidente che la risposta è che in definitiva è arbitrario e,
se si fissa un criterio,
che dovrebbe essere lo stesso che useremmo per delle prove in un esperimento,
ad esempio per stabilire che l'uscita di un dado è ``casuale''.

Una sequenza prestabilita di numeri che consideriamo casuali,
detti \emph{pseudocasuali},
può essere utile innanzitutto per fare simulazioni di fenomeni che consideriamo casuali,
ma anche per fare calcoli numerici.
Se dimostriamo che una certa statistica converge in probabilità a una quantità che vogliamo calcolare,
possiamo raggiungere la precisione di calcolo desiderata aumentando la lunghezza della sequenza pseudocasuale.
I metodi di calcolo e simulazione basati su sequenze pseudocasuali si chiamano \emph{Monte Carlo}.

\subsection{Integrali Monte Carlo}

Supponiamo di voler calcolare l'integrale di una funzione valutandola numericamente in numero finito $N$ di punti.
Il lettore conoscerà già il metodo dei trapezi e di Simpson (parabole),
che si generalizzano ad approssimare con polinomi di grado $m-1$ (\emph{$m$-sampling}).
Il metodo Monte Carlo più semplice per fare questo calcolo
è valutare la funzione su una sequenza pseudocasuale con distribuzione uniforme
e moltiplicare la media aritmetica dei valori per la lunghezza dell'intervallo.
Infatti per definizione l'integrale della funzione è il valore di aspettazione su una variabile uniforme:
\begin{align*}
	\int_a^b \de x\,f(x)
	&= (b-a)\int_a^b \frac{\de x\,f(x)}{b-a} = \\
	&= (b-a) E[f(x)], \quad p(x) = \frac{\chi_{(a,b)}(x)}{b-a}.
\end{align*}
La stima dell'errore del risultato è la deviazione standard campione sulla media,
quindi ha andamento asintotico $1/\sqrt N$.
Si può dimostrare che l'errore dell'$m$-sampling ha andamento $1/N^{2(m-1)}$,
quindi sembra che non abbiamo ottenuto nulla con il Monte Carlo.
Tuttavia, se l'integrale è in $d$ dimensioni, l'$m$-sampling ha un andamento $1/N^{2(m-1)/d}$,
cioè l'$N$ necessario per ottenere una certa precisione va come $e^{\lambda d}$.
Quindi aumentando il numero di dimensioni diventa rapidamente impossibile in pratica usare l'$m$-sampling.
Inoltre, supponiamo di aver eseguito un integrale e di non essere soddisfatti della precisione:
con l'$m$-sampling bisogna riprocedere da capo aumentando $N$,
mentre con Monte Carlo si può concatenare una nuova sequenza pseudocasuale e riutilizzare i calcoli precedenti.

\subsection{Generazione di sequenze pseudocasuali}

È comodo non dover scrivere esplicitamente un'intera sequenza pseudocasuale per
usarla. Esistono algoritmi ciclici che generano buone sequenze pseudocasuali
molto più lunghe dell'algoritmo. Tipicamente si studiano algoritmi che generano
distribuzioni uniformi e poi si ricavano le altri distribuzioni a partire da
quella uniforme con opportune statistiche. Ad esempio un modo stupido di
ottenere una variabile gaussiana è sommarne tante uniformi.

\paragraph{Cumulante}

Per una distribuzione su $\R$ con cumulante $F$,
se $u$ è uniforme in $(0,1)$,
si verifica immediatamente che $F^{-1}(u)$ è distribuita secondo $F$.

\paragraph{Von Neumann}

Esiste una statistica definita tutte per le distribuzioni a supporto compatto,
detta \emph{metodo di Von Neumann},
che manda una sequenza di variabili uniformi in una sequenza (di lunghezza casuale e minore) con la distribuzione desiderata:
sia $x$ uniforme con lo stesso supporto della distribuzione obiettivo~$p$
e $y$ uniforme in $(0,\max p)$, \emph{accetto} $x$ se $y<p(x)$ e la \emph{rigetto} altrimenti.
La sequenza delle $x$ accettate è distribuita secondo $p$.

\paragraph{MCMC}

Un altro metodo ancora più generico è il \emph{Markov Chain Monte Carlo}
(MCMC). Una catena di Markov è una sequenza casuale in cui la distribuzione di
probabilità di un elemento dipende solo dall'elemento precedente ed è sempre la
stessa. Detto meglio: sia $\mathbf x = (x_1,\dots,x_N)$ una sequenza
distribuita secondo una pdf $p(\mathbf x)$. In generale se calcolo la
probabilità condizionata di un elemento $x_k$ rispetto al precedente $x_{k-1}$
essa dipenderà ancora dagli altri $x_i$, $i \neq k$, $i \neq k-1$, e dipenderà
anche da $k$:
\begin{equation}
    p(x_k|x_{k-1}) = \frac {p(x_k, x_{k-1})} {p(x_{k-1})} = \frac
    {\int \prod_{q \neq k, k-1} \de x_q\, p(\mathbf x)}
    {\int \prod_{q \neq k-1} \de x_q\, p(\mathbf x)}.
\end{equation}
La $p(\mathbf x)$ è una catena di Markov se invece
$p(x_k|x_{k-1})$ dipende solo da $x_{k-1}$ ed è la stessa per tutti i $k$.

Detto dal punto di vista delle sequenze pseudocasuali, le catene di Markov sono
le sequenze in cui per generare l'elemento successivo mi basta sapere
l'elemento precedente. Possiamo chiederci se nel limitarci a studiare questo
tipo di sequenze ci stiamo perdendo qualcosa di utile. Le aggiunte possibili
sono due: potremmo usare $M$ campioni precedenti per generare il prossimo
anziché solo uno, e potremmo far dipendere $p(x_k|x_{k-1},\dots,x_{k-M})$ anche
da $k$. Possiamo ricondurre questi casi a una catena di Markov: basta
considerare la sequenza $y_k$ dove ogni elemento è una $(M+1)$-pla così
definita: $y_k = (k, x_k, x_{k-1}, \dots, x_{k-M+1})$. Quindi le catene di
Markov descrivono nel modo più generale possibile le sequenze casuali
generabili in pratica.

Supponiamo ora di voler usare MCMC per campionare una distribuzione data $p_0$,
ovvero vorremmo che $p(x_k) = p_0(x_k)$. Per prima cosa osserviamo che la
catena di Markov non è caratterizzata solo da $p(x_k|x_{k-1})$: bisogna anche
specificare $p(x_1)$. Questo però vuole anche dire che è impossibile campionare
esattamente $p_0$ con la catena: infatti se fosse $p(x_1) = p_0(x_1)$ allora
dovremmo essere già in grado di campionare $p_0$. In qualche modo bisogna
scegliere una $p(x_k|x_{k-1})$ che sappiamo campionare e tale che, per $k$
abbastanza grande, $p(x_k) \approx p_0(x_k)$. Inoltre anche se da un certo punto
in poi otteniamo $p(x_k) \approx p_0(x_k)$, in generale non è detto che
$x_k$ e $x_{k-1}$ siano indipendenti.

Non approfondiamo oltre la teoria e forniamo il più semplice esempio
funzionante di catena di Markov, l'\emph{algoritmo di Metropolis}. Consideriamo
una \emph{proposal distribution} $p_D(x;y)$ simmetrica, cioè $p_D(x;y) =
p_D(y;x)$. Fissato l'elemento di partenza $x_1$, l'algoritmo per generare
$x_k$ partendo da $x_{k-1}$ è:
\begin{enumerate}
    \item Estrarre $\tilde x$ secondo $p_D(\tilde x;x_{k-1})$.
    \item Sia $p_A = \min(1, p_0(\tilde x)/p_0(x_{k-1}))$. Scegliere con
    probabilità $p_A$ di usare $x_k = \tilde x$ e con probabilità $1-p_A$
    di usare $x_k = x_{k-1}$.
\end{enumerate}
Detto a parole, stiamo proponendo un candidato $\tilde x$ per $x_k$ e
lo accettiamo con probabilità $p_A$. Abbiamo
\begin{equation}
	p(x_k|x_{k-1})
	= p_D(x_k;x_{k-1}) \cdot \min\left(1,\frac{p_0(x_k)}{p_0(x_{k-1})}\right).
\end{equation}
Come al solito siamo stati sloppy chiamando tutte le probabilità $p$, adesso per
non fare confusione nel prossimo passaggio definiamo la funzione $f$
\begin{equation}
    f(x_k, x_{k-1}) = p(x_k|x_{k-1})
\end{equation}
e osserviamo che
\begin{equation}
    \label{eq:db}
    \frac {f(b, a)} {f(a, b)} = \frac {p_0(b)} {p_0(a)}.
\end{equation}
Questa condizione è detta \emph{bilancio dettagliato} e si può dimostrare che,
insieme a opportune ipotesi su $p_D$, porta la catena a campionare
asintoticamente la $p_0$.

Notiamo che nell'algoritmo $p_0$ è usato a meno di una costante moltiplicativa;
questo è molto utile quando si usa il teorema di Bayes: basta scrivere
\begin{equation*}
	p(x|y) \propto p(y|x) \cdot p(x)
\end{equation*}
senza calcolare
\begin{equation*}
	p(y) = \int \de x\, p(y|x)\cdot p(x).
\end{equation*}

% la parte sulla funzione psicometrica la salto perché è tutta inclusa nella lezione di Francavilla
