% Giacomo Petrillo
% lezione di Francavilla

\begin{exercise}
	\label{th:lrexp}
	Applicare likelihood ratio all'esponenziale.
\end{exercise}

\begin{solution*}
	Scriviamo l'esponenziale in funzione della media:
	\begin{equation*}
		p(t;\tau)
		= \frac1\tau e^{-t/\tau}.
	\end{equation*}
	La funzione di ordinamento che usiamo è
	\begin{align*}
		R_\tau(t)
		&\is \log \frac {p(t;\tau)} {\sup\limits_{\tau'} p(t;\tau')} = \\
		\intertext{(la stima di massima likelihood è $\tau=t$)}
		&= \log \frac {\frac1\tau e^{-t/\tau}} {\frac1t e^{-1}} = \\
		&= \log\frac t\tau + 1 - \frac t\tau.
	\end{align*}
	La funzione $\log x - x$ è convessa,
	quindi la regione $R_\tau(t)>R_\mathrm{min}$ è un intervallo,
	che scriviamo come $(q_\mathrm{min}\tau, q_\mathrm{max}\tau)$.
	Scriviamo l'equazione del coverage:
	\begin{align*}
		\mathrm{CL}
		&= \int_{q_\mathrm{min}\tau}^{q_\mathrm{max}\tau} \frac{\de t}\tau e^{-t/\tau} = \\
		&= e^{-q_\mathrm{min}} - e^{-q_\mathrm{max}}.
	\end{align*}
	Ricordando che vale
	\begin{align*}
		R_\mathrm{min} = R_\tau(q_\mathrm{min}\tau)
		&= R_\tau(q_\mathrm{max}\tau) \implies \\
		\implies q_\mathrm{min} e^{1-q_\mathrm{min}}
		&= q_\mathrm{max} e^{1-q_\mathrm{max}},
	\end{align*}
	abbiamo due equazioni per $q_\mathrm{min}$ e $q_\mathrm{max}$,
	che vanno risolte numericamente.
	Notiamo che abbiamo ottenuto le sezioni lungo $t$ della banda di confidenza
	indipendentemente da $\tau$
	usando implicitamente il pivot $t/\tau$.
	La stima intervallare è infine
	\begin{align*}
		\tau_\mathrm{min}
		&= \frac t {q_\mathrm{max}}, \\
		\tau_\mathrm{max}
		&= \frac t {q_\mathrm{min}}.
	\end{align*}
\end{solution*}

\begin{exercise}
	\label{th:lrtri}
	Applicare likelihood ratio alla triangolare
	\begin{equation*}
		p(x;m)
		= \frac2m\left(1-\frac xm\right),
		\quad x \in (0,m).
	\end{equation*}
\end{exercise}

\begin{solution}
	Troviamo la massima likelihood:
	\begin{align*}
		\log p(x;m)
		&= \log2 - \log m + \log\left(1-\frac xm\right) \\
		\pdv{}{m} \log p(x;m)
		&= -\frac1m + \frac1{1-\frac xm} \frac x{m^2} = \\
		&= \frac 1m \left( -1 + \frac x{m-x} \right) = 0 \implies \\
		\implies m
		&= 2x \implies \\
		\implies \sup\limits_m p(x;m)
		&= \frac1x \left(1-\frac12\right) = \\
		&= \frac1{2x}.
	\end{align*}
	Calcoliamo il likelihood ratio:
	\begin{align*}
		R_m(x)
		&\is \log \frac {\frac2m\left(1-\frac xm\right)} {\frac1{2x}} = \\
		&= \log\frac xm + \log\left(1-\frac xm\right) + \log4.
	\end{align*}
	La funzione $\log x + \log(1-x)$ è convessa,
	quindi l'integrale di coverage è
	\begin{align*}
		\mathrm{CL}
		&= \int_{u_\mathrm{min}m}^{u_\mathrm{max}m} \de x\, \frac2m\left(1-\frac xm\right) = \\
		&= \big[2u-u^2\big]_{u_\mathrm{min}m}^{u_\mathrm{max}m} = \\
		&= \big[1-(1-u)^2\big]_{u_\mathrm{min}m}^{u_\mathrm{max}m}.
	\end{align*}
	Vale anche
	\begin{align*}
		R_m(u_\mathrm{min}m)
		&= R_m(u_\mathrm{max}m) \implies \\
		\implies \log u_\mathrm{min} + \log(1-u_\mathrm{min})
		&= \log u_\mathrm{max} + \log(1-u_\mathrm{max}).
	\end{align*}
\end{solution}

Notiamo che nell'\autoref{th:lrexp} e nell'\autoref{th:lrtri}
il parametro del modello è un \emph{parametro di scala},
cioè $p(x/\theta;\theta)$ non dipende da $\theta$,
ovvero la pdf è nella forma
\begin{equation*}
	p(x;\theta) = \frac {f(x/\theta)} \theta.
\end{equation*}

\begin{example}
	\label{th:lrgauss}
	Consideriamo la gaussiana
	\begin{equation*}
		p(x;\mu)
		= \frac1{\sqrt{2\pi}\sigma} e^{-\frac12\left(\frac{x-\mu}\sigma\right)^2}.
	\end{equation*}
	Calcoliamo il likelihood ratio:
	\begin{align*}
		\lambda_\mu(x)
		&= 2\log\frac {\sup\limits_{\mu'} p(x;\mu')} {p(x;\mu)} = \\
		&= 2\log e^{\frac12\left(\frac{x-\mu}\sigma\right)^2} = \\
		&= \left(\frac{x-\mu}\sigma\right)^2.
	\end{align*}
	La variabile $(x-\mu)/\sigma$ è una gaussiana con media nulla e varianza unitaria,
	quindi la distribuzione di $\lambda$ non dipende da $(\mu,\sigma)$ ed è
	\begin{align*}
		p(\lambda)
		&= 2\cdot\frac {\frac1{\sqrt{2\pi}} e^{-\frac12 \lambda}} {\dv{\lambda}{\sqrt\lambda}} = \\
		&= \frac{\lambda^{-1/2}}{\sqrt{2\pi}}e^{-\lambda/2}.
	\end{align*}
	Notiamo che questa è la distribuzione $\chi^2$ con 1 grado di libertà\footnote{Vedi \autoref{th:wilks}.}.
	In generale si può mostrare che la somma dei quadrati di $n$ variabili gaussiane
	con media nulla e varianza unitaria
	ha distribuzione $\chi^2$ con $n$ gradi di libertà.
\end{example}

\begin{exercise}
	Applicare likelihood ratio a $N$ estrazioni della uniforme
	\begin{equation*}
		p(x;m)
		= \frac 1m,
		\quad x \in (0,m)
	\end{equation*}
	e calcolare la distribuzione del likelihood ratio.
\end{exercise}

\begin{solution}
	Usiamo il massimo che è sufficiente\footnote{Vedi \autoref{th:unifmaxpdf}.}.
	La distribuzione di $X\is\max\mathbf x$ è
	\begin{equation*}
		p(X;m)
		= \frac Nm \left(\frac Xm\right)^{N-1},
		\quad X \in (0,m).
	\end{equation*}
	Calcoliamo il likelihood ratio:
	\begin{align*}
		\lambda_m(X)
		&= 2\log\frac {\frac NX} {\frac Nm \left(\frac Xm\right)^{N-1}} = \\
		&= -2N\log\frac Xm.
	\end{align*}
	Calcoliamo la distribuzione di $\lambda$:
	\begin{align*}
		u
		&\is \frac Xm \\
		p(u)
		&= N u^{N-1} \\
		v
		&\is \log u \\
		p(v)
		&= \frac{Nu^{N-1}}{\dv{}u\log u}
		= \frac{Ne^{(N-1)v}}{e^{-v}}
		= N e^{Nv} \\
		\lambda
		&= -2Nv \\
		p(\lambda)
		&= \frac 12 e^{-\lambda/2}.
	\end{align*}
	Anche se $\lambda$ non ha la distribuzione asintotica del \autoref{th:wilks}
	(infatti è violata l'ipotesi che il supporto di $p(x;m)$ non dipenda da $m$),
	è comunque un pivot.
	Calcoliamo il coverage:
	\begin{align*}
		\mathrm{CL}
		&= \int_{\lambda<q} \de\lambda\, p(\lambda) = \\
		&= \int_0^{q/2} \de y\, e^{-y} = \\
		&= 1 - e^{-q/2}.
	\end{align*}
	Troviamo il confine non banale (l'altro è $m>X$) della banda:
	\begin{align*}
		\lambda
		&< q \iff \\
		\iff -2N\log\frac Xm
		&< q \iff \\
		\iff m
		&< Xe^{q/(2N)} = \\
		&= X\left(e^{-q/2}\right)^{-1/N} = \\
		&= \frac X {\sqrt[N]{1-\mathrm{CL}}}.
	\end{align*}
	Notiamo che otteniamo lo stesso risultato dell'\autoref{th:unifsup}.
\end{solution}

\subsection{Stime puntuali e intervallari}

Adesso che abbiamo definito e studiato la stima intervallare, ritorniamo al
discorso lasciato in sospeso all'inizio di questa sezione
(\ref{sec:intervallare}), cioè quando usare una stima puntuale piuttosto che
una intervallare.

Il criterio che avevamo dato perché una stima puntuale fosse ``buona'' era che
la varianza dello stimatore rimanesse abbastanza costante intorno alla stima,
in modo che avesse senso riportarne una stima e usarla per confrontarla con
altri risultati o per fare medie pesate. Inoltre avevamo richiesto che il bias
fosse piccolo, e che la distribuzione fosse approssimativamente gaussiana.

Adesso possiamo fare un'ulteriore considerazione: se la varianza è costante al
variare del valore vero e il bias è nullo, e conosciamo la forma della
distribuzione, allora possiamo generare intervalli di confidenza a partire
dalla stima: infatti probabilmente avrete già visto costruire intervalli
di confidenza dicendo che l'intervallo di $\pm 1 \sigma$ intorno alla stima è
un intervallo di confidenza con $\mathrm{CL} = 68\,\%$. Bene, adesso sapete
quali sono le ipotesi necessarie per dirlo.
