% Giacomo Petrillo
% lezione di Punzi la prima ora, poi Morello

Poiché in generale non esiste il test UMP,
bisogna cercare richieste più deboli per scegliere il test.
Una possibilità è privilegiare un'ipotesi alternativa
e usare Neyman-Pearson su quella.

Un caso tipico è quando ci sono alcune ipotesi alternative
per le quali è facile avere una potenza alta,
mentre altre per cui è invece importante scegliere bene il test.
Ad esempio, prendiamo una gaussiana e facciamo il test con l'ipotesi nulla di una certa media:
per medie alternative più lontane di quattro-cinque sigma,
ci aspettiamo che qualunque test sensato useremo rigetterà con buona potenza l'ipotesi nulla.
Il problema è per le medie vicine all'ipotesi nulla.
Introduciamo allora il concetto di \emph{LMP} (locally most powerful test).

Costruiamo un test LMP generale:
sviluppiamo in serie il logaritmo della likelihood intorno all'ipotesi nulla
\begin{align*}
	\log p(x;\theta)
	&= \log p(x;\theta_0)
	+ \left.\pdv{}{\theta} \log p(x;\theta)\right|_{\theta_0} (\theta-\theta_0)
	+ O\big((\theta-\theta_0)^2\big) \\
	\implies p(x;\theta)
	&= p(x;\theta_0)
	e^{\left.\pdv{}{\theta} \log p(x;\theta)\right|_{\theta_0} (\theta-\theta_0)}
	e^{O\left((\theta-\theta_0)^2\right)}.
\end{align*}
Vediamo che, a meno del termine all'ordine quadratico,
la distribuzione è nella forma del \autoref{th:umpge},
quindi restringendo il parametro a un intorno destro dell'ipotesi nulla,
il test UMP è definito dalla regione critica
\begin{equation*}
	C = \Setdef[x]{\left.\pdv{}{\theta} \log p(x;\theta)\right|_{\theta_0} > q(\alpha)}.
\end{equation*}

Un altro test generale che non è garantito funzionare ma spesso funziona bene
è il \emph{likelihood ratio test}, che è un'estensione del test di Neyman-Pearson.
Nel test di Neyman-Pearson la regione critica è
\begin{equation*}
	C = \Setdef[x]{\frac{p(x;H_1)}{p(x;H_0)} > q},
\end{equation*}
la estendiamo a un numero arbitrario di ipotesi alternative prendendo il limite superiore:
\begin{equation*}
	C = \Setdef[x]{\frac {\sup\limits_{\theta\neq\theta_0} p(x;\theta)} {p(x;\theta_0)} > q}.
\end{equation*}
Se l'ipotesi nulla è \emph{composta},
cioè è un'insieme di valori del parametro,
possiamo estendere ulteriormente prendendo il limite superiore anche a denominatore:
\begin{equation*}
	C = \Setdef[x]{\frac {\sup\limits_{\theta\not\in H_0} p(x;\theta)} {\sup\limits_{\theta\in H_0} p(x;\theta)} > q}.
\end{equation*}
Se $\boldsymbol\theta\in\R^d$
e l'ipotesi nulla composta sono le prime $n$ componenti di $\boldsymbol\theta$,
al numeratore prendiamo il limite superiore su tutto lo spazio
per poter applicare il \autoref{th:wilks} (teorema di Wilks):
\begin{equation*}
	C = \Setdef[x]{\frac
	{\sup\limits_{\boldsymbol\theta} p(x;\boldsymbol\theta)}
	{\sup\limits_{\theta_1,\dotsc,\theta_n} p(x;\boldsymbol\theta)} > q}.
\end{equation*}
Per il teorema di Wilks (esteso rispetto a come l'abbiamo enunciato),
questo likelihood ratio ($2\log$) ha asintoticamente nel numero di estrazioni
la distribuzione $\chi^2$ con $d-n$ gradi di libertà.

% ———————— parte di Morello ————————

\begin{exercise}
	Fare un test UMP per due medie della gaussiana con varianza fissata.
\end{exercise}

\begin{solution*}
	Consideriamo la gaussiana
	\begin{equation*}
		p(\mathbf x;\mu)
		= (2\pi)^{-1/(2N)}
		\exp \left( -\frac12 \sum_{i=1}^N (x_i-\mu)^2 \right).
	\end{equation*}
	Abbiamo solo due ipotesi,
	l'ipotesi nulla $\mu=\mu_0$ e $\mu=\mu_1\neq\mu_0$,
	quindi usiamo il test di Neyman-Pearson (\autoref{th:np}).
	Possiamo liberamente usare una funzione monotona del rapporto delle probabilità,
	quindi prendiamo il logaritmo:
	\begin{align*}
		2\log \frac {p(\mathbf x;\mu_1)} {p(\mathbf x;\mu_0)}
		&= \sum_i \big((x_i-\mu_0)^2 - (x_i-\mu_1)^2\big) = \\
		&= 2(\mu_1-\mu_0)N\bar x + N(\mu_0^2-\mu_1^2),
	\end{align*}
	che possiamo ulteriormente ridurre a
	\begin{equation*}
		\sgn(\mu_1-\mu_0)\cdot \bar x
		\isis t(\mathbf x).
	\end{equation*}
	Fissiamo $\mu_1 > \mu_0$.
	Falso positivo e falso negativo si possono calcolare direttamente con $t$,
	di cui conosciamo la distribuzione:
	\begin{align*}
		p(t;\mu)
		&= \operatorname{gauss}\big(t;\mu,1/\sqrt N\big) \\
		\alpha
		&= \int_q^\infty \de t\, p(t;\mu_0) \\
		\beta
		&= \int_{-\infty}^q \de t\, p(t;\mu_1).
	\end{align*}
	Questi integrali si calcolano numericamente,
	tipicamente usando la \emph{error function}:
	\begin{align*}
		\erf(x)
		&\is \frac2{\sqrt\pi} \int_0^x \de y\,e^{-y^2} \\
		\int_x^\infty \de y\, \operatorname{gauss}(y;\mu,\sigma)
		&= \frac12 \left( 1 - \erf\left(\frac{x-\mu}{\sqrt2\sigma}\right) \right).
	\end{align*}
\end{solution*}

\begin{example}
	Consideriamo due distribuzioni che parametrizziamo con le loro mediane $m_0$, $m_1$.
	Fissiamo $m_1>m_0$.
	Consideriamo la statistica definita su $N$ estrazioni
	\begin{equation*}
		N^+(\mathbf x)
		\is \#\setdef[x_i]{x_i\ge m_0},
		\quad N^- \is N - N^+.
	\end{equation*}
	Facciamo un test con una regione critica definita da
	\begin{equation*}
		N^+ \ge q(\alpha).
	\end{equation*}
	La taglia si può calcolare senza conoscere più in dettaglio l'ipotesi nulla:
	\begin{align*}
		\alpha
		&= P(N^+\ge q(\alpha);m_0) \\
		P(N^+;m_0)
		&= \binom N{N^+} \frac1{2^N},
	\end{align*}
	invece per la potenza è necessario conoscere
	\begin{equation*}
		P(x<m_0;m_1).
	\end{equation*}
\end{example}

\begin{exercise}
	Fare un test UMP sulla media della poissoniana con l'ipotesi nulla $\mu=b$ e le ipotesi alternative $\mu>b$.
\end{exercise}

\begin{solution}
	Possiamo applicare il \autoref{th:umpge},
	perché la poissoniana si può scrivere come
	\begin{align*}
		p(k;\mu)
		&= \frac1{k!} \cdot e^{-\mu} \cdot e^{k \cdot \log\mu} = \\
		&= F(k) \cdot G(\mu) \cdot e^{A(k) \cdot B(\mu)},
	\end{align*}
	con $A(k)=k$ e $B(\mu)=\log\mu$ monotona crescente.
	Allora il test UMP è definito dalla regione critica
	\begin{equation*}
		C = \setdef[k]{k \ge k_\mathrm{min}},
	\end{equation*}
	che in effetti era il test più intuitivo.
\end{solution}
