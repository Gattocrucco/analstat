% Giacomo Petrillo
% lezione di Punzi

\subsection{Istogrammi}
\label{sec:hist}

Consideriamo un istogramma, ovvero:
sia $\{S_j\}$ una partizione della variabile $x$;
date $N$ estrazioni di $x$, i \emph{conteggi} $k_j$ dei \emph{bin} $S_j$
sono il numero di estrazioni che cadono nel bin:
\begin{equation*}
	k_j \is \# (S_j \cap \{\mathbf x\}).
\end{equation*}
Per ogni $x$, la probabilità di cadere in un bin è per definizione la probabilità del bin
\begin{equation*}
	p_j \is P(S_j) = \int_{S_j} \de x\,p(x).
\end{equation*}
La distribuzione dei conteggi è allora la multinomiale:
\begin{equation*}
	P(\mathbf k)
	= \delta_{\sum_jk_j,N} \frac{N!}{\prod_jk_j!}\prod_j p_j^{k_j},
\end{equation*}
mentre la distribuzione di un singolo conteggio è binomiale:
\begin{equation*}
	P(k_j) = \binom N{k_j} p_j^{k_j} (1-p_j)^{N-k_j}.
\end{equation*}

Poniamo che il numero di estrazioni sia anch'esso una variabile e abbia distribuzione poissoniana:
\begin{equation*}
	P(N)
	= \frac{\mu^N}{N!}e^{-\mu}.
\end{equation*}
Reinterpretiamo la distribuzione di $\mathbf k$ come probabilità condizionata e calcoliamo la nuova distribuzione\footnote{Questo conto è già stato fatto nell'\autoref{th:taxi}.}:
\begin{align*}
	P(k_j)
	&= \sum_N P(k_j,N) = \\
	&= \sum_{N\ge k_j} P(k_j|N) P(N) = \\
	&= \left(\frac{p_j}{1-p_j}\right)^{k_j} \frac{e^{-\mu}}{k_j!}
	\sum_{N\ge k_j} \frac{\mu^N (1-p_j)^N}{(N-k_j)!} = \\
	&= \left(\frac{p_j}{1-p_j}\right)^{k_j} \frac{e^{-\mu}}{k_j!}
	\big(\mu(1-p_j)\big)^{k_j} \sum_{M\ge 0} \frac{\big(\mu(1-p_j)\big)^M}{M!} = \\
	&= \frac{(p_j\mu)^{k_j} e^{-\mu}}{k_j!} e^{(1-p_j)\mu} = \\
	&= \frac{(p_j\mu)^{k_j}}{k_j!} e^{-p_j\mu}.
\end{align*}
Quindi la distribuzione dei singoli conteggi è poissoniana con media $p_j\mu$.

Poniamo un modello per $x$ con parametro $\theta$.
Le probabilità dei bin dipendono allora da $\theta$:
\begin{equation*}
	p_j = \int_{S_j} \de x\, p(x;\theta).
\end{equation*}
Scriviamo il logaritmo della likelihood,
costruita a partire dall'istogramma,
sia nel caso di $N$ fisso che poissoniano.
\begin{align*}
	\log P(\mathbf k|N;\theta)
	&= f_1(\mathbf k) + \sum_j k_j\log p_j \\
	\log P(\mathbf k;\theta,\mu)
	&= f_2(\mathbf k) + \sum_j k_j\log p_j
	+ \log\mu\sum_jk_j - \mu\underbrace{\sum_jp_j}\limits_{=1} = \\
	&= f_3(\mathbf k,\mu) + \log P(\mathbf k|N;\theta).
\end{align*}
Per calcolare lo stimatore di massima likelihood per $\theta$ le due likelihood sono equivalenti.
La differenza diventa rilevante se usiamo un modello più complesso in cui $\mu$ dipende da $\theta$.
In questo ambito la likelihood con il termine in $\mu$ si chiama \emph{extended likelihood}.

\begin{fact}
	Nel caso di $N$ fisso lo stimatore di massima likelihood per $\theta$ ha asintoticamente varianza
	\begin{equation*}
		\var[\hat\theta]
		= \frac1{NA} + O\left(\frac1{N^2}\right)
	\end{equation*}
	e bias
	\begin{equation*}
		b(\theta)
		= -\frac B{2NA^2} + O\left(\frac1{N^2}\right),
	\end{equation*}
	dove $A$ e $B$ sono
	\begin{align*}
		A
		&\is \sum_j \frac1{p_j} \left(\pdv{p_j}{\theta}\right)^2 \\
		B
		&\is \sum_j \frac1{p_j} \pdv{p_j}{\theta} \Pdv[2]{p_j}{\theta}.
	\end{align*}
\end{fact}

\subsection{Varianza di massima likelihood}

Esponiamo una stima della varianza dello stimatore di massima likelihood.
\marginpar{Magari spostare questa parte subito dopo massima likelihood.}
Sappiamo che asintoticamente la varianza tende a $1/(IN)$:
\begin{align*}
	\frac1{IN}
	&= -\frac1N E \left[ \Pdv[2]{}{\theta} \log p(x;\theta); \theta \right]^{-1} \approx \\
	\intertext{sostituiamo il valore atteso con la media aritmetica}
	&\approx -\frac1N \left( \frac1N \sum_i \Pdv[2]{}{\theta} \log p(x_i;\theta) \right)^{-1} = \\
	&= - \left( \Pdv[2]{}{\theta} \log p(\mathbf x;\theta) \right)^{-1} \approx \\
	\intertext{sostituiamo il parametro con il suo stimatore}
	&\approx - \left( \Pdv[2]{}{\theta} \log p(\mathbf x;\theta) \right)^{-1}_{\theta=\hat\theta}.
\end{align*}
Le sostituzioni fatte valgono asintoticamente quindi questo stimatore è consistente.
I programmi numerici comunemente usati per calcolare gli stimatori di massima likelihood calcolano questa stima della varianza.
\marginpar{Nel caso di minimi quadrati non lineari credo che di solito utilizzino la stima Gauss-Newton dell'hessiana cioè la varianza del caso lineare linearizzando il modello non lineare nel minimo.}

\subsection{Minimi quadrati}

Consideriamo un modello gaussiano per $N$ variabili indipendenti $y_i$
con deviazioni standard fissate $\sigma_i$
e medie date da $\mu(x_i,\theta)$ dove $x_i$ sono fissati e $\theta$ è il parametro.
Scriviamo il logaritmo della likelihood:
\begin{align*}
	\log p(\mathbf y;\theta)
	&= -\frac N2\log(2\pi)
	- \sum_i\log\sigma_i
	- \frac12 \sum_i \left( \frac{y_i - \mu(x_i,\theta)}{\sigma_i} \right)^2.
\end{align*}
Per fare inferenza su $\theta$, l'unica parte che ci interessa è l'ultimo termine.
In particolare, per calcolare lo stimatore di massima likelihood, dobbiamo minimizzare
\begin{equation*}
	\frac12 \sum_i \left( \frac{y_i - \mu(x_i,\theta)}{\sigma_i} \right)^2;
\end{equation*}
questo è noto come \emph{metodo dei minimi quadrati}.

Se abbiamo un modello con distribuzioni non gaussiane ma che riteniamo siano approssimabili con gaussiane,
allora possiamo approssimare lo stimatore di massima likelihood con quello dei minimi quadrati.
Ad esempio, se abbiamo un istogramma\footnote{Vedi sezione sugli istogrammi (\ref{sec:hist}) per la notazione.}
(poniamo con numero totale di eventi poissoniano)
con abbastanza eventi in tutti i bin da poter approssimare le distribuzioni dei conteggi come gaussiane,
stimando la varianza di un conteggio $k$ con $k$ stesso e $\mu$ con $N$,
possiamo approssimare la stima di massima likelihood con il minimo di
\begin{equation*}
	\sum_j \frac{(k_j - Np_j(\theta))^2}{k}.
\end{equation*}
Nel caso dell'istogramma, un'ulteriore utile approssimazione può essere sostituire l'integrale
\begin{equation*}
	p_j(\theta) = \int_{S_j} \de x\, p(x;\theta)
\end{equation*}
con $p(x_j;\theta)$ per un qualche $x_j\in S_j$, allora l'espressione da minimizzare diventa
\begin{equation*}
	\sum_j \frac{(k_j - Np(x_j;\theta))^2}{k}.
\end{equation*}
Questa espressione definisce in ogni caso uno stimatore,
anche se non riteniamo che tutte le distribuzioni dei conteggi siano approssimabili con gaussiane.
Se $N$ non è abbastanza grande, le proprietà di questo stimatore andranno studiate ad hoc,
eventualmente con Monte Carlo.
È stato studiato ampiamente e risulta che tipicamente è peggiore di massima likelihood.
Bisogna quindi nella pratica fare attenzione a software di fit di istogrammi che senza dichiararlo usano questo metodo.

Torniamo all'origine del metodo dei minimi quadrati e estendiamolo a un caso più generale.
Consideriamo una variabile gaussiana $\mathbf y$ con matrice di covarianza fissata $V$ e media data da $\boldsymbol\mu(\theta)$.
Il termine del logaritmo della likelihood che dipende da $\theta$ è la forma quadratica
\begin{equation*}
	Q \is -\frac 12 (\mathbf y - \boldsymbol\mu(\theta))^\transp V^{-1} (\mathbf y - \boldsymbol\mu(\theta)).
\end{equation*}
Dimostriamo che il massimo di questa espressione è uno stimatore consistente
anche se la distribuzione non è gaussiana (ma $\boldsymbol\mu$ è comunque la media)
e $V$ non è la matrice di covarianza (ma è comunque definita positiva).
Consideriamo le derivate rispetto al parametro:
\begin{align*}
	\pdv Q\theta
	&= (\mathbf y - \boldsymbol\mu(\theta))^\transp V^{-1} \pdv{\boldsymbol\mu}\theta, \\
	\Pdv Q\theta
	&= - \pdv{\boldsymbol\mu}\theta^\transp V^{-1} \pdv{\boldsymbol\mu}\theta
	+ (\mathbf y - \boldsymbol\mu(\theta))^\transp V^{-1} \Pdv{\boldsymbol\mu}\theta.
\end{align*}
Il valore atteso della derivata prima nel valore vero è nullo:
\begin{equation*}
	E\left[\pdv Q\theta;\theta\right]
	\propto E[\mathbf y - \boldsymbol\mu(\theta);\theta] = 0
\end{equation*}
mentre il valore atteso della derivata seconda è negativo perché $V$ è definita positiva.
Allora $\pdv Q\theta$ soddisfa le proprietà della funzione implicita viste per massima likelihood\footnote{Vedi sezione \ref{sec:ml}.},
e il massimo di $Q$ definisce uno stimatore consistente.

Consideriamo il caso particolare in cui la media è lineare nel parametro:
\begin{equation*}
	\mu_i(\boldsymbol\theta) = A_{ik} \theta_k.
\end{equation*}
Allora il massimo della forma quadratica si ottiene per
\begin{equation*}
	\hat{\boldsymbol\theta} = (A^\transp V^{-1} A)^{-1} (A^\transp V^{-1}) \mathbf y.
\end{equation*}
Questo stimatore ha bias nullo:
\begin{equation*}
	E[\hat{\boldsymbol\theta}]
	= (A^\transp V^{-1} A)^{-1} (A^\transp V^{-1}) A \boldsymbol\theta
	= \boldsymbol\theta.
\end{equation*}
\begin{fact}[Teorema di Gauss-Markov]
	Se $V$ è effettivamente la matrice di covarianza,
	$\hat{\boldsymbol\theta}$ è lo stimatore lineare e con bias nullo di minima varianza\footnote{Anche detto BLUE ovvero Best Linear Unbiased Estimator.}.
\end{fact}
