% Giacomo Petrillo
% lezione di Punzi

\subsection{Istogrammi}
\label{sec:hist}

Consideriamo un istogramma, ovvero:
sia $\{S_j\}$ una partizione della variabile $x$;
date $N$ estrazioni di $x$, i \emph{conteggi} $k_j$ dei \emph{bin} $S_j$
sono il numero di estrazioni che cadono nel bin:
\begin{equation*}
	k_j \is \# (S_j \cap \{\mathbf x\}).
\end{equation*}
Per ogni $x$, la probabilità di cadere in un bin è per definizione la probabilità del bin
\begin{equation*}
	p_j \is P(S_j) = \int_{S_j} \de x\,p(x).
\end{equation*}
La distribuzione dei conteggi è allora la multinomiale:
\begin{equation*}
	P(\mathbf k)
	= \delta_{\sum_jk_j,N} \frac{N!}{\prod_jk_j!}\prod_j p_j^{k_j},
\end{equation*}
mentre la distribuzione di un singolo conteggio è binomiale:
\begin{equation*}
	P(k_j) = \binom N{k_j} p_j^{k_j} (1-p_j)^{N-k_j}.
\end{equation*}

Poniamo che il numero di estrazioni sia anch'esso una variabile e abbia distribuzione poissoniana:
\begin{equation*}
	P(N)
	= \frac{\mu^N}{N!}e^{-\mu}.
\end{equation*}
Reinterpretiamo la distribuzione di $\mathbf k$ come probabilità condizionata e calcoliamo la nuova distribuzione (questo conto è già stato fatto nell'\autoref{th:taxi}):
\begin{align*}
	P(k_j)
	&= \sum_N P(k_j,N) = \\
	&= \sum_{N\ge k_j} P(k_j|N) P(N) = \\
	&= \left(\frac{p_j}{1-p_j}\right)^{k_j} \frac{e^{-\mu}}{k_j!}
	\sum_{N\ge k_j} \frac{\mu^N (1-p_j)^N}{(N-k_j)!} = \\
	&= \left(\frac{p_j}{1-p_j}\right)^{k_j} \frac{e^{-\mu}}{k_j!}
	\big(\mu(1-p_j)\big)^{k_j} \sum_{M\ge 0} \frac{\big(\mu(1-p_j)\big)^M}{M!} = \\
	&= \frac{(p_j\mu)^{k_j} e^{-\mu}}{k_j!} e^{(1-p_j)\mu} = \\
	&= \frac{(p_j\mu)^{k_j}}{k_j!} e^{-p_j\mu}.
\end{align*}
Quindi la distribuzione dei singoli conteggi è poissoniana con media $p_j\mu$.

Poniamo un modello per $x$ con parametro $\theta$.
Le probabilità dei bin dipendono allora da $\theta$:
\begin{equation*}
	p_j = \int_{S_j} \de x\, p(x;\theta).
\end{equation*}
Scriviamo il logaritmo della likelihood,
costruita a partire dall'istogramma,
sia nel caso di $N$ fisso che poissoniano.
\begin{align*}
	\log P(\mathbf k|N;\theta)
	&= f_1(\mathbf k) + \sum_j k_j\log p_j \\
	\log P(\mathbf k;\theta,\mu)
	&= f_2(\mathbf k) + \sum_j k_j\log p_j
	+ \log\mu\sum_jk_j - \mu\underbrace{\sum_jp_j}\limits_{=1} = \\
	&= f_3(\mathbf k,\mu) + \log P(\mathbf k|N;\theta).
\end{align*}
Per calcolare lo stimatore di massima likelihood per $\theta$ le due likelihood sono equivalenti.
La differenza diventa rilevante se usiamo un modello più complesso in cui $\mu$ dipende da $\theta$.
In questo ambito la likelihood con il termine in $\mu$ si chiama \emph{extended likelihood}.

\begin{fact}
	Nel caso di $N$ fisso lo stimatore di massima likelihood per $\theta$ ha asintoticamente varianza
	\begin{equation*}
		\var[\hat\theta]
		= \frac1{NA} + O\left(\frac1{N^2}\right)
	\end{equation*}
	e bias
	\begin{equation*}
		b(\theta)
		= -\frac B{2NA^2} + O\left(\frac1{N^2}\right),
	\end{equation*}
	dove $A$ e $B$ sono
	\begin{align*}
		A
		&\is \sum_j \frac1{p_j} \left(\pdv{p_j}{\theta}\right)^2 \\
		B
		&\is \sum_j \frac1{p_j} \pdv{p_j}{\theta} \Pdv[2]{p_j}{\theta}.
	\end{align*}
\end{fact}

% qui c'era la stima della varianza di massima likelihood
% spostata nella lezione 27 ottobre

\subsection{Minimi quadrati}

Consideriamo un modello gaussiano per $N$ variabili indipendenti $y_i$
con deviazioni standard fissate $\sigma_i$
e medie date da $\mu(x_i,\theta)$ dove $x_i$ sono fissati e $\theta$ è il parametro.
Scriviamo il logaritmo della likelihood:
\begin{align*}
	\log p(\mathbf y;\theta)
	&= -\frac N2\log(2\pi)
	- \sum_i\log\sigma_i
	- \frac12 \sum_i \left( \frac{y_i - \mu(x_i,\theta)}{\sigma_i} \right)^2.
\end{align*}
Per fare inferenza su $\theta$, l'unica parte che ci interessa è l'ultimo termine.
In particolare, per calcolare lo stimatore di massima likelihood, dobbiamo minimizzare
\begin{equation*}
	\frac12 \sum_i \left( \frac{y_i - \mu(x_i,\theta)}{\sigma_i} \right)^2;
\end{equation*}
questo è noto come \emph{metodo dei minimi quadrati}.

Se abbiamo un modello con distribuzioni non gaussiane ma che riteniamo siano approssimabili con gaussiane,
allora possiamo approssimare lo stimatore di massima likelihood con quello dei minimi quadrati.
Ad esempio, se abbiamo un istogramma\footnote{Vedi sezione sugli istogrammi (\ref{sec:hist}) per la notazione.}
(poniamo con numero totale di eventi poissoniano)
con abbastanza eventi in tutti i bin da poter approssimare le distribuzioni dei conteggi come gaussiane,
stimando la varianza di un conteggio $k$ con $k$ stesso e $\mu$ con $N$,
possiamo approssimare la stima di massima likelihood con il minimo di
\begin{equation*}
	\sum_j \frac{(k_j - Np_j(\theta))^2}{k}.
\end{equation*}
Nel caso dell'istogramma, un'ulteriore utile approssimazione può essere sostituire l'integrale
\begin{equation*}
	p_j(\theta) = \int_{S_j} \de x\, p(x;\theta)
\end{equation*}
con $p(x_j;\theta)$ per un qualche $x_j\in S_j$, allora l'espressione da minimizzare diventa
\begin{equation*}
	\sum_j \frac{(k_j - Np(x_j;\theta))^2}{k}.
\end{equation*}
Questa espressione definisce in ogni caso uno stimatore,
anche se non riteniamo che tutte le distribuzioni dei conteggi siano approssimabili con gaussiane.
Se $N$ non è abbastanza grande, le proprietà di questo stimatore andranno studiate ad hoc,
eventualmente con Monte Carlo.
È stato studiato ampiamente e risulta che tipicamente è peggiore di massima likelihood.
Bisogna quindi nella pratica fare attenzione a software di fit di istogrammi che senza dichiararlo usano questo metodo.

Torniamo all'origine del metodo dei minimi quadrati e estendiamolo a un caso più generale.
Consideriamo una variabile gaussiana $\mathbf y$ con matrice di covarianza fissata $V$ e media data da $\boldsymbol\mu(\theta)$.
Il termine del logaritmo della likelihood che dipende da $\theta$ è la forma quadratica
\begin{equation*}
	Q \is -\frac 12 (\mathbf y - \boldsymbol\mu(\theta))^\transp V^{-1} (\mathbf y - \boldsymbol\mu(\theta)).
\end{equation*}
Dimostriamo che il massimo di questa espressione è uno stimatore consistente
anche se la distribuzione non è gaussiana (ma $\boldsymbol\mu$ è comunque la media)
e $V$ non è la matrice di covarianza (ma è comunque definita positiva).
Consideriamo le derivate rispetto al parametro:
\begin{align*}
	\pdv Q\theta
	&= (\mathbf y - \boldsymbol\mu(\theta))^\transp V^{-1} \pdv{\boldsymbol\mu}\theta, \\
	\Pdv Q\theta
	&= - \pdv{\boldsymbol\mu}\theta^\transp V^{-1} \pdv{\boldsymbol\mu}\theta
	+ (\mathbf y - \boldsymbol\mu(\theta))^\transp V^{-1} \Pdv{\boldsymbol\mu}\theta.
\end{align*}
Il valore atteso della derivata prima nel valore vero è nullo:
\begin{equation*}
	E\left[\pdv Q\theta;\theta\right]
	\propto E[\mathbf y - \boldsymbol\mu(\theta);\theta] = 0
\end{equation*}
mentre il valore atteso della derivata seconda è negativo perché $V$ è definita positiva.
Allora $\pdv Q\theta$ soddisfa le proprietà della funzione implicita viste per massima likelihood\footnote{Vedi sezione \ref{sec:ml}.},
\marginpar{Sistemare questa dimostrazione dopo aver sistemato quella di massima likelihood
(\autoref{teo:mle}).}
e il massimo di $Q$ definisce uno stimatore consistente.

Consideriamo il caso particolare in cui la media è lineare nel parametro:
\begin{equation*}
	\mu_i(\boldsymbol\theta) = A_{ik} \theta_k.
\end{equation*}
Allora il massimo della forma quadratica si ottiene per
\begin{equation*}
	\hat{\boldsymbol\theta} = (A^\transp V^{-1} A)^{-1} (A^\transp V^{-1}) \mathbf y.
\end{equation*}
Questo stimatore ha bias nullo:
\begin{equation*}
	E[\hat{\boldsymbol\theta}]
	= (A^\transp V^{-1} A)^{-1} (A^\transp V^{-1}) A \boldsymbol\theta
	= \boldsymbol\theta.
\end{equation*}

\begin{fact}[Teorema di Gauss-Markov]
	Se $V$ è effettivamente la matrice di covarianza,
	$\hat{\boldsymbol\theta}$ è lo stimatore lineare e con bias nullo di minima varianza, anche detto BLUE ovvero Best Linear Unbiased Estimator.
\end{fact}

\begin{exercise}
    %
    Calcolare la matrice di covarianza di $\hat{\boldsymbol\theta}$, nel
    caso in cui $V$ è la matrice di covarianza di $\mathbf y$.
    %
\end{exercise}

\begin{solution}
    %
    Primaditutto osserviamo che la matrice di covarianza di $M\mathbf y$~è
    %
    \begin{equation*}
        %
        C_{ij} = \cov[M_{ik} y_k, M_{jl} y_l] = (MVM^\top)_{ij}.
        %
    \end{equation*}
    %
    Quindi la matrice di covarianza di $\hat{\boldsymbol\theta}$ è
    %
    \begin{equation*}
        %
        (A^\transp V^{-1} A)^{-1} A^\transp V^{-1} V
        V^{-1} A (A^\transp V^{-1} A)^{-1}
        = (A^\transp V^{-1} A)^{-1},
        %
    \end{equation*}
    %
    dove abbiamo usato la simmetria di $V$ e $A^\transp V^{-1} A$.
    % 
\end{solution}
