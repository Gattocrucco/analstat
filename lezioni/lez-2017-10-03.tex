% Giacomo Petrillo
% lezione di Punzi

\subsection{Espansione in serie}

\begin{definition}[Matrice di covarianza]
	Date $n$ variabili $x_i$ si definisce \emph{matrice di covarianza} la matrice $n\times n$
	\begin{equation*}
		V_{ij} \is \cov[x_i,x_j].
	\end{equation*}
	Gli elementi diagonali sono le varianze e quelli fuori diagonale sono le covarianze.
\end{definition}

Data una statistica abbiamo visto come ricavarne esattamente la distribuzione;
può però essere utile ricavare solo alcuni momenti in modo approssimato.
Sia $\mathbf x\in\R^n$, sviluppiamo $s(\mathbf x)$ in serie intorno alla media:
\marginpar{C'è il problema che per calcolare la nuova media devo usare
la formula che c'è qui cioè $f(\mu) + \frac12f''(\mu)\sigma^2$, mentre se ho
fatto una misura $x$ che è uno stimatore unbiased di $\mu$ e voglio ottenere
una stima unbiased di $f(\mu)$ (cioè quello che serve in pratica la maggior
parte delle volte) devo usare $f(x) - \frac12f''(x)\sigma^2$.}
\begin{align*}
	s(\mathbf x) &= s(\boldsymbol \mu) + {}\\
	&+ \sum_i \left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu} (x_i - \mu_i) + {}\\
	&+ \frac12 \sum_{ij} \left.\frac{\partial^2 s}{\partial x_i \partial x_j}\right|_{\boldsymbol\mu} (x_i-\mu_i)(x_j-\mu_j) + {}\\
	&+ O(|\mathbf x - \boldsymbol\mu|^3).
\end{align*}
Calcoliamo la media e la varianza, ricordando che il valore atteso è lineare:
\begin{align*}
	E[s(\mathbf x)] &\approx s(\boldsymbol \mu) + {}\\
	&+ \sum_i \left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu} E[x_i - \mu_i] + {} & \Big\} = 0\\
	&+ \frac12 \sum_{ij} \left.\frac{\partial^2 s}{\partial x_i \partial x_j}\right|_{\boldsymbol\mu} E[(x_i-\mu_i)(x_j-\mu_j)] = \\
	&= s(\boldsymbol \mu) +
	\frac12 \sum_{ij} \left.\frac{\partial^2 s}{\partial x_i \partial x_j}\right|_{\boldsymbol\mu}
	\cov[x_i,x_j];
\end{align*}
\begin{align*}
	\var[s(\mathbf x)] &= E[(s - \avg s)^2] \approx \\
	&\approx E\left[
	\sum_i \left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu} (x_i - \mu_i) \cdot
	\sum_j \left.\frac{\partial s}{\partial x_j}\right|_{\boldsymbol\mu} (x_j - \mu_j) \right] = \\
	&= \sum_{ij}
	\left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu}
	\left.\frac{\partial s}{\partial x_j}\right|_{\boldsymbol\mu}
	E[ (x_i - \mu_i) (x_j - \mu_j) ] = \\
	&= \sum_{ij}
	\left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu}
	\left.\frac{\partial s}{\partial x_j}\right|_{\boldsymbol\mu}
	\cov[x_i,x_j].
\end{align*}
Nel caso che le $x_i$ siano scorrelate, questa approssimazione della varianza diventa
\begin{equation*}
	\var[s] \approx \sum_i
	\left( \left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu} \right)^2
	\var[x_i].
\end{equation*}
Probabilmente avrete già incontrato questa formula come ``formula di propagazione degli errori'';
sottolineiamo che è una del tutto generale approssimazione di un momento di una statistica e che non è detto sia sempre una buona approssimazione quando la usiamo per la propagazione degli errori (o, come preferiamo chiamarle, incertezze).

\begin{exercise}
	Mostrare che
	\begin{equation*}
		\cov[s_1(\mathbf x), s_2(\mathbf x)] \approx
		\sum_{ij}
		\left.\frac{\partial s_1}{\partial x_i}\right|_{\boldsymbol\mu}
		\left.\frac{\partial s_2}{\partial x_j}\right|_{\boldsymbol\mu}
		\cov[x_i,x_j].
	\end{equation*}
\end{exercise}

\subsection{Generatrice}

\begin{definition}[Funzione generatrice]
	Data la pdf $p(x)$, la sua \emph{funzione generatrice} è
	$M(t) \is E[e^{tx}]$.
\end{definition}

\noindent La funzione generatrice è utile per calcolare i momenti, infatti:
\begin{align*}
	M^{(n)}(t) &=
	\frac{\partial^n}{\partial t^n} E[e^{tx}] =
	E \left[ \frac{\partial^n}{\partial t^n} e^{tx} \right] =
	E[x^n e^{tx}]
	\intertext{che per $t=0$ diventa}
	&= E[x^n] = \mu_n(0).
\end{align*}
In base al \autoref{th:pdfmom} segue che la generatrice determina completamente la pdf.

Per due variabili indipendenti $x$, $y$ vale che
\begin{equation*}
	M_{x+y}(t) =
	E[e^{t(x+y)}] =
	E[e^{tx} e^{ty}] =
	E[e^{tx}] E[e^{ty}] =
	M_x(t) M_y(t);
\end{equation*}
inoltre ovviamente $M_{\alpha x}(t) = M_x(\alpha t)$.

\begin{example}
	Consideriamo una distribuzione uniforme in $(0,m)$:
	\begin{equation*}
		p(x) = \frac1m \chi_{(0,m)}(x) = \begin{cases}
			\frac 1m & 0 \le x \le m \\
			0  & \text{altrimenti.}
		\end{cases}
	\end{equation*}
	Calcoliamo la generatrice:
	\begin{align*}
		M(t) &= 
		\int \de x\, p(x) e^{tx} =
		\int_0^m \de x\, \frac{e^{tx}}{m} =
		\frac 1m \left[ \frac{e^{tx}}{t} \right]_{x=0}^m = \\
		&= \frac{e^{mt} - 1}{mt}.
	\end{align*}
	Sviluppiamo $M$ in serie:
	\begin{align*}
		M(t) &=
		\frac{mt + \frac{(mt)^2}2 + \frac{(mt)^3}{3!} + \dotsb}{mt} = \\
		&= 1 + \frac{mt}2 + \frac{(mt)^2}{3!} + \dotsb = \\
		&= \mu_0 + \mu_1t + \frac{\mu_2}2t^2 + \dotsb  \\
		\implies &\begin{cases}
			\mu_0 = 1 \\
			\mu_1 = \frac m2 \\
			\mu_2 = \frac{m^2}3 \\
			\vdots
		\end{cases}
	\end{align*}
	Dai primi momenti possiamo calcolare direttamente la varianza:
	\begin{equation*}
		\sigma^2 = \mu_2 - \mu_1^2 = \frac{m^2}{12}.
	\end{equation*}
\end{example}

\subsection{Caratteristica}

\begin{definition}[Funzione caratteristica]
	La \emph{funzione caratteristica} è la trasformata di Fourier della pdf:
	\begin{equation*}
		\phi(t) \is E[e^{itx}].
	\end{equation*}
\end{definition}
Dalla discussione sulle generatrici abbiamo immediatamente che
\begin{equation*}
	\phi_{x+y} = \phi_x\phi_y, \quad \mu_n = i^{-n} \phi^{(n)}(0).
\end{equation*}

\begin{fact}[Teorema di Paul-Levy]
	\label{th:paullevy}
	Consideriamo una successione $F_n$ di cumulanti e la successione $\phi_n$ delle corrispondenti caratteristiche.
	Se esiste il limite puntuale di $\phi_n$, cioè
	\begin{equation*}
		\forall t \quad \exists \lim_{n\to\infty} \phi_n(t) \isis \phi(t),
	\end{equation*}
	e il limite $\phi$ è continuo in 0, allora
	esiste una $F$ che è il limite puntuale di $F_n$ dove è continua, cioè
	\begin{equation*}
		\exists F \quad \forall x: \text{$F$ continua in $x$} \implies
		\lim_{n\to\infty} F_n(x) = F(x).
	\end{equation*}
\end{fact}

\section{Bernoulli, binomiale, poisson, esponenziale}

La distribuzione di una variabile che può assumere solo due valori si chiama \emph{distribuzione di Bernoulli}.
Fissiamo la notazione:
\begin{equation*}
	x \in \set{0,1}, \quad P(x) = \begin{cases}
		p & x = 1 \\
		1-p & x = 0.
	\end{cases}
\end{equation*}
Date $n$ variabili di Bernoulli indipendenti calcoliamo la probabilità che $k$ siano~1.
I~modi di disporre $k$ oggetti identici in $n$ posti sono $\binom nk$, quindi
\begin{equation*}
	P(k) = P(k;n,p) = \binom nk p^{k} (1-p)^{n-k};
\end{equation*}
questa distribuzione si chiama \emph{binomiale}.
Calcoliamone la generatrice:
\begin{align*}
	M_k(t) &=
	\sum_k e^{tk} P(k;n,p) =
	\sum_k \binom nk p^k e^{tk} (1-p)^{n-k} = \\
	&= (pe^t + 1-p)^n,
\end{align*}
ma in effetti la generatrice di una Bernoulli è
\begin{equation*}
	M_x(t) = pe^{1\cdot t} + (1-p)e^{0\cdot t} = pe^t + 1-p
\end{equation*}
e $k$ può essere scritto come $\sum_ix_i$, quindi deve essere $M_k = M_x^n$.
Estendiamo al caso in cui $x$ possa assumere più di due valori:
\begin{equation*}
	x \in \set{0,1,\dotsc}, \quad P(x) \isis p_x.
\end{equation*}
Date $n$ variabili come $x$,
calcoliamo la probabilità che $k_1$ siano $1$, che $k_2$ siano $2$ etc.
Ovviamente vale $k_0=n - \sum_{x\ge 1} k_x$.
I modi di estrarre degli insiemi di dimensione $k_x$ da un insieme di $n$ elementi sono contati dal \emph{coefficiente multinomiale}
\begin{equation*}
	\binom{n}{k_0,k_1,\dotsc} = \frac{n!}{k_0!k_1!\dotsm},
\end{equation*}
quindi la probabilità è
\begin{align*}
	P(k_1,k_2,\dotsc) = \frac{n!}{k_0!k_1!k_2!\dotsm} p_0^{k_0} p_1^{k_1} \dotsm.
\end{align*}
Notiamo che possiamo aggiungere $k_0$ alla probabilità usando una $\delta$ di Kronecker:
\begin{equation*}
	P(k_0,k_1,\dotsc) = \delta_{k_0,n-\sum_{x\ge 1}k_x} \frac{n!}{k_0!k_1!\dotsm} p_0^{k_0} p_1^{k_1} \dotsm.
\end{equation*}
Questa distribuzione si chiama \emph{multinomiale}.

Calcoliamo ora la probabilità che in un intervallo di tempo $T$ si verifichino $k$ eventi se per il singolo evento è definita una probabilità per unità di tempo $\lambda$.
Dividiamo l'intervallo $T$ in $N$ sottointervalli uguali.
<<Probabilità per unità di tempo>> significa che
la probabilità che in un sottointervallo si verifichi un solo evento è
\begin{equation*}
	p_1 = \lambda \frac TN + O\left(\frac1{N^2}\right),
\end{equation*}
quindi la probabilità $p_q$ che in un sottointervallo si verifichino $q>0$ eventi è
\begin{equation*}
	p_q = O\left(\frac1{N^q}\right),
\end{equation*}
e la probabilità che si verifichino 0 eventi è
\begin{equation*}
	p_0 = 1 - \lambda \frac TN + O\left(\frac1{N^2}\right).
\end{equation*}
Sia $n_q$ il numero di sottointervalli in cui si verificano $q$ eventi,
se $k$ è il numero totale di eventi vale ovviamente
\begin{equation*}
	k = \sum_{q\ge1} q n_q, \quad n_0 = N - \sum_{q\ge 1} n_q \isis N - s.
\end{equation*}
La distribuzione degli $n_q$ è la multinomiale
\begin{align*}
	P(n_1,n_2,\dotsc)
	&= \frac{N!}{n_0!n_1!\dotsm} p_0^{n_0} p_1^{n_1} \dotsm = \\
	&= \frac{N!}{(N - s)!}
	\left(1 - \lambda \frac TN + O\left(\frac1{N^2}\right)\right)^{N-s}
	\prod_{q\ge1} \frac{p_q^{n_q}}{n_q!} = \\
	&= O(N^s) e^{-\lambda T + O(1/N)}
	O\left(\frac1{N^k}\right) \prod_{q\ge1} \frac1{n_q!} = \\
	&= O\left(N^{\sum_{q\ge1} n_q - \sum_{q\ge1} qn_q}\right)
\end{align*}
che non va a zero solo se $n_q=0$ per $q > 1$ ovvero se $k=n_1$,
quindi alla fine la probabilità di avere $k$ eventi è data semplicemente da
\begin{align*}
	P(k)
	&= \frac{N!}{n_0!n_1!}p_0^{n_0}p_1^{n_1} = \\
	&= \frac{N!}{(N-k)!k!}
	\left(1 - \lambda \frac TN + O\left(\frac1{N^2}\right)\right)^{N-k}
	\left(\lambda \frac TN + O\left(\frac1{N^2}\right)\right)^k = \\
	&= \frac1{k!}e^{-\lambda T}(\lambda T)^k.
\end{align*}
Definendo $\mu\is\lambda T$, abbiamo
\begin{equation*}
	P(k;\mu) = \frac{\mu^k}{k!} e^{-\mu}.
\end{equation*}
Questa distribuzione si chiama \emph{poissoniana}.

Nello stesso contesto in cui abbiamo ricavato la poissoniana,
calcoliamo la pdf del tempo che intercorre tra un istante fissato $t=0$ e l'evento successivo.
Sia $F(t)$ la cumulante, si ha
\begin{align*}
	1-F(t+\de t)
	&= P(\text{$0$ eventi in $(0,t+\de t)$}) = \\
	&= P(\text{$0$ eventi in $(0,t)$}) \cdot P(\text{$0$ eventi in $(t,t+\de t)$}) = \\
	&= (1-F(t)) (1-\lambda\de t) = \\
	&= 1 - F(t) - \lambda(1-F(t))\de t \rightarrow \\
	&\rightarrow \frac{\de F}{\de t} = \lambda(1-F(t)) \rightarrow \\
	&\rightarrow F(t) = 1 - e^{-\lambda t} \rightarrow \\
	&\rightarrow p(t) = \frac{\de F}{\de t} = \lambda e^{-\lambda t}.
\end{align*}
Questa distribuzione si chiama \emph{esponenziale}.

% qui c'era la funzione caratteristica che ho spostato a dopo la generatrice
