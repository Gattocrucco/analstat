% Giacomo Petrillo
% lezione di Punzi

\subsection{Espansione in serie}

\begin{definition}[Matrice di covarianza]
	Date $n$ variabili $x_i$ si definisce \emph{matrice di covarianza} la matrice $n\times n$
	\begin{equation*}
		V_{ij} \is \cov[x_i,x_j].
	\end{equation*}
	Gli elementi diagonali sono le varianze e quelli fuori diagonale sono le covarianze.
\end{definition}

Data una statistica abbiamo visto come ricavarne esattamente la distribuzione;
può però essere utile ricavare solo alcuni momenti in modo approssimato.
Sia $\mathbf x\in\R^n$, sviluppiamo $s(\mathbf x)$ in serie intorno alla media:
%
\begin{align*}
	s(\mathbf x) &= s(\boldsymbol \mu) + {}\\
	&+ \sum_i \left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu} (x_i - \mu_i) + {}\\
	&+ \frac12 \sum_{ij} \left.\frac{\partial^2 s}{\partial x_i \partial x_j}\right|_{\boldsymbol\mu} (x_i-\mu_i)(x_j-\mu_j) + {}\\
	&+ O(|\mathbf x - \boldsymbol\mu|^3).
\end{align*}
Calcoliamo la media e la varianza, ricordando che il valore atteso è lineare:
\begin{align*}
	E[s(\mathbf x)] &\approx s(\boldsymbol \mu) + {}\\
	&+ \sum_i \left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu} E[x_i - \mu_i] + {} & \Big\} = 0\\
	&+ \frac12 \sum_{ij} \left.\frac{\partial^2 s}{\partial x_i \partial x_j}\right|_{\boldsymbol\mu} E[(x_i-\mu_i)(x_j-\mu_j)] = \\
	&= s(\boldsymbol \mu) +
	\frac12 \sum_{ij} \left.\frac{\partial^2 s}{\partial x_i \partial x_j}\right|_{\boldsymbol\mu}
	\cov[x_i,x_j]; \refstepcounter{equation}\tag{\theequation}\label{eq:meantaylor}
\end{align*}
\begin{align*}
	\var[s(\mathbf x)] &= E[(s - \avg s)^2] \approx \\
	&\approx E\left[
	\sum_i \left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu} (x_i - \mu_i) \cdot
	\sum_j \left.\frac{\partial s}{\partial x_j}\right|_{\boldsymbol\mu} (x_j - \mu_j) \right] = \\
	&= \sum_{ij}
	\left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu}
	\left.\frac{\partial s}{\partial x_j}\right|_{\boldsymbol\mu}
	E[ (x_i - \mu_i) (x_j - \mu_j) ] = \\
	&= \sum_{ij}
	\left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu}
	\left.\frac{\partial s}{\partial x_j}\right|_{\boldsymbol\mu}
	\cov[x_i,x_j].
\end{align*}
Nel caso che le $x_i$ siano scorrelate, questa approssimazione della varianza diventa
\begin{equation*}
	\var[s] \approx \sum_i
	\left( \left.\frac{\partial s}{\partial x_i}\right|_{\boldsymbol\mu} \right)^2
	\var[x_i].
\end{equation*}
Probabilmente avrete già incontrato questa formula come ``formula di propagazione degli errori'';
sottolineiamo che è una del tutto generale approssimazione di un momento di una statistica e che non è detto sia sempre una buona approssimazione quando la usiamo per la propagazione degli errori (o, come preferiamo chiamarle, incertezze).

\begin{exercise}
	Mostrare che
	\begin{equation*}
		\cov[s_1(\mathbf x), s_2(\mathbf x)] \approx
		\sum_{ij}
		\left.\frac{\partial s_1}{\partial x_i}\right|_{\boldsymbol\mu}
		\left.\frac{\partial s_2}{\partial x_j}\right|_{\boldsymbol\mu}
		\cov[x_i,x_j].
	\end{equation*}
\end{exercise}

\begin{exercise}
    %
    La formula \eqref{eq:meantaylor} serve per stimare la media di $s(\mathbf
    x)$ se so la media e la matrice di covarianza di $\mathbf x$. Supponiamo
    che $\mathbf x$ sia una misura, cioè stiamo misurando una quantità
    incognita $\mathbf x_0$ e $\mathbf x$ è una misura con un certo errore che
    solo in media dà il risultato corretto, ovvero $E[\mathbf x] = \mathbf
    x_0$. Se in questo senso vogliamo ottenere una misura di $s(\mathbf x_0)$ a
    partire da quella di $\mathbf x_0$, ad esempio $\mathbf x_0$ potrebbe
    essere la velocità e $s$ l'energia cinetica, che formula bisogna usare?
    %
\end{exercise}

\begin{solution}
    %
    La formula per la propagazione della media al second'ordine
    \eqref{eq:meantaylor} è
    %
    \begin{equation*}
        %
        E[s(\mathbf x)] \approx s(\boldsymbol\mu) +
        \frac12 \sum_{ij}
        \left.\frac{\partial^2 s}{\partial x_i \partial x_j}
        \right|_{\boldsymbol\mu}
        \operatorname{Cov}[x_i, x_j],
        %
    \end{equation*}
    %
    dove $\boldsymbol\mu = E[\mathbf x]$. Quindi, nella notazione
    dell'esercizio, $\mathbf x_0 = \boldsymbol\mu$. Noi vogliamo una quantità
    che si calcoli usando $\mathbf x$, e non $\mathbf x_0$ che è ignota,
    e che misuri $s(\mathbf x_0)$, cioè in base alla definizione dell'esercizio
    che abbia valore atteso uguale a $s(\mathbf x_0)$. Poiché quest'ultima
    quantità compare nella formula di propagazione come $s(\boldsymbol\mu)$,
    proviamo a portare dall'altra parte la sommatoria per isolarla:
    %
    \begin{equation*}
        %
        s(\boldsymbol\mu) \approx E[s(\mathbf x)] -
        \frac12 \sum_{ij}
        \left.\frac{\partial^2 s}{\partial x_i \partial x_j}
        \right|_{\boldsymbol\mu}
        \operatorname{Cov}[x_i, x_j].
        %
    \end{equation*}
    %
    Noi vogliamo $s(\boldsymbol\mu) = E[\text{funzione di $\mathbf x$}]$,
    quindi cerchiamo di portare dentro la sommatoria. La derivata va calcolata
    in $\boldsymbol\mu$, ma allo stesso modo in cui al primo ordine $E[f(x)]
    \approx f(E[x])$, abbiamo $E[f''(x)] \approx f''(E[x])$, quindi la
    possiamo valutare in~$\mathbf x$:
    %
    \begin{align*}
        %
        \hat s(\mathbf x) &\is s(\mathbf x) -
        \frac12 \sum_{ij}
        \left.\frac{\partial^2 s}{\partial x_i \partial x_j}
        \right|_{\mathbf x}
        \operatorname{Cov}[x_i, x_j], \\
        %
        E[\hat s(\mathbf x)] &\approx s(\boldsymbol\mu) = s(\mathbf x_0).
        %
    \end{align*}
    %
    In pratica la formula per $\hat s(\mathbf x)$ è simile a quella per
    $E[s(\mathbf x)]$ però con il segno meno. Di conseguenza in base alla
    situazione bisogna stare attenti a capire qual è il segno che serve.
    %
\end{solution}

\subsection{Generatrice}

\begin{definition}[Funzione generatrice]
	Data la pdf $p(x)$, la sua \emph{funzione generatrice} è
	$M(t) \is E[e^{tx}]$.
\end{definition}

\noindent La funzione generatrice è utile per calcolare i momenti, infatti:
\begin{align*}
	M^{(n)}(t) &=
	\frac{\partial^n}{\partial t^n} E[e^{tx}] =
	E \left[ \frac{\partial^n}{\partial t^n} e^{tx} \right] =
	E[x^n e^{tx}]
	\intertext{che per $t=0$ diventa}
	&= E[x^n] = \mu_n(0).
\end{align*}
In base al \autoref{th:pdfmom} segue che la generatrice determina completamente la pdf.

Per due variabili indipendenti $x$, $y$ vale che
\begin{equation*}
	M_{x+y}(t) =
	E[e^{t(x+y)}] =
	E[e^{tx} e^{ty}] =
	E[e^{tx}] E[e^{ty}] =
	M_x(t) M_y(t);
\end{equation*}
inoltre ovviamente $M_{\alpha x}(t) = M_x(\alpha t)$.

\begin{example}
	Consideriamo una distribuzione uniforme in $(0,m)$:
	\begin{equation*}
		p(x) = \frac1m \chi_{(0,m)}(x) = \begin{cases}
			\frac 1m & 0 \le x \le m \\
			0  & \text{altrimenti.}
		\end{cases}
	\end{equation*}
	Calcoliamo la generatrice:
	\begin{align*}
		M(t) &= 
		\int \de x\, p(x) e^{tx} =
		\int_0^m \de x\, \frac{e^{tx}}{m} =
		\frac 1m \left[ \frac{e^{tx}}{t} \right]_{x=0}^m = \\
		&= \frac{e^{mt} - 1}{mt}.
	\end{align*}
	Sviluppiamo $M$ in serie:
	\begin{align*}
		M(t) &=
		\frac{mt + \frac{(mt)^2}2 + \frac{(mt)^3}{3!} + \dotsb}{mt} = \\
		&= 1 + \frac{mt}2 + \frac{(mt)^2}{3!} + \dotsb = \\
		&= \mu_0 + \mu_1t + \frac{\mu_2}2t^2 + \dotsb  \\
		\implies &\begin{cases}
			\mu_0 = 1 \\
			\mu_1 = \frac m2 \\
			\mu_2 = \frac{m^2}3 \\
			\vdots
		\end{cases}
	\end{align*}
	Dai primi momenti possiamo calcolare direttamente la varianza:
	\begin{equation*}
		\sigma^2 = \mu_2 - \mu_1^2 = \frac{m^2}{12}.
	\end{equation*}
\end{example}

\subsection{Caratteristica}

\begin{definition}[Funzione caratteristica]
	La \emph{funzione caratteristica} è la trasformata di Fourier della pdf:
	\begin{equation*}
		\phi(t) \is E[e^{itx}].
	\end{equation*}
\end{definition}
Dalla discussione sulle generatrici abbiamo immediatamente che
\begin{equation*}
	\phi_{x+y} = \phi_x\phi_y, \quad \mu_n = i^{-n} \phi^{(n)}(0).
\end{equation*}

\begin{fact}[Teorema di Paul-Levy]
	\label{th:paullevy}
	Consideriamo una successione $F_n$ di cumulanti e la successione $\phi_n$ delle corrispondenti caratteristiche.
	Se esiste il limite puntuale di $\phi_n$, cioè
	\begin{equation*}
		\forall t \quad \exists \lim_{n\to\infty} \phi_n(t) \isis \phi(t),
	\end{equation*}
	e il limite $\phi$ è continuo in 0, allora
	esiste una $F$ che è il limite puntuale di $F_n$ dove è continua, cioè
	\begin{equation*}
		\exists F \quad \forall x: \text{$F$ continua in $x$} \implies
		\lim_{n\to\infty} F_n(x) = F(x).
	\end{equation*}
\end{fact}

\section{Bernoulli, binomiale, Poisson, esponenziale}

La distribuzione di una variabile che può assumere solo due valori si chiama \emph{distribuzione di Bernoulli}.
Fissiamo la notazione:
\begin{equation*}
	x \in \set{0,1}, \quad P(x) = \begin{cases}
		p & x = 1 \\
		1-p & x = 0.
	\end{cases}
\end{equation*}
Date $n$ variabili di Bernoulli indipendenti calcoliamo la probabilità che $k$ siano~1.
I~modi di disporre $k$ oggetti identici in $n$ posti sono $\binom nk$, quindi
\begin{equation*}
	P(k) = P(k;n,p) = \binom nk p^{k} (1-p)^{n-k};
\end{equation*}
questa distribuzione si chiama \emph{binomiale}.
Calcoliamone la generatrice:
\begin{align*}
	M_k(t) &=
	\sum_k e^{tk} P(k;n,p) =
	\sum_k \binom nk p^k e^{tk} (1-p)^{n-k} = \\
	&= (pe^t + 1-p)^n,
\end{align*}
ma in effetti la generatrice di una Bernoulli è
\begin{equation*}
	M_x(t) = pe^{1\cdot t} + (1-p)e^{0\cdot t} = pe^t + 1-p
\end{equation*}
e $k$ può essere scritto come $\sum_ix_i$, quindi deve essere $M_k = M_x^n$.
Estendiamo al caso in cui $x$ possa assumere più di due valori:
\begin{equation*}
	x \in \set{0,1,\dotsc}, \quad P(x) \isis p_x.
\end{equation*}
Date $n$ variabili come $x$,
calcoliamo la probabilità che $k_1$ siano $1$, che $k_2$ siano $2$ etc.
Ovviamente vale $k_0=n - \sum_{x\ge 1} k_x$.
I modi di estrarre degli insiemi di dimensione $k_x$ da un insieme di $n$ elementi sono contati dal \emph{coefficiente multinomiale}
\begin{equation*}
	\binom{n}{k_0,k_1,\dotsc} = \frac{n!}{k_0!k_1!\dotsm},
\end{equation*}
quindi la probabilità è
\begin{align*}
	P(k_1,k_2,\dotsc) = \frac{n!}{k_0!k_1!k_2!\dotsm} p_0^{k_0} p_1^{k_1} \dotsm.
\end{align*}
Notiamo che possiamo aggiungere $k_0$ alla probabilità usando una $\delta$ di Kronecker:
\begin{equation*}
	P(k_0,k_1,\dotsc) = \delta_{k_0,n-\sum_{x\ge 1}k_x} \frac{n!}{k_0!k_1!\dotsm} p_0^{k_0} p_1^{k_1} \dotsm.
\end{equation*}
Questa distribuzione si chiama \emph{multinomiale}.

%%%%%% vecchia derivazione della poissoniana %%%%%%
% Calcoliamo ora la probabilità che in un intervallo di tempo $T$ si verifichino $k$ eventi se per il singolo evento è definita una probabilità per unità di tempo $\lambda$.
% \marginpar{Modi alternativi di ricavare la Poisson (e poi l'esponenziale):
% 1) chiedere l'invarianza per somma di variabili (lo fa Del Prete), 2) prendere $N$
% estrazioni di una uniforme su $(0,T)$ e fare il limite $N\to\infty$ con $N/T =
% \lambda$. Direi che queste due sono più chiare del poccio che sto facendo qui.
% La più chiara è la 2) perché è formale ma è anche costruttiva, la 1) è più
% elegante ma anche più astratta.}
% Dividiamo l'intervallo $T$ in $N$ sottointervalli uguali.
% <<Probabilità per unità di tempo>> significa che
% la probabilità che in un sottointervallo si verifichi un solo evento è
% \begin{equation*}
%     p_1 = \lambda \frac TN + O\left(\frac1{N^2}\right),
% \end{equation*}
% quindi la probabilità $p_q$ che in un sottointervallo si verifichino $q>0$ eventi è
% \begin{equation*}
%     p_q = O\left(\frac1{N^q}\right),
% \end{equation*}
% e la probabilità che si verifichino 0 eventi è
% \begin{equation*}
%     p_0 = 1 - \lambda \frac TN + O\left(\frac1{N^2}\right).
% \end{equation*}
% Sia $n_q$ il numero di sottointervalli in cui si verificano $q$ eventi,
% se $k$ è il numero totale di eventi vale ovviamente
% \begin{equation*}
%     k = \sum_{q\ge1} q n_q, \quad n_0 = N - \sum_{q\ge 1} n_q \isis N - s.
% \end{equation*}
% La distribuzione degli $n_q$ è la multinomiale
% \begin{align*}
%     P(n_1,n_2,\dotsc)
%     &= \frac{N!}{n_0!n_1!\dotsm} p_0^{n_0} p_1^{n_1} \dotsm = \\
%     &= \frac{N!}{(N - s)!}
%     \left(1 - \lambda \frac TN + O\left(\frac1{N^2}\right)\right)^{N-s}
%     \prod_{q\ge1} \frac{p_q^{n_q}}{n_q!} = \\
%     &= O(N^s) e^{-\lambda T + O(1/N)}
%     O\left(\frac1{N^k}\right) \prod_{q\ge1} \frac1{n_q!} = \\
%     &= O\left(N^{\sum_{q\ge1} n_q - \sum_{q\ge1} qn_q}\right)
% \end{align*}
% che non va a zero solo se $n_q=0$ per $q > 1$ ovvero se $k=n_1$,
% quindi alla fine la probabilità di avere $k$ eventi è data semplicemente da
% \begin{align*}
%     P(k)
%     &= \frac{N!}{n_0!n_1!}p_0^{n_0}p_1^{n_1} = \\
%     &= \frac{N!}{(N-k)!k!}
%     \left(1 - \lambda \frac TN + O\left(\frac1{N^2}\right)\right)^{N-k}
%     \left(\lambda \frac TN + O\left(\frac1{N^2}\right)\right)^k = \\
%     &= \frac1{k!}e^{-\lambda T}(\lambda T)^k.
% \end{align*}
% Definendo $\mu\is\lambda T$, abbiamo
% \begin{equation*}
%     P(k;\mu) = \frac{\mu^k}{k!} e^{-\mu}.
% \end{equation*}
% Questa distribuzione si chiama \emph{poissoniana}.

Ora vogliamo studiare come si distribuiscono più variabili reali indipendenti
e con probabilità uniforme ovunque, ovvero detto con il tempo, come si
distribuiscono i tempi di avvenimenti indipendenti e casuali che hanno uguale
probabilità di avvenire in qualsiasi momento. Formalizziamo questo concetto
come $N$ estrazioni $(x_1,\ldots,x_N)$ indipendenti da una distribuzione
uniforme sull'intervallo $(0,L)$, cioè $p(x_i) = \chi_{(0,L)}(x_i)/L$, nel
limite in cui $L$ e $N$ vanno a infinito mantenendo un rapporto costante
$\lambda \is N/L$, cioè $\lambda$ è il numero di eventi per unità di tempo.

Vogliamo calcolare la probabilità che avvengano $k$ e solo $k$ eventi in un
tempo $T$. La probabilità che una singola $x_i$ cada in un'intervallo lungo
$T$ è $T/L$, quindi la probabilità che $k$ lo facciano è data dalla binomiale:
%
\begin{align*}
    %
    P(k) &= \binom Nk \left(\frac TL\right)^k \left(1-\frac TL\right)^{N-k}
    = \binom Nk \left(\frac{\lambda T}N\right)^k \left(1-\frac{\lambda T}N\right)^{N-k} = \\
    %
    &= \binom Nk \left(\frac\mu N\right)^k \left(1-\frac\mu N\right)^{N-k},
    %
\end{align*}
%
definendo $\mu \is \lambda T$. Facendo il limite, viene
%
\begin{align*}
    %
    P(k;\mu) &= \lim_{N\to\infty}
    \binom Nk \left(\frac\mu N\right)^k \left(1-\frac\mu N\right)^{N-k} = \\
    %
    &= \lim_{N\to\infty}
    \frac NN \frac{N-1}N \cdots \frac{N-k+1}N
    \frac{\mu^k}{k!}
    \left(1-\frac\mu N\right)^{N-k} = \\
    %
    &= \frac{\mu^k}{k!} e^{-\mu}.
    %
\end{align*}
%
Questa distribuzione di chiama \emph{poissoniana}.

Nello stesso contesto in cui abbiamo ricavato la poissoniana,\marginpar{Potrei
ricavare l'esponenziale in modo pulito come distribuzione della differenza tra
due $x_i$ successive, dovrebbe venire la distribuzione $\beta(1,N)$ riscalata.}
calcoliamo la pdf del tempo che intercorre tra un istante fissato $t=0$ e
l'evento successivo. Sia $F(t)$ la cumulante, si ha
%
\begin{align*}
	1-F(t+\de t)
	&= P(\text{$0$ eventi in $(0,t+\de t)$}) = \\
	&= P(\text{$0$ eventi in $(0,t)$}) \cdot P(\text{$0$ eventi in $(t,t+\de t)$}) = \\
	&= (1-F(t)) (1-\lambda\de t) = \\
	&= 1 - F(t) - \lambda(1-F(t))\de t \rightarrow \\
	&\rightarrow \frac{\de F}{\de t} = \lambda(1-F(t)) \rightarrow \\
	&\rightarrow F(t) = 1 - e^{-\lambda t} \rightarrow \\
	&\rightarrow p(t) = \frac{\de F}{\de t} = \lambda e^{-\lambda t}.
\end{align*}
Questa distribuzione si chiama \emph{esponenziale}.

% qui c'era la funzione caratteristica che ho spostato a dopo la generatrice
