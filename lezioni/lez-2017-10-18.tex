% Giacomo Petrillo
% lezione di Francavilla

\begin{exercise}
	\label{th:troncexp}
	Consideriamo un'esponenziale troncata a destra:
	\begin{equation*}
		p(t;\lambda)
		= \frac{\lambda e^{-\lambda t}}{1 - e^{-\lambda T}},
		\quad t \in (0,T).
	\end{equation*}
	Calcolare l'informazione di Fisher per $\lambda$.
\end{exercise}

\begin{solution*}
	\begin{align*}
		\log p(t;\lambda)
		&= \log\lambda - \lambda t - \log(1-e^{-\lambda T}) \\
		\frac{\partial}{\partial\lambda} \log p(t;\lambda)
		&= \frac1\lambda - t - \frac{-(-T)e^{-\lambda T}}{1-e^{-\lambda T}} = \\
		&= \frac1\lambda - t - \frac{T}{e^{\lambda T}-1} \\
		-\frac{\partial^2}{\partial\lambda^2} \log p(t;\lambda)
		&= \frac1{\lambda^2} - \frac{T^2e^{\lambda T}}{(e^{\lambda T}-1)^2}.
	\end{align*}
	Notiamo che per $\lambda T\to0$ l'informazione va a zero mentre per $\lambda T\to\infty$
	riotteniamo l'informazione dell'esponenziale.
	Calcoliamo la variazione relativa rispetto a quest'ultima:
	\begin{equation*}
		\frac{I_T(\lambda)}{I_\infty(\lambda)}
		= \lambda^2 I_T(\lambda)
		= 1 - \frac {(\lambda T)^2e^{\lambda T}} {(e^{\lambda T}-1)^2}.
	\end{equation*}
	Ad esempio, per $\lambda T=5$ l'informazione cala del \SI{17}\percent.
	Se immaginiamo di perdere bernouillianamente una frazione
	\begin{equation*}
		\epsilon
		= \int_T^\infty \de t\, \lambda e^{-\lambda t}
		= e^{-\lambda T}
	\end{equation*}
	delle misure di un'esponenziale,
	la variazione relativa dell'informazione di Fisher è~$-\epsilon$
	che per $\lambda T=5$ vale \SI{0.7}\percent.
	\marginpar{Questa cosa non è chiarissima perché se immagino di misurare il tempo tra un evento e l'altro e non osservo un evento con probabilità $\epsilon$ (vedi \autoref{th:salvexp}) l'informazione di Fisher non cambia.
	Quello che qui si intende è come se facessi le misure e poi ne buttassi via alcune (tipo trigger).}
\end{solution*}

\begin{exercise}
	Calcolare l'informazione di Fisher di un'esponenziale troncata a sinistra.
\end{exercise}

\begin{solution}
	Scriviamo la pdf:
	\begin{align*}
		p(t;\lambda)
		&= \frac {\lambda e^{-\lambda t}} {\int_T^\infty \de t\, \lambda e^{-\lambda t}} = \\
		&= \lambda e^{-\lambda (t-T)}, \quad t\in(T,\infty).
	\end{align*}
	Com'era intuitivo è un'esponenziale traslata quindi l'informazione di Fisher è ancora~$1/\lambda^2$.
\end{solution}

\begin{exercise}
	\label{th:avgsuffgauss}
	La media aritmetica è una statistica sufficiente per la media della gaussiana?
\end{exercise}

\begin{solution*}
	Sì:
	\begin{align*}
		p(\mathbf x;\mu)
		&= \prod_{i=1}^N \frac1{\sqrt{2\pi}\sigma} 
		\exp \left( -\frac12 \left( \frac{x_i-\mu}\sigma \right)^2 \right) = \\
		&= \left( \frac1{\sqrt{2\pi}\sigma} \right)^N
		\exp \left( -\frac12 \sum_{i=1}^N \left( \frac{x_i-\mu}\sigma \right)^2 \right) = \\
		&\left[ \sum_{i=1}^N (x_i-\mu)^2
		= \sum_{i=1}^N x_i^2 + N\mu^2 - 2\mu N\bar x \right] \\
		&= \left( \frac1{\sqrt{2\pi}\sigma} \right)^N
		\exp \left( -\frac12 \sum_{i=1}^N \frac{x_i^2}{\sigma^2} \right)
		\cdot \exp \left( -\frac{N\mu^2-2\mu N\bar x}{2\sigma^2} \right) = \\
		&= f(\mathbf x) \cdot g(\bar x,\mu).
	\end{align*}
\end{solution*}

\begin{exercise}
	\label{th:rmssuffgauss}
	Lo scarto quadratico medio è una statistica sufficiente per la deviazione standard della gaussiana?
\end{exercise}

\begin{solution*}
	È ancor più immediato dell'\autoref{th:avgsuffgauss}:
	\begin{align*}
		\hat\sigma^2(\mathbf x)
		&\is \frac1N \sum_{i=1}^N (x_i-\mu)^2 \\
		p(\mathbf x;\sigma)
		&= \left( \frac1{\sqrt{2\pi}\sigma} \right)^N
		\exp \left( -\frac12 \sum_{i=1}^N \left( \frac{x_i-\mu}\sigma \right)^2 \right) = \\
		&= \left( \frac1{\sqrt{2\pi}\sigma} \right)^N
		\exp \left( -\frac12 \frac{N\hat\sigma^2}{\sigma^2} \right) = \\
		&= g(\hat\sigma,\sigma).
	\end{align*}
\end{solution*}

\begin{exercise}
	Verificare se la media aritmetica è sufficiente per la media delle distribuzioni
	binomiale,
	poissoniana,
	esponenziale.
\end{exercise}

\begin{solution}
	È sufficiente per tutte e tre.
	\begin{description}
		\item[Binomiale]
		La media è $np$, fissiamo $n$ e usiamo $p$ come parametro:
		\begin{align*}
			p(\mathbf k;p)
			&= \prod_{i=1}^N \binom{n}{k_i} p^{k_i} (1-p)^{n-k_i} = \\
			&= \left( \prod_{i=1}^N \binom{n}{k_i} \right) \cdot
			p^{N\bar k} (1-p)^{Nn-N\bar k}.
		\end{align*}
		\item[Poissoniana]
		\begin{align*}
			p(\mathbf k;\mu)
			&= \prod_{i=1}^N \frac{\mu^{k_i}}{k_i!}e^{-\mu} = \\
			&= \left( \prod_{i=1}^N \frac1{k_i!} \right) \cdot \mu^{N\bar k} e^{-N\mu}.
		\end{align*}
		\item[Esponenziale]
		La media è $1/\lambda$, usiamo $\lambda$ come parametro:
		\begin{align*}
			p(\mathbf t;\lambda)
			&= \prod_{i=1}^N \lambda e^{-\lambda t_i} = \\
			&= \lambda^N e^{-N\lambda\bar t}.
		\end{align*}
	\end{description}
\end{solution}

\begin{exercise}
	Applicare il teorema di Darmois (\autoref{th:darmois}) alla gaussiana separatamente per la media e per la varianza.
\end{exercise}

\begin{solution*}
	Dobbiamo scrivere la pdf nella forma:
	\begin{equation*}
		p(x;\theta)
		= \exp \left( \sum_{k=1}^d \alpha_k(x)a_k(\theta) + \beta(x) + \gamma(\theta) \right).
	\end{equation*}
	Procediamo per la media:
	\begin{align*}
		p(x;\mu)
		&= \frac1{\sqrt{2\pi}\sigma}
		\exp\left( -\frac12 \left(\frac{x-\mu}\sigma\right)^2 \right) = \\
		&= \frac1{\sqrt{2\pi}\sigma}
		\exp\left( \frac{-x^2-\mu^2+2\mu x}{2\sigma^2} \right) = \\
		&= \exp \left( x\frac\mu{\sigma^2} - \frac{x^2}{2\sigma^2}
		 - \frac{\mu^2}{2\sigma^2} - \log(\sqrt{2\pi}\sigma) \right),
	\end{align*}
	quindi le varie funzioni del teorema di Darmois sono:
	\begin{align*}
		\alpha(x) &= x \hspace{5em} (d = 1)\\
		a(\mu) &= \frac{\mu}{\sigma^2} \\
		\beta(x) &= - \frac{x^2}{2\sigma^2} \\
		\gamma(\mu) &= - \frac{\mu^2}{2\sigma^2} - \log(\sqrt{2\pi}\sigma),
	\end{align*}
	da cui ricaviamo la statistica sufficiente:
	\begin{equation*}
		s(\mathbf x)
		= \sum_{i=1}^N \alpha(x_i)
		= N\bar x.
	\end{equation*}
	Come già sapevamo, la media aritmetica è sufficiente,
	quindi se a meno di cambi di variabile esiste una sola statistica sufficiente,
	deve essere la media aritmetica.
	
	Vediamo per la varianza:
	\begin{align*}
		p(x;\sigma)
		&= \exp \left( (2x\mu-x^2)\frac1{2\sigma^2}
		- \frac{\mu^2}{2\sigma^2}
		-\log(\sqrt{2\pi}\sigma) \right) \\
		\implies s(\mathbf x)
		&= \sum_{i=1}^N (2x_i\mu-x_i^2) = \\
		&= -\sum_{i=1}^N (x_i-\mu)^2 + \mu^2 = \\
		&= \mu^2 - N\hat\sigma^2.
	\end{align*}
\end{solution*}

\begin{exercise}
	Trovare una statistica sufficiente simultaneamente per media e varianza della gaussiana.
\end{exercise}

\begin{solution}
	Usiamo il teorema di Darmois:
	\begin{align*}
		p(x;\mu,\sigma)
		&= \exp \left( \frac{-x^2-\mu^2+2\mu x}{2\sigma^2} - \log(\sqrt{2\pi}\sigma) \right) = \\
		&= \exp \left( -x^2\frac1{2\sigma^2}
		+ x\frac\mu{\sigma^2}
		-\frac{\mu^2}{2\sigma^2} - \log(\sqrt{2\pi}\sigma) \right);
	\end{align*}
	le varie funzioni sono:
	\begin{align*}
		a_{1,2}(x) &= x^2, x \hspace{6em} (d=2)\\
		\alpha_{1,2}(\mu,\sigma) &= -\frac1{2\sigma^2}, \frac\mu{\sigma^2} \\
		\beta(x) &= 0 \\
		\gamma(\mu,\sigma) &= -\frac{\mu^2}{2\sigma^2} - \log(\sqrt{2\pi}\sigma)
	\end{align*}
	quindi la statistica sufficiente è
	\begin{equation*}
		\mathbf s(\mathbf x) = \sum_{i=1}^N \begin{pmatrix} x_i^2 \\ x_i \end{pmatrix}
	\end{equation*}
	dove la somma si intende componente per componente.
	Questa statistica ha l'utile proprietà che può essere calcolata leggendo una $x$ alla volta e aggiungendo $(x^2,x)$ alla somma parziale, quindi non serve salvare tutte le misure.
	Notiamo che con un cambio di variabili possiamo scrivere la statistica in un modo più familiare:
	\begin{align*}
		\mathbf S(\mathbf s)
		&= \begin{pmatrix}
			s_2 / N \\
			s_1 / N - s_2^2 / N^2
		\end{pmatrix} \\
		S_1(\mathbf s(\mathbf x)) 
		&= \frac{\sum_ix_i}N = \bar x \\
		S_2(\mathbf s(\mathbf x))
		&= \frac{\sum_ix_i^2}N - \frac{\big(\sum_ix_i\big)^2}{N^2} = \\
		&= \frac{\sum_ix_i^2}N - 2\frac{\big(\sum_ix_i\big)^2}{N^2} + \frac{\big(\sum_ix_i\big)^2}{N^2} = \\
		&= \frac1N\sum_ix_i^2 - \frac1N2\bar x\sum_ix_i + \bar x^2 = \\
		&= \frac1N\sum_i(x_i-\bar x)^2.
	\end{align*}
\end{solution}

\begin{exercise}
	Applicare Darmois a binomiale, poissoniana, esponenziale.
\end{exercise}

\begin{exercise}
	Calcolare l'informazione di Fisher della poissoniana e della binomiale e verificare che la statistica sufficiente ha la stessa informazione.
\end{exercise}

\begin{solution}
	Calcoliamo l'informazione di Fisher per una estrazione della poissoniana:
	\begin{align*}
		\log p(k;\mu)
		&= k\log\mu - \log(k!) - \mu \\
		\frac{\partial}{\partial\mu} \log p(k;\mu)
		&= \frac k\mu - 1 \\
		-\frac{\partial^2}{\partial\mu^2} \log p(k;\mu)
		&= \frac k{\mu^2} \\
		I(\mu)
		&= \frac{E[k]}{\mu^2}
		= \frac1\mu.
	\end{align*}
	La statistica sufficiente è la somma.
	La somma di poissoniane è poissoniana
	\begin{equation*}
		p(s;\mu)
		= \frac{(N\mu)^s}{s!}e^{-N\mu};
	\end{equation*}
	l'informazione di Fisher ha una derivata al quadrato nel parametro quindi
	\begin{equation*}
		I(\mu)
		= N^2 I(N\mu),
	\end{equation*}
	ma $I(N\mu)$ è quella poissoniana quindi
	\begin{equation*}
		I(\mu) = N^2 \frac1{N\mu} = \frac N\mu
	\end{equation*}
	che è l'informazione di $N$ estrazioni della poissoniana.
	Procediamo con la binomiale:
	\begin{align*}
		\log p(k;p)
		&= \log\binom nk + k\log p + (n-k)\log(1-p) \\
		\frac{\partial}{\partial p} \log p(k;p)
		&= \frac kp - \frac{n-k}{1-p} \\
		-\frac{\partial^2}{\partial p^2} \log p(k;p)
		&= \frac k{p^2} + \frac{n-k}{(1-p)^2} \\
		I(p)
		&= \frac{np}{p^2} + \frac{n-np}{(1-p)^2} = \\
		&= \frac n{p(1-p)}.
	\end{align*}
	Per come abbiamo definito la binomiale, la somma di binomiali è binomiale sommando gli $n$,
	quindi otteniamo l'informazione moltiplicando per $N$ esattamente come per le $N$ estrazioni.
\end{solution}

\begin{exercise}
	\label{th:fishgauss}
	Calcolare l'informazione di Fisher della gaussiana per $(\mu,\sigma)$ e per $(\mu,\sigma^2)$.
\end{exercise}

\begin{solution}
	Procediamo per $(\mu,\sigma)$:
	\begin{align*}
		L &\is \log p(x;\mu,\sigma) \\
		L &= -\frac12 \log(2\pi) - \log\sigma - \frac12\left(\frac{x-\mu}{\sigma}\right)^2 \\
		\intertext{calcoliamo le derivate prime:}
		\frac{\partial L}{\partial\mu}
		&= \frac{x-\mu}{\sigma^2} \\
		\frac{\partial L}{\partial\sigma}
		&= -\frac1\sigma - \frac12(x-\mu)^2 \frac{-2}{\sigma^3} = \\
		&= \frac{(x-\mu)^2}{\sigma^3} - \frac1\sigma \\
		\intertext{calcoliamo le derivate seconde:}
		-\frac{\partial^2 L}{\partial\mu^2}
		&= \frac1{\sigma^2} \\
		-\frac{\partial^2 L}{\partial\sigma^2}
		&= 3\frac{(x-\mu)^2}{\sigma^4} - \frac1{\sigma^2} \\
		-\frac{\partial^2 L}{\partial\mu\partial\sigma}
		&= 2\frac{x-\mu}{\sigma^3}
		\intertext{per ricavare l'informazione prendiamo il valore atteso:}
		I(\mu,\sigma)
		&= \begin{pmatrix}
			\frac1{\sigma^2} & 0 \\
			0 & \frac2{\sigma^2}
		\end{pmatrix}
		\intertext{passiamo direttamente da $\sigma$ a $\sigma^2$:}
		I_{\sigma^2\sigma^2}
		&= E \left[ \left( \frac{\partial L}{\partial \sigma^2} \right)^2 \right] = \\
		&= E \left[ \left( \frac{\partial\sigma}{\partial\sigma^2} \frac{\partial L}{\partial\sigma} \right)^2 \right] = \\
		&= \left( \frac{\partial\sigma}{\partial\sigma^2} \right)^2 I_{\sigma\sigma} = \\
		&= \left( \frac1{2\sigma} \right)^2 \frac2{\sigma^2}
		= \frac1{2\sigma^4}.
	\end{align*}
\end{solution}

\begin{exercise}
	Per la gaussiana, calcolare la varianza della varianza campione
	\begin{equation*}
		\hat V(\mathbf x) = \frac1{N-1} \sum_{i=1}^N (x_i - \bar x)^2
	\end{equation*}
	e confrontarla con l'informazione di Fisher per la varianza.
\end{exercise}

\begin{solution}
	La varianza di $\hat V$ non dipende da $\mu$ e scala come $\sigma^4$,
	quindi fissiamo la distribuzione delle $x_i$ a media nulla e varianza unitaria:
	\begin{equation*}
		p(x) = \frac1{\sqrt{2\pi}} e^{-\frac12 x^2}.
	\end{equation*}
	Innanzitutto ricaviamo alcune utili proprietà della varianza:
	\begin{align*}
		\var[\lambda a] &= \lambda^2 \var[a], \\
		\var[a]
		&= E[(a-E[a])^2] = \\
		&= E[a^2] - E[2aE[a]] + E[E[a]^2] = \\
		&= E[a^2] - 2E[a]^2 + E[a]^2 = \\
		&= E[a^2] - E[a]^2, \\
		\var[a+b]
		&= E[(a+b)^2] - E[a+b]^2 = \\
		&= E[a^2 + b^2 + 2ab] - E[a]^2 - E[b]^2 - 2E[a]E[b] = \\
		&= \var[a] + \var[b] + 2(E[ab] - E[a]E[b]) = \\
		&= \var[a] + \var[b] + 2\cov[a,b] \\
		\intertext{che, se $a$ e $b$ sono indipendenti, diventa}
		\var[a+b] &= \var[a] + \var[b].
	\end{align*}
	Adesso calcolare la varianza di $\hat V$ è semplice:
	\begin{align*}
		\sum_i (x_i-\bar x)^2
		&= \sum_ix_i^2 - 2\bar x\sum_ix_i + N\bar x^2 = \\
		&= \sum_ix_i^2 - N\bar x^2, \\
		\var[\hat V]
		&= \frac1{(N-1)^2} \left( \sum_i\var[x_i^2] - \var[N\bar x^2] \right) = \\
		&= \frac1{(N-1)^2} \big( N\var[x^2] - N^2\var[\bar x^2] \big);
	\end{align*}
	la media è gaussiana con varianza $1/N$, quindi
	\begin{align*}
		\var[\bar x^2]
		= \var \left[ \frac{x^2}N \right ]
		= \frac{\var[x^2]}{N^2};
	\end{align*}
	alla fine dobbiamo calcolare solo $\var[x^2]$:
	\begin{align*}
		E[x^4]
		&= \frac1{\sqrt{2\pi}} \int\de x\, x^4 e^{-\frac12 x^2} = \\
		&= \frac1{\sqrt{2\pi}} \int\de x\, x^3 \left (-\frac{\de}{\de x} e^{-\frac12 x^2} \right) = \\
		&= \frac1{\sqrt{2\pi}} \int\de x\, 3x^2 e^{-\frac12 x^2} = \\
		&= 3E[x^2] = 3, \\
		\var[x^2]
		&= E[x^4] - E[x^2]^2
		= 2,
	\end{align*}
	dunque infine
	\begin{align*}
		\var[\hat V]
		&= \frac1{(N-1)^2} \left( 2N - N^2 \frac2{N^2} \right) = \\
		&= \frac2{N-1}
	\end{align*}
	che, per varianza qualunque, diventa
	\begin{equation*}
		\var[\hat V]
		= \frac{2\sigma^4}{N-1}.
	\end{equation*}
	L'informazione di Fisher per la varianza è
	\begin{equation*}
		I(\sigma^2)
		= \frac N{2\sigma^4},
	\end{equation*}
	viene dunque spontaneo calcolare il prodotto delle due:
	\begin{equation*}
		\var[\hat V]I(\sigma^2)
		= \frac N{N-1}.
	\end{equation*}
\end{solution}
